<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_99</id><course_url>https://www.coursera.org/learn/ml-classification</course_url><course_name>Machine Learning: Classification</course_name><course_platform>Coursera</course_platform><course_instructor>Carlos Guestrin</course_instructor><course_introduction>Case Studies: Analyzing Sentiment &amp; Loan Default Prediction

In our case study on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...).  In our second case study for this course, loan default prediction, you will tackle financial data, and predict when a loan is likely to be risky or safe for the bank. These tasks are an examples of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification. 

In this course, you will create classifiers that provide state-of-the-art performance on a variety of tasks.  You will become familiar with  the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting.  In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent.  You will implement these technique on real-world, large-scale machine learning tasks.  You will also address significant tasks you will face in real-world applications of ML, including handling missing data and measuring precision and recall to evaluate a classifier.  This course is hands-on, action-packed, and full of visualizations and illustrations of how these techniques will behave on real data.  We've also included optional content in every module, covering advanced topics for those who want to go even deeper! 

Learning Objectives: By the end of this course, you will be able to:
   -Describe the input and output of a classification model.
   -Tackle both binary and multiclass classification problems.
   -Implement a logistic regression model for large-scale classification.  
   -Create a non-linear model using decision trees.
   -Improve the performance of any model using boosting.
   -Scale your methods with stochastic gradient ascent.
   -Describe the underlying decision boundaries.  
   -Build a classification model to predict sentiment in a product review dataset.  
   -Analyze financial data to predict loan defaults.
   -Use techniques for handling missing data.
   -Evaluate your models using precision-recall metrics.
   -Implement these techniques in Python (or in the language of your choice, though Python is highly recommended).</course_introduction><course_category>Browse.Data Science.Machine Learning</course_category><course_tag>Logistic Regression//Statistical Classification//Classification Algorithms//Decision Tree</course_tag><course_rating>4.7</course_rating><course_orgnization>University of Washington</course_orgnization><course_chapter>Welcome!//Linear Classifiers &amp; Logistic Regression//Learning Linear Classifiers//Overfitting &amp; Regularization in Logistic Regression//Decision Trees//Preventing Overfitting in Decision Trees//Handling Missing Data//Boosting//Precision-Recall//Scaling to Huge Datasets &amp; Online Learning</course_chapter><course_sub_chapter>[['Welcome to the classification course, a part of the Machine Learning Specialization', 'What is this course about?', 'Impact of classification', 'Course overview', 'Outline of first half of course', 'Outline of second half of course', 'Assumed background', "Let's get started!"], ['Linear classifiers: A motivating example', 'Intuition behind linear classifiers', 'Decision boundaries', 'Linear classifier model', 'Effect of coefficient values on decision boundary', 'Using features of the inputs', 'Predicting class probabilities', 'Review of basics of probabilities', 'Review of basics of conditional probabilities', 'Using probabilities in classification', 'Predicting class probabilities with (generalized) linear models', 'The sigmoid (or logistic) link function', 'Logistic regression model', 'Effect of coefficient values on predicted probabilities', 'Overview of learning logistic regression models', 'Encoding categorical inputs', 'Multiclass classification with 1 versus all', 'Recap of logistic regression classifier'], ['Goal: Learning parameters of logistic regression', 'Intuition behind maximum likelihood estimation', 'Data likelihood', 'Finding best linear classifier with gradient ascent', 'Review of gradient ascent', 'Learning algorithm for logistic regression', 'Example of computing derivative for logistic regression', 'Interpreting derivative for logistic regression', 'Summary of gradient ascent for logistic regression', 'Choosing step size', 'Careful with step sizes that are too large', 'Rule of thumb for choosing step size', '(VERY OPTIONAL) Deriving gradient of logistic regression: Log trick', '(VERY OPTIONAL) Expressing the log-likelihood', '(VERY OPTIONAL) Deriving probability y=-1 given x', '(VERY OPTIONAL) Rewriting the log likelihood into a simpler form', '(VERY OPTIONAL) Deriving gradient of log likelihood', 'Recap of learning logistic regression classifiers'], ['Evaluating a classifier', 'Review of overfitting in regression', 'Overfitting in classification', 'Visualizing overfitting with high-degree polynomial features', 'Overfitting in classifiers leads to overconfident predictions', 'Visualizing overconfident predictions', '(OPTIONAL) Another perspecting on overfitting in logistic regression', 'Penalizing large coefficients to mitigate overfitting', 'L2 regularized logistic regression', 'Visualizing effect of L2 regularization in logistic regression', 'Learning L2 regularized logistic regression with gradient ascent', 'Sparse logistic regression with L1 regularization', 'Recap of overfitting &amp; regularization in logistic regression'], ['Predicting loan defaults with decision trees', 'Intuition behind decision trees', 'Task of learning decision trees from data', 'Recursive greedy algorithm', 'Learning a decision stump', 'Selecting best feature to split on', 'When to stop recursing', 'Making predictions with decision trees', 'Multiclass classification with decision trees', 'Threshold splits for continuous inputs', '(OPTIONAL) Picking the best threshold to split on', 'Visualizing decision boundaries', 'Recap of decision trees'], ['A review of overfitting', 'Overfitting in decision trees', "Principle of Occam's razor: Learning simpler decision trees", 'Early stopping in learning decision trees', '(OPTIONAL) Motivating pruning', '(OPTIONAL) Pruning decision trees to avoid overfitting', '(OPTIONAL) Tree pruning algorithm', 'Recap of overfitting and regularization in decision trees'], ['Challenge of missing data', 'Strategy 1: Purification by skipping missing data', 'Strategy 2: Purification by imputing missing data', 'Modifying decision trees to handle missing data', 'Feature split selection with missing data', 'Recap of handling missing data'], ['The boosting question', 'Ensemble classifiers', 'Boosting', 'AdaBoost overview', 'Weighted error', 'Computing coefficient of each ensemble component', 'Reweighing data to focus on mistakes', 'Normalizing weights', 'Example of AdaBoost in action', 'Learning boosted decision stumps with AdaBoost', 'The Boosting Theorem', 'Overfitting in boosting', 'Ensemble methods, impact of boosting &amp; quick recap'], ['Case-study where accuracy is not best metric for classification', 'What is good performance for a classifier?', 'Precision: Fraction of positive predictions that are actually positive', 'Recall: Fraction of positive data predicted to be positive', 'Precision-recall extremes', 'Trading off precision and recall', 'Precision-recall curve', 'Recap of precision-recall'], ["Gradient ascent won't scale to today's huge datasets", 'Timeline of scalable machine learning &amp; stochastic gradient', "Why gradient ascent won't scale", 'Stochastic gradient: Learning one data point at a time', 'Comparing gradient to stochastic gradient', 'Why would stochastic gradient ever work?', 'Convergence paths', 'Shuffle data before running stochastic gradient', 'Choosing step size', "Don't trust last coefficients", '(OPTIONAL) Learning from batches of data', '(OPTIONAL) Measuring convergence', '(OPTIONAL) Adding regularization', 'The online learning task', 'Using stochastic gradient for online learning', 'Scaling to huge datasets through parallelization &amp; module recap']]</course_sub_chapter><course_time>Approx. 42 hours to complete</course_time><reviews>['Very impressive course, I would recommend taking course 1 and 2 in this specialization first since they skip over some things in this course that they have explained thoroughly in those courses', 'Hats off to the team who put the course together! Prof Guestrin is a great teacher. The course gave me in-depth knowledge regarding classification and the math and intuition behind it. It was fun!', 'Very helpful. Many ThanksSome suggestions:1.Please add LDA into the module.2.It is really important if you guys can provide more examples for pandas and scikit-learn users in programming assignments like you do in regression module.', 'First, coursera is a ghost town.  There is no activity on the forum.  Real responses stopped a year ago. Most of the activity is from 3 years ago. This course is dead.', 'Relying on a non-open source library for all of the code examples vitiates the value of this course.  It should use Pandas and sklearn.', 'Quite Interesting. Entertaining and the lectures are quite easy to follow.', 'Very good lessons on classification.', "It's such a well organized course. Concepts are taught in an interesting way and made simple to understand through examples that thread along the course. I would recommend any aspiring data scientists to take this course. Thank you Carlos and Emily.", 'Excellent lessons on this important topic Classification. I think all major areas were explained quite nicely, with proper examples.', 'This course covered very interesting aspects of real-world applications for machine learning. From my point of view, the theory was very clear an valuable, until that point that the programming assignments closed the cycle beautifully.', 'very good', 'Good Course!!', 'Really awesome course. Nice balance between practical uses, theory, and implementation projects. It\'s good they kept the "optional" videos for the more detailed discussion instead of just removing that material. Totally recommend it.', 'the person who wants to start career in machine learning must take this course! Its awsome :)', 'Very good course for classification in machine learning - top presentation documents - very well structured and practical', 'one of the best experience about this course i gained I learned a lot about machine learning classification further machine learning regression thanks a lot Coursera :) ', 'It is really useful and up to date.', "Great course, provided details that not show in others' and textbooks.", 'it was easy to understand', 'Nice!!', 'Brilliant course!', 'useful and helpful course', 'good', 'very good course for classification.', 'It is very intuitive and easy to follow.']</reviews><reviewers>['By Christian J', 'By Saqib N S', 'By Feng G', 'By Lewis C L', 'By Alex H', 'By Javier A', 'By Xue', 'By Sathiraju E', 'By Nitin D ', 'By leonardo d', 'By Nidal M G', 'By Gaurav G', 'By Manuel G', 'By Arslan a', 'By Reinhold L ', 'By Shazia B', 'By Jialie ( Y', 'By Zhongkai M', 'By Satish K D', 'By Shashidhar Y', 'By parv j', 'By FanPingjie', 'By Akash G', 'By Aayush A', 'By Sara E E']</reviewers><review_date>['Jan 25, 2017', 'Oct 16, 2016', 'Jul 12, 2018', 'Jun 13, 2019', 'Feb 08, 2018', 'Nov 25, 2018', 'Dec 15, 2018', 'Nov 28, 2018', 'Dec 18, 2018', 'Dec 02, 2018', 'Dec 04, 2018', 'Dec 27, 2018', 'Jan 01, 2019', 'Feb 18, 2019', 'Mar 21, 2019', 'Mar 25, 2019', 'Feb 08, 2019', 'Feb 12, 2019', 'Feb 03, 2019', 'Apr 02, 2019', 'Mar 03, 2019', 'Dec 09, 2018', 'Mar 10, 2019', 'Jul 16, 2018', 'Mar 29, 2018']</review_date></doc>