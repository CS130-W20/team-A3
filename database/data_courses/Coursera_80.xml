<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_80</id><course_url>https://www.coursera.org/learn/supervised-learning</course_url><course_name>Обучение на размеченных данных</course_name><course_platform>Coursera</course_platform><course_instructor>Evgeniy Riabenko</course_instructor><course_introduction>Обучение на размеченных данных или обучение с учителем – это наиболее распространенный класс задач машинного обучения. К нему относятся те задачи, где нужно научиться предсказывать некоторую величину для любого объекта, имея конечное число примеров. Это может быть предсказание уровня пробок на участке дороги, определение возраста пользователя по его действиям в интернете, предсказание цены, по которой будет куплена подержанная машина.

В этом курсе вы научитесь формулировать и, конечно, решать такие задачи. В центре нашего внимания будут успешно применяемые на практике алгоритмы классификации и регрессии: линейные модели, нейронные сети, решающие деревья и так далее. Особый акцент мы сделаем на такой мощной технике как построение композиций, которая позволяет существенно повысить качество отдельных алгоритмов и широко используется при решении прикладных задач. В частности, мы узнаем про случайные леса и про метод градиентного бустинга.

Построение предсказывающих алгоритмов — это лишь часть работы при решении задачи анализа данных. Мы разберемся и с другими этапами: оценивание обобщающей способности алгоритмов, подбор параметров модели, выбор и подсчет метрик качества.

Задания и видео курса разработаны на Python 2.</course_introduction><course_category>Browse.Data Science.Machine Learning</course_category><course_tag>Random Forest//Python Programming//Machine Learning//Supervised Learning</course_tag><course_rating>4.8</course_rating><course_orgnization>Moscow Institute of Physics and Technology</course_orgnization><course_chapter>Машинное обучение и линейные модели//Борьба с переобучением и оценивание качества//Линейные модели: классификация и практические аспекты//Решающие деревья и композиции алгоритмов//Нейронные сети и обзор методов</course_chapter><course_sub_chapter>[['Как прекрасны машинное обучение и анализ данных', 'Как устроена специализация, и зачем ее проходить', 'МФТИ', 'Знакомство с машинным обучением', 'Обучение на размеченных данных', 'Обучение без учителя', 'Признаки в машинном обучении', 'Линейные модели в задачах регрессии', 'Обучение линейной регрессии', 'Градиентный спуск для линейной регрессии', 'Стохастический градиентный спуск', 'Линейная классификация', 'Функции потерь в задачах классификации'], ['Проблема переобучения', 'Регуляризация', 'Оценивание качества алгоритмов', 'Сравнение алгоритмов и выбор гиперпараметров', 'Метрики качества в задачах регрессии', 'Метрики качества классификации', 'Точность и полнота', 'Объединение точности и полноты', 'Качество оценок принадлежности классу', 'Встроенные датасеты. Sklearn.datasets', 'Кросс-валидация. Sklearn.cross_validation', 'Линейные модели. Sklearn.linear_model. Классификация', 'Линейные модели. Sklearn.linear_model. Регрессия', 'Метрики качества. Sklearn.metrics'], ['Задача регрессии', 'Метод максимального правдоподобия', 'Регрессия как максимизация правдоподобия', 'Регрессия как оценка среднего', 'Регуляризация', 'Задача оценивания вероятностей и логистическая регрессия', 'Масштабирование признаков', 'Спрямляющие пространства', 'Работа с категориальными признаками', 'Несбалансированные данные', 'Многоклассовая классификация', 'Подбор параметров по сетке. Sklearn.grid_search', 'Задача: bike sharing demand', 'Задача: bike sharing demand. Продолжение'], ['Решающие деревья', 'Обучение решающих деревьев', 'Критерии информативности', 'Критерии останова и стрижка деревьев', 'Решающие деревья и категориальные признаки', 'Решающие деревья в sklearn', 'Композиции деревьев', 'Смещение и разброс', 'Случайные леса', 'Трюки со случайными лесами', 'Случайные леса в sklearn', 'Композиции простых алгоритмов', 'Градиентный бустинг', 'Борьба с переобучением в градиентном бустинге', 'Градиентный бустинг для регрессии и классификации', 'Градиентный бустинг над решающими деревьями', 'Градиентный бустинг в XGBoost'], ['Однослойная нейронная сеть', 'Многослойная нейронная сеть. Функция ошибки', 'Оптимизация параметров нейронной сети', 'Регуляризация и прореживание нейронной сети', 'Байесовская классификация: содержание урока', 'Спам-фильтры и наивный байесовский классификатор', 'Байесовский классификатор', 'Восстановление распределений (часть 1)', 'Восстановление распределений (часть 2)', 'Минимизация риска', 'Минимизация риска и анализ функций потерь', 'Метрические алгоритмы и SVM: содержание урока', 'Метод k ближайших соседей', 'Настройка параметров в kNN', 'Метрики в kNN', 'Проклятие размерности', 'Рекомендации фильмов с помощью kNN', 'Метод опорных векторов (SVM)', 'Ядра в методе опорных векторов (Kernel trick)', 'Теорема Байеса', 'Байесовский подход к теории вероятностей', 'Байесовские модели в задачах машинного обучения', 'Бонусное видео']]</course_sub_chapter><course_time>Approx. 43 hours to complete</course_time><reviews>['Первые 4 недели более-менее, хотя с теми же недостатками что и первый курс: галопом по европам, быстро, шаблонно отчитать формулы. Но 5 неделя - перебор даже для вашего стиля, так неинтересно рассказать про NN - нужно особое умение. Слайд где нарисованы 3 желтых пятна разного размера и какая-то точка X* - это вообще "эталон" визуализации)) Постараюсь успеть до окончания подписки как можно больше, но продлять спецуху желание пропало.', 'Без отличного математического бэкграунда (или просто отличного понимания всех понятий из первого курса - особенно, теория вероятности, операции в векторных пространствах и пр.), а также не понимая математики первых недель курса, всю  математику курса понять вряд ли получится. Если такого бэкграунда нет - это будет неплохой обзорный курс, но его в любом случае нужно будет углублять уже самостоятельно. Для меня по содержанию - отлично.', 'Слишком рано стало сложно, мало примеров и много формул.', 'Очень большая теоретическая нагрузка. Несмотря на то, что я "в теме" некоторые вещи пришлось открывать заново. Отдельно хотелось бы проехаться по примерам. Я так понял кур был подготовлен в 2016 году, с тех пор прошло 2 года - примеры надо бы обновить, т.к. часть библиотек питона обновилась (особенно sklearn) и некоторые методы ругаются, что скоро перестанут работать, в одном примере у меня совсем отказался строиться график.. ну не то, чтобы мне лень но технические проблемы курса при обучении решать - отнимает много ценного времени.. с временем у меня конкретные проблемы в силу занятости.', 'Каждый раз, когда приходилось заниматься подгонкой ответа под грейдер, у меня неистово бомбило. :(', 'В целом отлично, но с грейдером на третьей неделе пришлось очень помучиться. Надеюсь, что поправили.', 'Очень неравномерное распределение нагрузки по неделям', 'До четвёртой недели всё хорошо, на 4 неделе про градиентный бустинг задача невнятная, 5 неделю можно было растянуться на 2-3 и дать нормальное объяснение нейронным сетям. Pybrain включить отдельное приключение. Вопросы на форумах месяцами лежат, так что лучше сразу гуглить.', 'Очень интересно, но очень трудновоспринимаемо', 'Наверное, второй курс - самый тяжелый из всей специализации в плане задач по программированию, но это не делает его менее интересным, нужно всего лишь преодолеть эти трудности :)', 'Замечательный курс, который дает массу знаний и навыков по теме анализа данных.\n', 'Хороший курс, позволяющий ознакомиться в алгоритмами. Дает примерное представление как работают алгоритмы, какие вообще алгоритмы бывают и как их нужно использовать в sklearn. Соколов - "explanator" from the God.', 'Очень непростой, но интересный курс.', 'Очень понравился курс. Если формулы на слайдах появляются, то подробно разбираются. Спасибо, за задание "Градиентный бустинг своими руками".  Это задание, действительно, помогло лучше разобраться в алгоритме. Отдельно хотелось бы отметить Евгения Соколова, его лекции было приятно слушать и материал усваивался легко. ', 'Хороший курс, добротный, но хотелось бы увидеть больше информации о нейронных сетях, либо совсем тогда убрать эту тему из данного курса, ибо времени было уделено ей совсем мало.', 'Спасибо большое за курс!', 'Курс интересный. Правда местами складывалось впечатление что переход от простых вещей к более сложным отсутствует. ', 'Было трудно', 'В целом хорошо, но теория далеко не всегда очевидна. Ее изложения, конечно, хватает для того чтобы сдать тесты, однако цельная картина не всегда складывается. Сложность заданий нелинейная: что-то с первого раза без ошибок делается, а что-то только после прочтения форума. Есть проблема с принятием ответов в практике. Но курс хорош практикой и вовлекает в поиск способов решения возникающих проблем.', 'В некоторых заданиях по программированию не хватает конкретики, кое-где для внесения определенности при объявлении классификатора полезно было бы зафиксировать random_stare. Прекрасно было бы побольше рассказать про нейронные сети и их построение, разобрав популярные пакеты TensorFlow или Theano. Ну и, например, keras, как надстройку над ними.', 'Отличный курс для общего понимания Обучения на размеченных данных. Но иногда бывает резкое углубление в математику, что под силу не для всех.', 'Я отдаю деньги, а потом мне еще и ребусы в заданиях разгадывать. Если вы уж делаете этот курс для людей, которые успешно работаю в сфере анализа данных - так вы хотя бы пишите об этом. Не все ваши студенты закончили МФТИ!', 'Интересный курс, но сложность заданий не всегда соответствует излагаемому материалу, да и порядок изложения не всегда логичен. Хорошо, что у sklearn подробная доступная документация :)', 'Лекции и задаение по нейронным сетям - низкого качества, лучше убрать из курса и сделать ссылки на такие статьи, как: ', 'Достаточно информативный и интересный курс. Но версия python 2.7 по тихоньку устаревает и некоторые задачи уже трактуются не корректно, не смотря на то что в самом начале курса это оговаривалось. Я бы предложил обновить курс под python 3']</reviews><reviewers>['By Pavel A', 'By Кузьмин Ю', 'By Andrei E', 'By Капустин А Н', 'By Филипп', 'By Aleksandr S', 'By Антюфриева Л А', 'By Бунятов А И', 'By Konstantin K', 'By Kira V', 'By Sergei B', 'By Timur B', 'By Задойный А', 'By Oleg D', 'By Nikita G', 'By Maxim', 'By Burobin I V', 'By Фомин А Г', 'By Usenko S', 'By Голубев К О', 'By Нестеров А А', 'By Сокольцов В Ю', 'By Daniel B', 'By Petrukhin I', 'By Alexander A']</reviewers><review_date>['Oct 11, 2018', 'Aug 06, 2017', 'Aug 18, 2018', 'Oct 02, 2018', 'Aug 09, 2017', 'Apr 24, 2016', 'Dec 03, 2018', 'Jan 28, 2019', 'Mar 29, 2018', 'Jun 08, 2017', 'Jul 08, 2016', 'Apr 21, 2018', 'Jun 10, 2016', 'May 25, 2016', 'Jul 27, 2017', 'Dec 30, 2016', 'Jul 10, 2019', 'Jun 17, 2018', 'Nov 17, 2017', 'Jul 23, 2017', 'Sep 17, 2017', 'Jun 12, 2017', 'May 11, 2019', 'Mar 25, 2018', 'Mar 21, 2018']</review_date></doc>