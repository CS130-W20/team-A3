<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_466</id><course_url>https://www.coursera.org/learn/building-resilient-streaming-systems-gcp-fr</course_url><course_name>Building Resilient Streaming Systems on Google Cloud Platform en Français</course_name><course_platform>Coursera</course_platform><course_instructor>Google Cloud Training</course_instructor><course_introduction>Ce cours à la demande s'appuie sur la formation Google Cloud Platform Big Data and Machine Learning Fundamentals. Il s'agit d'un cours accéléré, que vous pourrez effectuer en une semaine. Grâce à une série de conférences vidéo, de démonstrations et d'ateliers, vous apprendrez à créer des pipelines de flux de données à l'aide de Google Cloud Pub/Sub et de Dataflow. Objectif : faciliter la prise de décisions en temps réel. Vous apprendrez également à créer des tableaux de bord pour présenter des résultats personnalisés à différents groupes d'intervenants.


Prérequis :
• Avoir suivi la formation Google Cloud Platform Big Data and Machine Learning Fundamentals (ou disposer d'une expérience équivalente)
• Quelques connaissances en Java

Objectifs :
• Savoir quand utiliser l'analyse de flux en temps réel
• Gérer les événements de données avec le service de messagerie asynchrone Google Cloud PubSub
• Rédiger des pipelines de flux de données et effectuer des transformations le cas échéant
• Maîtriser les deux facettes d'un pipeline de flux de données : production et consommation
• Faire interagir Dataflow, BigQuery et Cloud Pub/Sub pour obtenir des analyses et des flux en temps réel</course_introduction><course_category>Browse.Data Science.Data Analysis</course_category><course_tag /><course_rating /><course_orgnization>Google Cloud</course_orgnization><course_chapter>Module 1 : Architecture des pipelines d'analyse des flux de données//Module 2 : Ingestion de volumes variables//Module 3 : Mise en œuvre de pipelines de flux de données//Module 4 : Analyse de flux de données et tableaux de bord//Module 5 : Répondre aux exigences de débit et de latence</course_chapter><course_sub_chapter>[["Qu'est-ce que le flux ?", "Défi n° 1 : Les volumes variables nécessitent une capacité d'ingestion pour évoluer et tolérer les pannes", 'Défi n° 2 : Une latence est inévitable', "Défi n° 3 : Besoin d'informations instantanées", 'Présentation de plusieurs scénarios de flux'], ["Qu'est-ce que Pub/Sub ?", 'Fonctionnement : Thèmes et abonnements', "Présentation de l'atelier", 'Atelier : Démonstration et évaluation'], ['Flux Dataflow', 'Défis du traitement par flux', "Développement d'un pipeline de traitement des données par flux pour le trafic en direct", 'Gestion des données en retard : filigranes, déclenchements et accumulation', "Présentation de l'atelier", 'Atelier : Démonstration et évaluation'], ['Analyse de flux et tableaux de bord', "Présentation de l'atelier", 'Atelier : Démonstration et évaluation'], ['Débit et latence', "Bigtable : Service NoSQL d'envergure et rapide en autoscaling", 'Ingestion dans Bigtable', 'Concevoir pour Bigtable', 'Flux dans Bigtable', 'Atelier : Démonstration et évaluation', 'Considérations sur les performances', 'Résumé de la spécialisation Data Engineering sur GCP']]</course_sub_chapter><course_time>Approx. 8 hours to complete</course_time><reviews>[]</reviews><reviewers>[]</reviewers><review_date>[]</review_date></doc>