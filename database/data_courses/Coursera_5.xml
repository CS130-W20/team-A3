<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_5</id><course_url>https://www.coursera.org/learn/deep-neural-network</course_url><course_name>Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</course_name><course_platform>Coursera</course_platform><course_instructor>Andrew Ng</course_instructor><course_introduction>This course will teach you the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. 

After 3 weeks, you will: 
- Understand industry best-practices for building deep learning applications. 
- Be able to effectively use the common neural network "tricks", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, 
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
- Be able to implement a neural network in TensorFlow. 

This is the second course of the Deep Learning Specialization.</course_introduction><course_category>Browse.Data Science.Machine Learning</course_category><course_tag>Hyperparameter//Tensorflow//Hyperparameter Optimization//Deep Learning</course_tag><course_rating>4.9</course_rating><course_orgnization>deeplearning.ai</course_orgnization><course_chapter>Practical aspects of Deep Learning//Optimization algorithms//Hyperparameter tuning, Batch Normalization and Programming Frameworks</course_chapter><course_sub_chapter>[['Train / Dev / Test sets', 'Bias / Variance', 'Basic Recipe for Machine Learning', 'Regularization', 'Why regularization reduces overfitting?', 'Dropout Regularization', 'Understanding Dropout', 'Other regularization methods', 'Normalizing inputs', 'Vanishing / Exploding gradients', 'Weight Initialization for Deep Networks', 'Numerical approximation of gradients', 'Gradient checking', 'Gradient Checking Implementation Notes', 'Yoshua Bengio interview'], ['Mini-batch gradient descent', 'Understanding mini-batch gradient descent', 'Exponentially weighted averages', 'Understanding exponentially weighted averages', 'Bias correction in exponentially weighted averages', 'Gradient descent with momentum', 'RMSprop', 'Adam optimization algorithm', 'Learning rate decay', 'The problem of local optima', 'Yuanqing Lin interview'], ['Tuning process', 'Using an appropriate scale to pick hyperparameters', 'Hyperparameters tuning in practice: Pandas vs. Caviar', 'Normalizing activations in a network', 'Fitting Batch Norm into a neural network', 'Why does Batch Norm work?', 'Batch Norm at test time', 'Softmax Regression', 'Training a softmax classifier', 'Deep learning frameworks', 'TensorFlow']]</course_sub_chapter><course_time>Approx. 14 hours to complete</course_time><reviews>['Walking away from this course, I do *not* feel adequately prepared to implement (end-to-end) everything that I\'ve learned. I felt this way after the first course of this series, but even more so now. Yes, I understand the material, but the programming assignments really don\'t amount to more than "filling in the blanks"--that doesn\'t really test whether or not I\'ve mastered the material. I understand that this is terribly hard to accomplish through a MOOC, and having taught university-level courses myself, I understand how much effort is involved in doing so in the "real world". In either case, if I\'m paying for a course, I expect to have a solid grasp on the material after completing the material, and though you\'ve clearly put effort into assembling the programming exercises, they don\'t really gauge this on any level. Perhaps it would be worth considering a higher cost of the course in order to justify the level of effort required to put together assessments that genuinely put the student through their paces in order to assure that a "100%" mark genuinely reflects both to you and the learner that they have truly internalized and mastered the material. It seems to me that this would pay off dividends not only for the learner, but also for the you as the entity offering such a certificate.', 'Lectures are good. Quizzes and programming exercises too easy.', 'The course provides very good insights of the practical aspect of implementing neural networks in general. Prof. Ng, as always, delivered very clear explanation for even the difficult concepts, and I have thoroughly enjoyed every single lecture video.', 'Very good course. Andrew really steps it up in part two with lots of valuable information. ', 'As far as the video lectures is concerned, the videos are excellent; it is the same quality as the other courses from the same instructor. This course contains a lot of relevant and useful material, and is worth studying, and complements the first course (and the free ML course very well).', "Excellent course. When I learned about implementing ANN using keras in python, I just followed some tutorials but didn't understand the tradeoff among many parameters like the number of layers, nodes per layers, epochs, batch size, etc. This course is helping me a lot to understand them.  Great work Mr. Andrew Ng. :) ", 'Thank you Andrew!! I know start to use Tensorflow, however, this tool is not well for a research goal. Maybe, pytorch could be considered in the future!! And let us know how to use pytorch in Windows.', 'very practical.', 'Add more programming assignments to clear fundamentals.', 'programming assignments too easy', "I did not think this was a great course, especially since it's paid. The programming assignment notebooks are very buggy and the course mentors are of varying quality. It feels more than a bit unfinished. It also covers two completely different topics - tools for improving deep learning nets and tensorflow - and doesn't make much of an effort to integrate them at all. The course could have used at least one more week of content and assignments to better explain the point of tf.", '牛！', 'This course guides you through the details required to finetune your learning algorithms.', 'awesome!', 'thanks all', 'Amazing course, starts right off the bat with hyperparameters, regularization and tunings.', 'Yet another great course from Prof. Andrew Ng and Coursera. Deeply grateful to all involved in the preparation of this course. Absolutely essential to learn these concepts if we want to build and optimize deep neural networks for creating great products!', None, 'Wery usefull and clear', 'awesome', 'Excellent Course', 'cool', 'As good as the first one with more insights in hyperparameters tuning and faster convergence techniques. The whole course finishes with a small introduction to TensorFlow.', 'I love this course. It has many in-depth tips and advices based on many real life experiences. Many suggestions can be applied directly into solving difficult Deep Learning practices.', 'It is already good, But places need to correct typo']</reviews><reviewers>['By Brennon B', 'By oli c', 'By Lien C', 'By Matthew G', 'By Alan S', 'By Md. R K S', 'By Xiao G', 'By Tang Y', 'By Harsh V', 'By Yuhang W', 'By Ethan G', 'By 侯宇翔', 'By Manuel H C B D', 'By Sagar J', 'By LeslieJ', 'By Sachin G W', 'By Satyam D', 'By 黄怡欣', 'By Maxim D', 'By Anamika M', 'By Ganesh V K', 'By Jhon S', 'By Marc M v W', 'By Leigh L', 'By Priyanka D G']</reviewers><review_date>['Apr 23, 2018', 'Dec 09, 2018', 'Mar 31, 2019', 'Apr 18, 2019', 'Sep 30, 2017', 'Apr 15, 2019', 'Oct 31, 2017', 'Apr 15, 2019', 'Jan 22, 2019', 'Nov 25, 2018', 'Oct 17, 2017', 'Dec 11, 2018', 'Dec 12, 2018', 'Dec 12, 2018', 'Dec 10, 2018', 'Dec 11, 2018', 'Dec 12, 2018', 'Dec 11, 2018', 'Nov 25, 2018', 'Nov 25, 2018', 'Nov 25, 2018', 'Nov 26, 2018', 'Nov 25, 2018', 'Nov 26, 2018', 'Nov 25, 2018']</review_date></doc>