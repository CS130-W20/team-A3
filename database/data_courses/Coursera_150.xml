<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_150</id><course_url>https://www.coursera.org/learn/sample-based-learning-methods</course_url><course_name>Sample-based Learning Methods</course_name><course_platform>Coursera</course_platform><course_instructor>Martha White</course_instructor><course_introduction>In this course, you will learn about several algorithms that can learn near optimal policies based on trial and error interaction with the environment---learning from the agent’s own experience. Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. We will cover intuitively simple but powerful Monte Carlo methods, and temporal difference learning methods including Q-learning. We will wrap up this course investigating how we can get the best of both worlds: algorithms that can combine model-based planning (similar to dynamic programming) and temporal difference updates to radically accelerate learning.

By the end of this course you will be able to:
 
- Understand Temporal-Difference learning and Monte Carlo as two strategies for estimating value functions from sampled experience
- Understand the importance of exploration, when using sampled experience rather than dynamic programming sweeps within a model
- Understand the connections between Monte Carlo and Dynamic Programming and TD. 
- Implement and apply the TD algorithm, for estimating value functions
- Implement and apply Expected Sarsa and Q-learning (two TD methods for control) 
- Understand the difference between on-policy and off-policy control
- Understand planning with simulated experience (as opposed to classic planning strategies)
- Implement a model-based approach to RL, called Dyna, which uses simulated experience 
- Conduct an empirical study to see the improvements in sample efficiency when using Dyna</course_introduction><course_category>Browse.Data Science.Machine Learning</course_category><course_tag>Artificial Intelligence (AI)//Machine Learning//Reinforcement Learning//Function Approximation//Intelligent Systems</course_tag><course_rating>4.8</course_rating><course_orgnization>University of Alberta</course_orgnization><course_chapter>Welcome to the Course! // Monte Carlo Methods for Prediction &amp; Control//Temporal Difference Learning Methods for Prediction //Temporal Difference Learning Methods for Control //Planning, Learning &amp; Acting</course_chapter><course_sub_chapter>[['Course Introduction', 'Instructor Introductions'], ['What is Monte Carlo?', 'Using Monte Carlo for Prediction', 'Using Monte Carlo for Action Values', 'Using Monte Carlo methods for generalized policy iteration', 'Solving the Blackjack Example', 'Epsilon-soft policies', 'Why does off-policy learning matter?', 'Importance Sampling', 'Off-Policy Monte Carlo Prediction', 'Week 1 Summary'], ['What is Temporal Difference (TD) learning?', 'Rich Sutton: The Importance of TD Learning', 'The advantages of temporal difference learning', 'Comparing TD and Monte Carlo', 'Andy Barto and Rich Sutton: More on the History of RL', 'Week 2 Summary'], ['Sarsa: GPI with TD', 'Sarsa in the Windy Grid World', 'What is Q-learning?', 'Q-learning in the Windy Grid World', 'How is Q-learning off-policy?', 'Expected Sarsa', 'Expected Sarsa in the Cliff World', 'Generality of Expected Sarsa', 'Week 3 Summary'], ['What is a Model?', 'Comparing Sample and Distribution Models', 'Random Tabular Q-planning', 'The Dyna Architecture', 'The Dyna Algorithm', 'Dyna &amp; Q-learning in a Simple Maze', 'What if the model is inaccurate?', 'In-depth with changing environments', 'Week 4 Summary', 'Congratulations!']]</course_sub_chapter><course_time>Approx. 17 hours to complete</course_time><reviews>['Course was amazing until I reached the final assignment. What a terrible way to grade the notebook part. Also, nobody around in the forums to help... I would still recommend this to anyone interested, unless you have no intention of doing the weekly readings.', "Great course! Lots of hands-on RL algorithms. I'm looking forward to the next course in the specialization.", 'Very good. ', 'Great Course. Every aspect top notch', 'A good course with proper Mathematical insights ', "This is THE course to go with Sutton &amp; Barto's Reinforcement Learning: An Introduction. "]</reviews><reviewers>['By Manuel V d S', 'By Stewart A', 'By LuSheng Y', 'By Luiz C', 'By Ashish S', 'By Neil S']</reviewers><review_date>['Sep 11, 2019', 'Sep 03, 2019', 'Sep 10, 2019', 'Sep 13, 2019', 'Sep 16, 2019', 'Sep 12, 2019']</review_date></doc>