<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_100</id><course_url>https://www.coursera.org/learn/practical-rl</course_url><course_name>Practical Reinforcement Learning</course_name><course_platform>Coursera</course_platform><course_instructor>Pavel Shvechikov</course_instructor><course_introduction>Welcome to the Reinforcement Learning course. 

Here you will find out about:

- foundations of RL methods: value/policy iteration, q-learning, policy gradient, etc.
--- with math &amp; batteries included

- using deep neural networks for RL tasks
--- also known as "the hype train"

- state of the art RL algorithms
--- and how to apply duct tape to them for practical problems.

- and, of course, teaching your neural network to play games
--- because that's what everyone thinks RL is about. We'll also use it for seq2seq and contextual bandits.

Jump in. It's gonna be fun!

Do you have technical problems? Write to us: coursera@hse.ru</course_introduction><course_category>Browse.Data Science.Machine Learning</course_category><course_tag /><course_rating>4.1</course_rating><course_orgnization>National Research University Higher School of Economics</course_orgnization><course_chapter>Intro: why should i care?//At the heart of RL: Dynamic Programming//Model-free methods//Approximate Value Based Methods//Policy-based methods//Exploration</course_chapter><course_sub_chapter>[['Why should you care', 'Reinforcement learning vs all', 'Multi-armed bandit', 'Decision process &amp; applications', 'Markov Decision Process', 'Crossentropy method', 'Approximate crossentropy method', 'More on approximate crossentropy method', 'Evolution strategies: core idea', 'Evolution strategies: math problems', 'Evolution strategies: log-derivative trick', 'Evolution strategies: duct tape', 'Blackbox optimization: drawbacks'], ['Reward design', 'State and Action Value Functions', 'Measuring Policy Optimality', 'Policy: evaluation &amp; improvement', 'Policy and value iteration'], ['Model-based vs model-free', 'Monte-Carlo &amp; Temporal Difference; Q-learning', 'Exploration vs Exploitation', 'Footnote: Monte-Carlo vs Temporal Difference', 'Accounting for exploration. Expected Value SARSA.', 'On-policy vs off-policy; Experience replay'], ['Supervised &amp; Reinforcement Learning', 'Loss functions in value based RL', 'Difficulties with Approximate Methods', "DQN – bird's eye view", 'DQN – the internals', 'DQN: statistical issues', 'Double Q-learning', 'More DQN tricks', 'Partial observability'], ['Intuition', 'All Kinds of Policies', 'Policy gradient formalism', 'The log-derivative trick', 'REINFORCE', 'Advantage actor-critic', 'Duct tape zone', 'Policy-based vs Value-based', 'Case study: A3C', 'A3C case study (2/2)', 'Combining supervised &amp; reinforcement learning'], ['Recap: bandits', 'Regret: measuring the quality of exploration', "The message just repeats. 'Regret, Regret, Regret.'", 'Intuitive explanation', 'Thompson Sampling', 'Optimism in face of uncertainty', 'UCB-1', 'Bayesian UCB', 'Introduction to planning', 'Monte Carlo Tree Search']]</course_sub_chapter><course_time>Approx. 39 hours to complete</course_time><reviews>['I would give it -5 star if it was possible. The course material is so vague but still understandable if you sleep on them 10 times more than watching it. Maybe Andrew Ng courses or Python Course or Advanced ML course on google cloud (GCD ) spoiled me However statistically and self-judgement , this is not the case. ', 'Pros: ', "have to give a one star on this course, content hard to understand, speaker speaks too fast, programming assignment many mistakes, move on to david silver's youtube video for RL.", 'Course 4 of Advanced Machine Learning, Practical Reinforcement Learning, is harder than Course 1, Introduction to Deep Learning.  (I jumped to Course 4 after Course 1).  That is saying quite a lot because I would describe Course 1 as "fiendishly difficult".', 'Interesting  topic, however several things are not acceptable for a paid course:', 'A great course with very practical assignments to help you learn how to implement RL algorithms. But it also has some stupid quiz questions which makes you feel confusing.', 'Brilliant content but quite some bugs in assignments ', "The course is really in 'beta' state. Be prepared to struggle against not only the practical assignments themselves, but also against their bugs and assignment grading infrastructure problems.", 'The class is very immature as of September 2018.  A good reason for taking this course is because it is one of few online courses where you can play with actual programming exercises of various reinforcement learning techniques, from dynamic programming to deep Q networks and actor critiques.  Examples are mostly for environments of Open AI gym.  You can also see examples where you use libraries such as tensorflow and pytorch used in the framework.  However, the codes, including submission and grading system, have numerous bugs, which forces you to do extra debugging works unrelated to the course topics.  Fortunately some early takers of the class left helpful comments on the forum, with which you can solve the most of issues if you read them carefully.', 'Still needs a lot of work ', "Indeed, this the 1st reinforcement learning course during May 2018. The topics and supporting materials are good for learning the course. Unfortunately, the course is not well-prepared in different aspects: 1) The assignments contained many bugs. One may spend half of the time to fix the bugs in the assignments. Sometimes, one may not be able to find tutor to ask for a help. The only thing one can do is helping herself or waiting for other classmates' feedbacks.2) Quiz is not designed for help one's learning. The questions in quiz are very confusing sometime. Also, one cannot get the correct answers after repeating the video several times. Sometime even one cannot find the topics in the lecture video. It takes you long time to try 'trail and error'.In all, it seem this course is not a well-prepared course in Coursera. I have paid and enrolled in many Coursera courses. Unfortunately, one might feel disappointed this time. A feedback from a PhD student (also a loyal customer of Coursera).", 'Well Prepared and taught course.. Will highly recommend as the primer for reinforcement learning', 'This is one of the Best Course available on Reinforcement Learning. I have gone through various study material but the depth and practical knowledge given in the course is awesome. ', "Had a lot of fun doing this course. Although some of my fellow classmates are complaining that there are a few bugs in assignments, fixing those bugs itself can be a learning experience. The assignments,in general, are fun, particularly the honor's assignments. ", "I really like the lectures and homework, especially the coding assignments, which help me play games with RL and also improve understanding of the typical RL algorithms. Also, the discussion forum is very helpful and I can usually get out of stuck by following mentors' and other students' advice. Great thanks to Pavel Shvechikov and Alexander Panin for making such a useful course available!", 'Great content! The python notebook submit problems leave a lot more to be desired. ', 'Доведите ноутбуки и grader до ума, не позорьтесь пожалуйста!', 'Course was very challenging what is good! Did several courses that were too easy. Quizzes were sometimes difficult to pass because of the way the answers are evaluated (all answers have to be correct) and even after watching the video several times the answers were not obvious.', 'This course was theoretically fulfilling, however i felt that the teachers failed to explain core principles with ease and felt a connection break in between their accent, their lectures and the slides in the background', 'The course gives a good intro to reniforcement learning. I liked the fact the assignments here are shorter compared to other coursers. However, the quality of preparation of the material is very low. In many cases there are problems with the code and you cannot submit from coursera. I had to download the docker container locally and fix the bugs in order to submit. Quizes are not very nicely prepared and mathematical notation not very clear. I think I struggled a lot to get some of the quizes finished as the accepted score is quite high and some questions require multiple answers and you have to get them all right in order to get a score. I think the authors need to spend more time refining the quizes as well as the assignments', 'The material covered in this course is very comprehensive, up-to-date, and broad. It goes far beyond typical RL courses/tutorials. BUT, at the moment the course is extremely raw: ', 'One of the speakers speaks too fast', 'Submission python code is very buggy. Instructors are hard to understand.', 'Instructor talks to fast and is hard to understand. Materials are full of bugs (which they admit).', 'Course not ready and has installation prerequisites. Seems to use a libraries (Docker, Env). ']</reviews><reviewers>['By Hamed N', 'By Pedro L A V', 'By Xiao M', 'By Jay G', 'By MASSON', 'By Fan Z', 'By maciej.osinski', 'By Roman P', 'By Kota M', 'By Tomas L', 'By Zikai W', 'By Vaibhav O', 'By Ajay K', 'By Sahil J', 'By Tingting X', 'By Chua R R', 'By Sergey', 'By Thomas F', 'By Keshav V J', 'By Hany A', 'By Mikhail V', 'By Sandeep K C', 'By Michel C', 'By Robert E', 'By Antony L']</reviewers><review_date>['Apr 23, 2019', 'Nov 27, 2018', 'Aug 19, 2018', 'Oct 30, 2018', 'Apr 07, 2019', 'Feb 14, 2019', 'Nov 02, 2018', 'Nov 05, 2018', 'Oct 04, 2018', 'Dec 28, 2018', 'Jun 16, 2018', 'Mar 17, 2019', 'May 28, 2019', 'Aug 03, 2018', 'Apr 22, 2019', 'Dec 24, 2018', 'Oct 13, 2018', 'Aug 05, 2019', 'Dec 27, 2018', 'Feb 16, 2019', 'May 23, 2019', 'Jul 13, 2019', 'Jun 12, 2019', 'Aug 17, 2019', 'Mar 12, 2019']</review_date></doc>