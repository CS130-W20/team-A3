<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_83</id><course_url>https://www.coursera.org/learn/ml-regression</course_url><course_name>Machine Learning: Regression</course_name><course_platform>Coursera</course_platform><course_instructor>Emily Fox</course_instructor><course_introduction>Case Study - Predicting Housing Prices

In our first case study, predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...).  This is just one of the many places where regression can be applied.  Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.

In this course, you will explore regularized linear regression models for the task of prediction and feature selection.  You will be able to handle very large sets of features and select between models of various complexity.  You will also analyze the impact of aspects of your data -- such as outliers -- on your selected models and predictions.  To fit these models, you will implement optimization algorithms that scale to large datasets.

Learning Outcomes:  By the end of this course, you will be able to:
   -Describe the input and output of a regression model.
   -Compare and contrast bias and variance when modeling data.
   -Estimate model parameters using optimization algorithms.
   -Tune parameters with cross validation.
   -Analyze the performance of the model.
   -Describe the notion of sparsity and how LASSO leads to sparse solutions.
   -Deploy methods to select between models.
   -Exploit the model to form predictions. 
   -Build a regression model to predict prices using a housing dataset.
   -Implement these techniques in Python.</course_introduction><course_category>Browse.Data Science.Machine Learning</course_category><course_tag>Linear Regression//Ridge Regression//Lasso (Statistics)//Regression Analysis</course_tag><course_rating>4.8</course_rating><course_orgnization>University of Washington</course_orgnization><course_chapter>Welcome//Simple Linear Regression//Multiple Regression//Assessing Performance//Ridge Regression//Feature Selection &amp; Lasso//Nearest Neighbors &amp; Kernel Regression//Closing Remarks</course_chapter><course_sub_chapter>[['Welcome!', 'What is the course about?', 'Outlining the first half of the course', 'Outlining the second half of the course', 'Assumed background'], ['A case study in predicting house prices', 'Regression fundamentals: data &amp; model', 'Regression fundamentals: the task', 'Regression ML block diagram', 'The simple linear regression model', 'The cost of using a given line', 'Using the fitted line', 'Interpreting the fitted line', 'Defining our least squares optimization objective', 'Finding maxima or minima analytically', 'Maximizing a 1d function: a worked example', 'Finding the max via hill climbing', 'Finding the min via hill descent', 'Choosing stepsize and convergence criteria', 'Gradients: derivatives in multiple dimensions', 'Gradient descent: multidimensional hill descent', 'Computing the gradient of RSS', 'Approach 1: closed-form solution', 'Approach 2: gradient descent', 'Comparing the approaches', 'Influence of high leverage points: exploring the data', 'Influence of high leverage points: removing Center City', 'Influence of high leverage points: removing high-end towns', 'Asymmetric cost functions', 'A brief recap'], ['Multiple regression intro', 'Polynomial regression', 'Modeling seasonality', 'Where we see seasonality', 'Regression with general features of 1 input', 'Motivating the use of multiple inputs', 'Defining notation', 'Regression with features of multiple inputs', 'Interpreting the multiple regression fit', 'Rewriting the single observation model in vector notation', 'Rewriting the model for all observations in matrix notation', 'Computing the cost of a D-dimensional curve', 'Computing the gradient of RSS', 'Approach 1: closed-form solution', 'Discussing the closed-form solution', 'Approach 2: gradient descent', 'Feature-by-feature update', 'Algorithmic summary of gradient descent approach', 'A brief recap'], ['Assessing performance intro', 'What do we mean by "loss"?', 'Training error: assessing loss on the training set', 'Generalization error: what we really want', 'Test error: what we can actually compute', 'Defining overfitting', 'Training/test split', 'Irreducible error and bias', 'Variance and the bias-variance tradeoff', 'Error vs. amount of data', 'Formally defining the 3 sources of error', 'Formally deriving why 3 sources of error', 'Training/validation/test split for model selection, fitting, and assessment', 'A brief recap'], ['Symptoms of overfitting in polynomial regression', 'Overfitting demo', 'Overfitting for more general multiple regression models', 'Balancing fit and magnitude of coefficients', 'The resulting ridge objective and its extreme solutions', 'How ridge regression balances bias and variance', 'Ridge regression demo', 'The ridge coefficient path', 'Computing the gradient of the ridge objective', 'Approach 1: closed-form solution', 'Discussing the closed-form solution', 'Approach 2: gradient descent', 'Selecting tuning parameters via cross validation', 'K-fold cross validation', 'How to handle the intercept', 'A brief recap'], ['The feature selection task', 'All subsets', 'Complexity of all subsets', 'Greedy algorithms', 'Complexity of the greedy forward stepwise algorithm', 'Can we use regularization for feature selection?', 'Thresholding ridge coefficients?', 'The lasso objective and its coefficient path', 'Visualizing the ridge cost', 'Visualizing the ridge solution', 'Visualizing the lasso cost and solution', 'Lasso demo', 'What makes the lasso objective different', 'Coordinate descent', 'Normalizing features', 'Coordinate descent for least squares regression (normalized features)', 'Coordinate descent for lasso (normalized features)', 'Assessing convergence and other lasso solvers', 'Coordinate descent for lasso (unnormalized features)', 'Deriving the lasso coordinate descent update', 'Choosing the penalty strength and other practical issues with lasso', 'A brief recap'], ['Limitations of parametric regression', '1-Nearest neighbor regression approach', 'Distance metrics', '1-Nearest neighbor algorithm', 'k-Nearest neighbors regression', 'k-Nearest neighbors in practice', 'Weighted k-nearest neighbors', 'From weighted k-NN to kernel regression', 'Global fits of parametric models vs. local fits of kernel regression', 'Performance of NN as amount of data grows', 'Issues with high-dimensions, data scarcity, and computational complexity', 'k-NN for classification', 'A brief recap'], ['Simple and multiple regression', 'Assessing performance and ridge regression', 'Feature selection, lasso, and nearest neighbor regression', "What we covered and what we didn't cover", 'Thank you!']]</course_sub_chapter><course_time>Approx. 36 hours to complete</course_time><reviews>['Excellent course, the professors made it very easy to learn quite powerful technics like gradient descend and coordinate descend. I always saw them like black-boxes, but now, thanks to this course I not only understand how they really work, but I learned how to apply them to real data. This course was simply awesome.', 'I enjoy the lectures. The professor has a good speaking and teaching style which keeps me interested. Lots of concrete math examples which make it easier to understand. Very good slides which are well formulated and easy to understand', 'I loved this course because of the detail understanding of the concepts. I was looking for a course which provide detail understanding of algorithms, and here I am. I am giving four stars for what has been given in detail, not five because I something is left ;) interpretation.. ', '4.9 Stars really but had to round. Really enjoyable course and extremely well presented. As a working statistician/analyst this stuff hits on a lot of the import underlying logic that needs to be in your head when looking at real world projects. The 0.1 star drop is because some of the language in the questions can be confusing, an easy fix. ', 'This course is extremely awesome! ', 'Extremely well designed course. ', 'Wow, just wow ! This course had a great scope, digging in on the concepts / methodologies that are crucial for regression, while at the same time discussing more general and always-present concepts of a machine learning task. A learning powerhouse ! I think i must pass it a second time, to really get into the details.', "Amazing course! Thoroughly enjoyed it, and really appreciated the level of detail in some of the theoretical concepts. Yet it also stayed within what's practically useful and had a good amount of hands-on implementation.   ", 'The mathematical proof and concept given behind lasso and ridge regression is awesome.', 'I must say it was great learning experiance. Everything releted to ML regression has been covered so eloquently.', 'please take care while framing assignment and quize question it is very difficult to understand what exactly u want us to do', 'Great course! Very good insight!', 'Great course.  You get to write the algorithms for OLS regressions, ridge regression, lasso regression, and for k-nearest neighbor models.  The instruction even includes some optional graduate-level videos on with more detailed explanations of how more advanced algorithms for solving the regressions may be developed (eg, subgradients for lasso regression).', "Excellent course that is the second in this specialization. It goes beyond the Foundations course and delves further into utilizing machine learning with regression based methods. The course also uses Python. There is some requirement that you should have some degree of familiarity with programming, although you can pick up some skills in coding in Python even if you are not familiar with it (- I wasn't familiar with Python much, although I am familiar with other languages). ", "It provided practical details the are not described to much in others' courses.", 'Thorough explanations of the essential concepts are provided! Valuable course and lectures.', 'I took this class long time ago and just revisited it today. Compared to other online class, this class has a lot details. I am satisfied with both the clarity and depth of the content.', 'Great in-depth coverage', 'regression best now', 'In Depth coverage of lot of concepts, fully enjoyed it! Recommended to anyone wanting to explore in depth concepts of regression.', 'Very good course. The lecturer is very good and the information is very comprehensive.', 'I really enjoyed learning through out this course. I did little bit struggle with Python but now I am a bot more confident to take on advanced programming in Python.', 'A great curse focused on understanding the mathematics of the algorithms, clearly explained and detailed. Contains "advanced" optional topics for further learning and forces you to program you own algorithms. ', 'Very good assignments. ', 'Very well-organized and clear. Learned a lot about regression.']</reviews><reviewers>['By leonardo d', 'By Jafed E', 'By Hiral P', 'By Phil O', 'By Mohamed A H', 'By Kunal T', 'By Ilias A', 'By Manuel G', 'By YASHKUMAR R T', 'By kripa s', 'By akashkr1498', 'By Jenhau C', 'By Christopher M', 'By Yamin A', 'By Zhongkai M', 'By Tahereh R', 'By Ling Z', 'By Ayush K', 'By Akash G', 'By Surendar R', 'By Refael J', 'By Konduri V', 'By Francisco J', 'By Pavan B', 'By Xue']</reviewers><review_date>['Oct 28, 2018', 'Jul 06, 2019', 'Oct 09, 2018', 'Dec 10, 2018', 'Nov 27, 2018', 'Dec 19, 2018', 'Dec 30, 2018', 'Jan 01, 2019', 'Mar 24, 2019', 'Mar 25, 2019', 'Mar 28, 2019', 'Mar 31, 2019', 'Jan 26, 2019', 'Feb 10, 2019', 'Feb 12, 2019', 'Apr 02, 2019', 'Apr 09, 2019', 'Mar 08, 2019', 'Mar 09, 2019', 'Dec 23, 2018', 'Dec 23, 2018', 'Dec 25, 2018', 'Jan 20, 2019', 'Jan 21, 2019', 'Dec 08, 2018']</review_date></doc>