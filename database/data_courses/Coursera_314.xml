<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_314</id><course_url>https://www.coursera.org/learn/predictive-analytics</course_url><course_name>Practical Predictive Analytics: Models and Methods</course_name><course_platform>Coursera</course_platform><course_instructor>Bill Howe</course_instructor><course_introduction>Statistical experiment design and analytics are at the heart of data science.  In this course you will design statistical experiments and analyze the results using modern methods.  You will also explore the common pitfalls in interpreting statistical arguments, especially those associated with big data.  Collectively, this course will help you internalize a core set of practical and effective machine learning methods and concepts, and apply them to solve some real world problems.

  
Learning Goals: After completing this course, you will be able to:
1. Design effective experiments and analyze the results
2. Use resampling methods to make clear and bulletproof statistical arguments without invoking esoteric notation
3. Explain and apply a core set of classification methods of increasing complexity (rules, trees, random forests), and associated optimization methods (gradient descent and variants)
4. Explain and apply a set of unsupervised learning concepts and methods
5. Describe the common idioms of large-scale graph analytics, including structural query, traversals and recursive queries, PageRank, and community detection</course_introduction><course_category>Browse.Data Science.Data Analysis</course_category><course_tag>Random Forest//Predictive Analytics//Machine Learning//R Programming</course_tag><course_rating>4.1</course_rating><course_orgnization>University of Washington</course_orgnization><course_chapter>Practical Statistical Inference//Supervised Learning//Optimization//Unsupervised Learning</course_chapter><course_sub_chapter>[['Appetite Whetting: Bad Science', 'Hypothesis Testing', 'Significance Tests and P-Values', 'Example: Difference of Means', 'Deriving the Sampling Distribution', 'Shuffle Test for Significance', 'Comparing Classical and Resampling Methods', 'Bootstrap', 'Resampling Caveats', 'Outliers and Rank Transformation', 'Example: Chi-Squared Test', 'Bad Science Revisited: Publication Bias', 'Effect Size', 'Meta-analysis', "Fraud and Benford's Law", "Intuition for Benford's Law", "Benford's Law Explained Visually", 'Multiple Hypothesis Testing: Bonferroni and Sidak Corrections', 'Multiple Hypothesis Testing: False Discovery Rate', 'Multiple Hypothesis Testing: Benjamini-Hochberg Procedure', 'Big Data and Spurious Correlations', 'Spurious Correlations: Stock Price Example', 'How is Big Data Different?', 'Bayesian vs. Frequentist', 'Motivation for Bayesian Approaches', "Bayes' Theorem", "Applying Bayes' Theorem", 'Naive Bayes: Spam Filtering'], ['Statistics vs. Machine Learning', 'Simple Examples', 'Structure of a Machine Learning Problem', 'Classification with Simple Rules', 'Learning Rules', 'Rules: Sequential Covering', 'Rules Recap', 'From Rules to Trees', 'Entropy', 'Measuring Entropy', 'Using Information Gain to Build Trees', 'Building Trees: ID3 Algorithm', 'Building Trees: C.45 Algorithm', 'Rules and Trees Recap', 'Overfitting', 'Evaluation: Leave One Out Cross Validation', 'Evaluation: Accuracy and ROC Curves', 'Bootstrap Revisited', 'Ensembles, Bagging, Boosting', 'Boosting Walkthrough', 'Random Forests', 'Random Forests: Variable Importance', 'Summary: Trees and Forests', 'Nearest Neighbor', 'Nearest Neighbor: Similarity Functions', 'Nearest Neighbor: Curse of Dimensionality'], ['Optimization by Gradient Descent', 'Gradient Descent Visually', 'Gradient Descent in Detail', 'Gradient Descent: Questions to Consider', 'Intuition for Logistic Regression', 'Intuition for Support Vector Machines', 'Support Vector Machine Example', 'Intuition for Regularization', 'Intuition for LASSO and Ridge Regression', 'Stochastic and Batched Gradient Descent', 'Parallelizing Gradient Descent'], ['Introduction to Unsupervised Learning', 'K-means', 'DBSCAN', 'DBSCAN Variable Density and Parallel Algorithms']]</course_sub_chapter><course_time>Approx. 13 hours to complete</course_time><reviews>['V', 'Nice course', 'Professor Bill Howe gives great reactions to when there are typos on the slides!', 'Fantastic course! Excellent conceptual teaching for people who already know the subject but need some more clarity on how to approach statistical tests and machine learning.', 'The topic the professor covers are awesome. Going from statistics to machine learning is something very awesome about this course', '  Excellent thoughts and concepts presented.  ', 'Its a great review course. Prior knowledge is necessary', 'Nive that the course covered a broad range of topics.', 'Excellent course with amazing practical exercises!', None, 'I enjoy this course. The delivery and the course topics were very interesting. I learnt a lot and peer reviewing other people assignments is a great learning opportunity .', 'Its Hard! but AWESOME, some much info packed in a few lectures!', 'A quick overview of technology terms used for Machine Learning, and gentle introduction into learning through Kaggle. ', 'Very nice assignments and content. You learn a lot when you complete all assignments.', 'Great course!', 'Excellent!!', 'great for learner', 'I can feel Prof. Howe tried to cover as much as possible and to build a foundation for both practicing as well as further study on the topics. However, I do feel it is not patient enough to give a detailed yet easy-to-follow explanation for some of the topics, and I had to do quite some self-readings to close the gap. I think it will be helpful if the course can provide some reading materials on how some of the formulas are derived (e.g. gradient descent, logistic regression etc.) as a supplement.', 'Too little people participated and long peer review time.', "Great Course but the assigment don't show the understanding of the course", 'Very good approach to each method; the assignments are a good test for the topics.', "Excellent Lectures.  Since the course is several years old the organization of some of the assignments needs updating.  That's the only reason I gave it 4 instead of 5 stars.", 'Excellent crash course in machine learning and introduction to the kaggle data science competitions. However, the grading system had bugs and was unable to accept two answers as correct making it very frustrating. The grader was finally fixed so next round of this course should be a better experience.', 'More dynamic visualisation please, and it will be 5*.', 'The entire course is an overview! This course  will be a revision if you already know the concepts.']</reviews><reviewers>['By Anand P', 'By Yogesh B N', 'By Shota M', 'By Seema P', 'By prasad v', 'By Shivanand R K', 'By Tamal R', 'By Chen', 'By Artur S', 'By Balaji N', 'By Kenneth P', 'By francisco y', 'By SIEW W L', 'By Kevin R', 'By Daniel A', 'By Sergio G', 'By Menghe L', 'By Yifei G', 'By Bingcheng L', 'By Antonio P L', 'By Roberto S', 'By William L K', 'By Jason M', 'By Zoltan P', 'By Harini D']</reviewers><review_date>['Feb 11, 2019', 'Feb 20, 2019', 'Feb 24, 2016', 'Dec 23, 2016', 'Nov 12, 2015', 'Jun 18, 2016', 'Feb 17, 2016', 'Jul 20, 2016', 'Nov 24, 2015', 'Nov 16, 2015', 'Feb 08, 2016', 'Jan 19, 2016', 'Jun 06, 2016', 'Nov 11, 2015', 'Nov 23, 2015', 'Oct 30, 2017', 'Jun 12, 2017', 'Jun 26, 2019', 'Aug 07, 2019', 'Jan 08, 2016', 'Jun 13, 2017', 'Jun 06, 2017', 'Dec 19, 2015', 'Dec 23, 2015', 'Aug 31, 2016']</review_date></doc>