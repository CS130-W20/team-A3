<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_144</id><course_url>https://www.coursera.org/learn/ml-clustering-and-retrieval</course_url><course_name>Machine Learning: Clustering &amp; Retrieval</course_name><course_platform>Coursera</course_platform><course_instructor>Emily Fox</course_instructor><course_introduction>Case Studies: Finding Similar Documents

A reader is interested in a specific news article and you want to find similar articles to recommend.  What is the right notion of similarity?  Moreover, what if there are millions of other documents?  Each time you want to a retrieve a new document, do you need to search through all other documents?  How do you group similar documents together?  How do you discover new, emerging topics that the documents cover?   

In this third case study, finding similar documents, you will examine similarity-based algorithms for retrieval.  In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA).  You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce.

Learning Outcomes:  By the end of this course, you will be able to:
   -Create a document retrieval system using k-nearest neighbors.
   -Identify various similarity metrics for text data.
   -Reduce computations in k-nearest neighbor search by using KD-trees.
   -Produce approximate nearest neighbors using locality sensitive hashing.
   -Compare and contrast supervised and unsupervised learning tasks.
   -Cluster documents by topic using k-means.
   -Describe how to parallelize k-means using MapReduce.
   -Examine probabilistic clustering approaches using mixtures models.
   -Fit a mixture of Gaussian model using expectation maximization (EM).
   -Perform mixed membership modeling using latent Dirichlet allocation (LDA).
   -Describe the steps of a Gibbs sampler and how to use its output to draw inferences.
   -Compare and contrast initialization techniques for non-convex optimization objectives.
   -Implement these techniques in Python.</course_introduction><course_category>Browse.Data Science.Data Analysis</course_category><course_tag>Data Clustering Algorithms//K-Means Clustering//Machine Learning//K-D Tree</course_tag><course_rating>4.6</course_rating><course_orgnization>University of Washington</course_orgnization><course_chapter>Welcome//Nearest Neighbor Search//Clustering with k-means//Mixture Models//Mixed Membership Modeling via Latent Dirichlet Allocation//Hierarchical Clustering &amp; Closing Remarks</course_chapter><course_sub_chapter>[['Welcome and introduction to clustering and retrieval tasks', 'Course overview', 'Module-by-module topics covered', 'Assumed background'], ['Retrieval as k-nearest neighbor search', '1-NN algorithm', 'k-NN algorithm', 'Document representation', 'Distance metrics: Euclidean and scaled Euclidean', 'Writing (scaled) Euclidean distance using (weighted) inner products', 'Distance metrics: Cosine similarity', 'To normalize or not and other distance considerations', 'Complexity of brute force search', 'KD-tree representation', 'NN search with KD-trees', 'Complexity of NN search with KD-trees', 'Visualizing scaling behavior of KD-trees', 'Approximate k-NN search using KD-trees', 'Limitations of KD-trees', 'LSH as an alternative to KD-trees', 'Using random lines to partition points', 'Defining more bins', 'Searching neighboring bins', 'LSH in higher dimensions', '(OPTIONAL) Improving efficiency through multiple tables', 'A brief recap'], ['The goal of clustering', 'An unsupervised task', 'Hope for unsupervised learning, and some challenge cases', 'The k-means algorithm', 'k-means as coordinate descent', 'Smart initialization via k-means++', 'Assessing the quality and choosing the number of clusters', 'Motivating MapReduce', 'The general MapReduce abstraction', 'MapReduce execution overview and combiners', 'MapReduce for k-means', 'Other applications of clustering', 'A brief recap'], ['Motiving probabilistic clustering models', 'Aggregating over unknown classes in an image dataset', 'Univariate Gaussian distributions', 'Bivariate and multivariate Gaussians', 'Mixture of Gaussians', 'Interpreting the mixture of Gaussian terms', 'Scaling mixtures of Gaussians for document clustering', 'Computing soft assignments from known cluster parameters', "(OPTIONAL) Responsibilities as Bayes' rule", 'Estimating cluster parameters from known cluster assignments', 'Estimating cluster parameters from soft assignments', 'EM iterates in equations and pictures', 'Convergence, initialization, and overfitting of EM', 'Relationship to k-means', 'A brief recap'], ['Mixed membership models for documents', 'An alternative document clustering model', 'Components of latent Dirichlet allocation model', 'Goal of LDA inference', 'The need for Bayesian inference', 'Gibbs sampling from 10,000 feet', 'A standard Gibbs sampler for LDA', 'What is collapsed Gibbs sampling?', 'A worked example for LDA: Initial setup', 'A worked example for LDA: Deriving the resampling distribution', 'Using the output of collapsed Gibbs sampling', 'A brief recap'], ['Module 1 recap', 'Module 2 recap', 'Module 3 recap', 'Module 4 recap', 'Why hierarchical clustering?', 'Divisive clustering', 'Agglomerative clustering', 'The dendrogram', 'Agglomerative clustering details', 'Hidden Markov models', "What we didn't cover", 'Thank you!']]</course_sub_chapter><course_time>Approx. 48 hours to complete</course_time><reviews>["I enrolled in this specialization to learn machine learning using GraphLab Create. Half way into the specialization the creators sold Turi, GrapLab's parent company, making it non available to the general public (not even by paying) and then all the knowledge devalued. I wish I had known this and I would have enrolled on a different specialization. The creators still give you the possibility of using numpy, scikit learn and pandas but I had already done a lot with GraphLab create. The time I invested on my nights after work became a waste. I was trying to convince the company I worked for to buy licenses for GraphLab create. ", "If you are considering this specialization I would recommend the Andrew Ng course instead and the main reason is that it isn't depend on proprietary ML framework. Despite the good lectures, the assignments don't help you develop the knowledge required for ML developer role.", 'I like the course very much. I learnt so many advance concept and real life implementation.. but slightly disappointed by the quiz question please be specific what you wanted us to answer. looking forward for SVM and deep learning material.', "The materials presented are excellent with well prepared skeleton codes for all ML models. Comparing this course to its three preceding ones, this course is more challenging both conceptually and computationally. The slight drawback is that, because of the highly technical nature of the last three weeks' materials, there isn't enough guidance about how one may construct the ML algorithms from scratch, that is, learners with less experience in computing will, more or less, have to accept the sample codes with little confidence about how to (re)write such codes in the first place.", 'This course rushed through the material at the end. ', 'I found this Course less well prepared than the previous 3 modules. Misleading hints in the assignments, code errors, etc... Also, I found the amount of work required higher, which is not in itself a bad thing, just a bit unexpected.', 'The course, and indeed the whole specialization, was advertised as not requiring the Graphlab Create toolkit. This is untrue, as the final programming assignment does require it. The general dependence on SFrame is understandable since it is open source, but requiring any interaction with a licensed product (even if temporary and research licenses are available) greatly negatively impacted my experience in this course. ', 'Excellent course, which gives you all you need to learn about machine learning. Concepts and hands on practical ex', 'Great but hard~!', "I'd bring the last summary video at the beginning (the great summary of all weeks of the course). This would outline the course evolution in advance and give guidance what's ahead. IMHO this would help to not get lost when drill down in a single section.", 'Excellent course material and fantastic delivery. You guys made this complex learning so simple and interesting . Thanks for all this, keep the good works.', 'Excellent course with great and reachable explanation', 'It was great but I was also interested to implement the solutions with pyspark...though I did it eventually. Thank you!', 'session was very helpful &amp; full with relevant contents ', 'G', 'Excellent.', 'This course is very useful to know about the concepts of machine learning and do hands-on activities.', 'Great assignments : )', '\n\nExcellent, good contribution to the technical and practical knowledge ML  ', 'The course is really helpful, though it would be better for teacher to illustrate the concepts by using examples, instead of abstract terminologies', 'Very nice course. Things are well explained, however some concepts could be expanded more.', 'Machine Learning: Clustering &amp; Retrieval good and learn easily', 'Nice content and well made presentations.', 'The teacher is awesome', 'Emily is an extremely awesome instructor. For those who have some background in statistics, biostats , econometrics and math and want to study machine learning by themselves, these modules can be an outline that introduce basic topics in machine learning. ']</reviews><reviewers>['By Ernie M', 'By Eugene K', 'By akashkr1498', 'By Tsz W K', 'By Hamel H', 'By Andr√© F d A F C', 'By James F', 'By Somu P', 'By Xue', 'By Martin R', 'By Jay K S', 'By KAI N', 'By Vikash S N', 'By Manoj K', 'By Nagendra K M R', 'By VITTE', 'By Susree S M', 'By Zhongkai M', 'By Edwin P', 'By Jialie ( Y', 'By Sathiraju E', 'By Akash G', 'By PRAVEEN R U', 'By Juan F H', 'By Feng G']</reviewers><review_date>['Sep 25, 2017', 'Feb 10, 2017', 'Jul 08, 2019', 'May 15, 2017', 'Aug 07, 2016', 'Jul 25, 2016', 'Aug 10, 2016', 'Nov 17, 2018', 'Dec 19, 2018', 'Dec 12, 2018', 'Jan 05, 2019', 'Jan 03, 2019', 'Feb 03, 2019', 'Nov 26, 2018', 'Nov 11, 2018', 'Nov 11, 2018', 'Nov 14, 2018', 'Feb 12, 2019', 'Feb 15, 2019', 'Feb 21, 2019', 'Mar 03, 2019', 'Mar 11, 2019', 'Dec 27, 2018', 'Nov 15, 2018', 'Aug 09, 2018']</review_date></doc>