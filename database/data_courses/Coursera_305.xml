<?xml version='1.0' encoding='utf-8'?>
<doc><id>Coursera_305</id><course_url>https://www.coursera.org/learn/ntumlone-algorithmicfoundations</course_url><course_name>機器學習基石下 (Machine Learning Foundations)---Algorithmic Foundations</course_name><course_platform>Coursera</course_platform><course_instructor>林軒田</course_instructor><course_introduction>Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algorithmic, theoretical and practical tools that any user of machine learning needs to know. This second course of the two would focus more on algorithmic tools, and the other course would focus more on mathematical tools. [機器學習旨在讓電腦能由資料中累積的經驗來自我進步。我們的兩項姊妹課程將介紹各領域中的機器學習使用者都應該知道的基礎演算法、理論及實務工具。本課程將較為著重方法類的工具，而另一課程將較為著重數學類的工具。]</course_introduction><course_category>Browse.Data Science.Machine Learning</course_category><course_tag /><course_rating>4.9</course_rating><course_orgnization>National Taiwan University</course_orgnization><course_chapter> 第九講: Linear Regression//第十講: Logistic Regression//第十一講: Linear Models for Classification//第十二講: Nonlinear Transformation//第十三講: Hazard of Overfitting//第十四講: Regularization//第十五講: Validation//第十六講: Three Learning Principles</course_chapter><course_sub_chapter>[['Linear Regression Problem', 'Linear Regression Algorithm', 'Generalization Issue', 'Linear Regression for Binary Classification'], ['Logistic Regression Problem', 'Logistic Regression Error', 'Gradient of Logistic Regression Error', 'Gradient Descent'], ['Linear Models for Binary Classification', 'Stochastic Gradient Descent', 'Multiclass via Logistic Regression', 'Multiclass via Binary Classification'], ['Quadratic Hypothesis', 'Nonlinear Transform', 'Price of Nonlinear Transform', 'Structured Hypothesis Sets'], ['What is Overfitting?', 'The Role of Noise and Data Size', 'Deterministic Noise', 'Dealing with Overfitting'], ['Regularized Hypothesis Set', 'Weight Decay Regularization', 'Regularization and VC Theory', 'General Regularizers'], ['Model Selection Problem', 'Validation', 'Leave-One-Out Cross Validation', 'V-Fold Cross Validation'], ["Occam's Razor", 'Sampling Bias', 'Data Snooping', 'Power of Three']]</course_sub_chapter><course_time>Approx. 12 hours to complete</course_time><reviews>['很好的课程，更加注重算法的理论推导，当然也不乏运用的技巧。之前看过吴恩达老师的机器学习课程，感觉林老师这门课更加的深入，吴恩达老师的课省去了公式的推导，更偏向工程的实践，两门课可以算是相辅相成的。', '透過這次的測驗可以有助於了解自己的觀念是否還夠清楚，以及有沒有什麼地方需要再加強的。', '林老師的課不僅聽起來比較清晰易懂，並且深度足夠（比Andrew Ng的課而言深度要大不少），值得多次聽講。作業質量也比較高，能夠有很好的鍛煉效果。期待後續的技法課程能夠在coursera上面公佈。', '课程内容做到了理论与实践兼顾，作业内容非常棒，对进一步深化理解课程很有帮助', '深入淺出，一個讚字，唯一美中不足是中文字幕偶爾有錯誤。', '课程设计的极好！', "Very good course for exactly what it's for - theoretical foundation. I wish there were more courses that aim at applying these techniques in practice with actual problems.", '最佳机器学习入门课程', '最早知道林老师的机器学习基石和技法课程，还是Coursera改版之前的事。当时还在上学，挑选着章节，把林老师的讲解作为课堂内容的补充理解来听，当时就觉得林老师讲的特别棒，课堂上迷迷糊糊的内容来这里听一遍就好很多。现在工作了，终于能抽出时间来把整个课程完整的跟下来，仍然觉得受益匪浅。真的非常感谢林老师能够制作出这样用心的的课程，放在网上与大家分享，也感谢您和TA们从过去到现在一直关注着讨论区的动态，为大家答疑。谢谢！', '感谢老师的付出~^^', '對兩學分來說 loading 好重ＯＡＯ', 'Best ML course I have ever taken!', 'A great course', 'What an amazing course! I hope professor can give new courses in the future and cover more practical things with so hard theoretical things.', 'great!', '  Nice course, excellent course design.    ', 'good, but quiz is not enough.', '機器學習必學課程', 'Very helpful!', 'good explaination  the foundation of all ML models  ', 'best', '谢谢老师助教们的付出！', 'good job 希望技法的课也能开呀。', 'gj', 'Very interesting course for me！ I love it very much.']</reviews><reviewers>['By Jeff', 'By Jeremy L', 'By lcy9086', 'By Ho K', 'By Andrew H', 'By QIQING', 'By ZIAN X', 'By Yuan W', 'By Yijie Q', 'By Yixin B', 'By 陳約廷', 'By Yen, Y', 'By 蕭敬霖', 'By Harry L', 'By 王阳', 'By 林澤佑', 'By Chen X', 'By 曾令燊', 'By Rui Z', 'By Wang B', 'By Deleted A', 'By hexinlin', 'By 闻道暮东', 'By 粘耕毓', 'By t_xinxishijie']</reviewers><review_date>['Oct 03, 2018', 'Dec 03, 2017', 'Apr 18, 2018', 'Jun 06, 2019', 'Dec 09, 2018', 'Dec 11, 2018', 'Feb 04, 2019', 'Feb 04, 2019', 'Nov 26, 2018', 'Jan 12, 2019', 'Feb 22, 2019', 'Aug 27, 2018', 'Sep 14, 2018', 'Dec 05, 2017', 'Feb 05, 2018', 'Feb 18, 2018', 'Jan 29, 2018', 'Jul 01, 2018', 'Feb 10, 2018', 'Jun 07, 2018', 'Feb 27, 2018', 'Jul 03, 2018', 'May 25, 2018', 'Mar 10, 2018', 'Mar 15, 2018']</review_date></doc>