In this course, you will learn how to analyze map <phrase>data</phrase> using different <phrase>data</phrase> types and methods to answer geographic questions. First, you will learn how to <phrase>filter</phrase> a <phrase>data</phrase> set using different types of queries to find just the <phrase>data</phrase> you need to answer a particular question. Then, we will discuss simple yet powerful analysis methods that use <phrase>vector</phrase> <phrase>data</phrase> to find spatial relationships within and between <phrase>data</phrase> sets. In this section, you will also learn about how to use ModelBuilder, a simple but powerful tool for building analysis flowcharts that can then also be <phrase>run</phrase> as models. You will then learn how to find, understand, and use remotely sensed <phrase>data</phrase> such as <phrase>satellite</phrase> imagery, as a rich source of <phrase>GIS</phrase> <phrase>data</phrase>. You will then learn how to analyze <phrase>raster</phrase> <phrase>data</phrase>. Finally, you will complete your own project where you get to try out the new skills and tools you have learned about in this course.
Наука о данных включает большой спектр подходов и методов сбора, обработки, анализа и визуализации массивов данных любого размера. Отдельным практически важным направлением данной науки является работа с большими данными с помощью новых принципов математического и вычислительного моделирования, когда классические методы перестают работать ввиду невозможности их масштабирования. Настоящий курс призван помочь обучающемуся изучить основы предметной области через постановку и решение типичных задач, с которыми исследователь в области науки о данных может столкнуться в своей работе. Чтобы научить слушателя решать такие задачи, авторы курса предоставляют обучающемуся необходимый теоретический минимум и показывают, как пользоваться инструментальной базой на практике.
Learn how <phrase>probability</phrase>, <phrase>math</phrase>, and <phrase>statistics</phrase> can be used to help <phrase>baseball</phrase>, <phrase>football</phrase> and <phrase>basketball</phrase> teams improve, player and lineup selection as well as in <phrase>game</phrase> strategy.
The third course in this specialization is Achieving Advanced Insights with BigQuery. Here we will build on your growing <phrase>knowledge</phrase> of <phrase>SQL</phrase> as we dive into advanced functions and how to break apart a complex query into manageable steps.   We will <phrase>cover</phrase> the internal <phrase>architecture</phrase> of BigQuery (column-based sharded storage) and advanced <phrase>SQL</phrase> topics like nested and repeated fields through the use of Arrays and Structs. Lastly we will dive into optimizing your queries for performance and how you can secure your <phrase>data</phrase> through authorized views.  >>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
In this course you have the opportunity to use the skills you acquired in the two <phrase>SAS</phrase> <phrase>programming</phrase> courses to solve realistic problems. This course is also designed to give you a thorough review of <phrase>SAS</phrase> <phrase>programming</phrase> concepts so you are prepared to take the <phrase>SAS</phrase> Certified Specialist: Base <phrase>Programming</phrase> Using <phrase>SAS</phrase> 9.4 Exam.
This course will <phrase>introduce</phrase> the learner to applied <phrase>machine learning</phrase>, focusing more on the techniques and methods than on the <phrase>statistics</phrase> behind these methods. The course will start with a discussion of how <phrase>machine learning</phrase> is different than <phrase>descriptive statistics</phrase>, and <phrase>introduce</phrase> the <phrase>scikit learn</phrase> <phrase>toolkit</phrase> through a <phrase>tutorial</phrase>. The issue of dimensionality of <phrase>data</phrase> will be discussed, and the <phrase>task</phrase> of clustering <phrase>data</phrase>, as well as evaluating those clusters, will be tackled. Supervised approaches for creating predictive models will be described, and learners will be able to apply the <phrase>scikit learn</phrase> predictive modelling methods while understanding process issues related to <phrase>data</phrase> generalizability (e.g. <phrase>cross validation</phrase>, <phrase>overfitting</phrase>). The course will end with a look at more advanced techniques, such as building ensembles, and practical limitations of predictive models. By the end of this course, students will be able to identify the difference between a supervised (classification) and unsupervised (clustering) technique, identify which technique they need to apply for a particular dataset and need, <phrase>engineer</phrase> features to meet that need, and write <phrase>python</phrase> code to <phrase>carry out</phrase> an analysis.   This course should be taken after Introduction to <phrase>Data Science</phrase> in <phrase>Python</phrase> and Applied Plotting, Charting & <phrase>Data</phrase> Representation in <phrase>Python</phrase> and before Applied <phrase>Text Mining</phrase> in <phrase>Python</phrase> and Applied Social Analysis in <phrase>Python</phrase>.
Do you have <phrase>data</phrase> and wonder what it can tell you?  Do you need a deeper understanding of the core ways in which <phrase>machine learning</phrase> can improve your <phrase>business</phrase>?  Do you want to be able to converse with specialists about anything from <phrase>regression</phrase> and classification to <phrase>deep learning</phrase> and <phrase>recommender systems</phrase>?  In this course, you will get hands-on experience with <phrase>machine learning</phrase> from a series of practical <phrase>case</phrase>-studies.  At the end of the first course you will have studied how to predict <phrase>house</phrase> prices based on <phrase>house</phrase>-level features, analyze sentiment from user reviews, retrieve documents of interest, recommend <phrase>products</phrase>, and search for images.  Through hands-on practice with these <phrase>use cases</phrase>, you will be able to apply <phrase>machine learning</phrase> methods in a wide <phrase>range</phrase> of domains.  This first course treats the <phrase>machine learning</phrase> <phrase>method</phrase> as a <phrase>black</phrase> box.  Using this abstraction, you will focus on understanding tasks of interest, matching these tasks to <phrase>machine learning</phrase> tools, and assessing the quality of the output. In subsequent courses, you will delve into the components of this <phrase>black</phrase> box by examining models and <phrase>algorithms</phrase>.  Together, these pieces form the <phrase>machine learning</phrase> <phrase>pipeline</phrase>, which you will use in developing intelligent applications.  <phrase>Learning Outcomes</phrase>:  By the end of this course, you will be able to:    -Identify <phrase>potential applications</phrase> of <phrase>machine learning</phrase> in practice.      -Describe the core differences in analyses <phrase>enabled</phrase> by <phrase>regression</phrase>, classification, and clustering.    -Select the appropriate <phrase>machine learning</phrase> <phrase>task</phrase> for a potential <phrase>application</phrase>.      -Apply <phrase>regression</phrase>, classification, clustering, retrieval, <phrase>recommender systems</phrase>, and <phrase>deep learning</phrase>.    -Represent your <phrase>data</phrase> as features to serve as input to <phrase>machine learning</phrase> models.     -Assess the <phrase>model</phrase> quality in terms of relevant error metrics for each <phrase>task</phrase>.    -Utilize a dataset to fit a <phrase>model</phrase> to analyze new <phrase>data</phrase>.    -Build an <phrase>end-to-end</phrase> <phrase>application</phrase> that uses <phrase>machine learning</phrase> at its core.      -Implement these techniques in <phrase>Python</phrase>.
Курс знакомит слушателей с основными принципами работы со структурированными данными в реляционной модели, учит проектировать данные, описывать объекты базы данных в терминах реальной СУБД, составлять запросы на языке <phrase>SQL</phrase>, использовать представления, процедуры, функции и триггеры, создавать индексы, управлять конкурентным доступом к данным и манипулировать механизмом транзакций. Основу курса составляют изучение и применение языка <phrase>SQL</phrase> для создания, модификации объектов баз данных и управления данными в произвольной реляционной базе данных. Выполнение практических задач в рамках курса предполагает использование СУБД My <phrase>SQL</phrase>. В курсе рассматриваются этапы проектирования реляционных баз данных, правила составления запросов, основные методы индексирования данных. В курсе будут изучены вопросы использования транзакций и прав доступа к данным. Также курс дает обзор современных тенденций в области науки о данных в связи с появлением BigData. В заключении курса будут показаны сферы применения <phrase>NoSQL</phrase> баз данных и указаны современные подходы к обработке <phrase>big data</phrase>.
This course aims to help you to draw better statistical inferences from <phrase>empirical research</phrase>. First, we will discuss how to correctly interpret p-values, effect sizes, <phrase>confidence intervals</phrase>, <phrase>Bayes</phrase> Factors, and likelihood ratios, and how these <phrase>statistics</phrase> answer different questions you might be interested in. Then, you will learn how to <phrase>design</phrase> experiments where the <phrase>false positive rate</phrase> is controlled, and how to decide upon the <phrase>sample size</phrase> for your study, for example in <phrase>order</phrase> to achieve <phrase>high</phrase> <phrase>statistical power</phrase>. Subsequently, you will learn how to interpret evidence in the <phrase>scientific literature</phrase> given widespread <phrase>publication bias</phrase>, for example by learning about p-curve analysis. Finally, we will <phrase>talk</phrase> about how to do <phrase>philosophy</phrase> of <phrase>science</phrase>, theory <phrase>construction</phrase>, and cumulative <phrase>science</phrase>, including how to perform replication studies, why and how to pre-register your experiment, and how to share your <phrase>results</phrase> following <phrase>Open Science</phrase> principles.   In practical, hands on assignments, you will learn how to simulate t-<phrase>tests</phrase> to learn which p-values you can expect, calculate likelihood ratio's and get <phrase>an introduction</phrase> the binomial <phrase>Bayesian statistics</phrase>, and learn about the positive predictive value which expresses the <phrase>probability</phrase> published <phrase>research</phrase> findings are true. We will experience the problems with optional stopping and learn how to prevent these problems by using sequential analyses. You will calculate effect sizes, see how <phrase>confidence intervals</phrase> work through simulations, and practice doing a-priori power analyses. Finally, you will learn how to examine whether the <phrase>null hypothesis</phrase> is true using equivalence testing and <phrase>Bayesian statistics</phrase>, and how to pre-register a study, and share your <phrase>data</phrase> on the <phrase>Open Science</phrase> Framework.  All videos now have <phrase>Chinese</phrase> subtitles. More than 10.000 learners have enrolled so far!
The course will teach you how to develop <phrase>deep learning</phrase> models using  Pytorch. The course will start with Pytorch's  tensors and <phrase>Automatic differentiation</phrase> package. Then each section will <phrase>cover</phrase> different models starting off with fundamentals such as <phrase>Linear Regression</phrase>, and logistic/softmax <phrase>regression</phrase>. Followed by  Feedforward <phrase>deep neural networks</phrase>, the role of different <phrase>activation functions</phrase>, normalization and dropout layers. Then <phrase>Convolutional Neural Networks</phrase> and Transfer learning will be <phrase>covered</phrase>. Finally, several other <phrase>Deep learning</phrase> methods will be <phrase>covered</phrase>. <phrase>Learning Outcomes</phrase>: After completing this course, learners will be able to: •	explain and apply their <phrase>knowledge</phrase> of <phrase>Deep Neural Networks</phrase> and related <phrase>machine learning</phrase> methods •	know how to use <phrase>Python</phrase> <phrase>libraries</phrase> such as PyTorch  for <phrase>Deep Learning</phrase> applications  •	build <phrase>Deep Neural Networks</phrase> using PyTorch
Seja <phrase>bem</phrase>-vindo à <phrase>arte</phrase> <phrase>e</phrase> à ciência do aprendizado de máquina. Neste curso, você aprenderá as habilidades básicas de intuição, bom senso <phrase>e</phrase> experimentação <phrase>em</phrase> aprendizado de máquina para ajustar <phrase>e</phrase> otimizar seus modelos com o objetivo de conseguir o melhor desempenho.      Além disso, você conhecerá as ferramentas envolvidas no processo de treinamento. Primeiro, você fará ajustes manuais para observar <phrase>os</phrase> efeitos no desempenho do modelo. Depois de <phrase>se</phrase> familiarizar com essas ferramentas, também conhecidas como hiperparâmetros, você aprenderá a ajustá-las automaticamente usando o <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> no <phrase>Google</phrase> <phrase>Cloud</phrase> Platform.
Are you trying to understand <phrase>data</phrase> from your <phrase>research</phrase>? Learn how and when to conduct mediation, moderation, and conditional indirect effects analyses? Or, perhaps, how to theorize and <phrase>test</phrase> your theoretical models? If so, this is the course for you! We will walk you through the steps of <phrase>conducting</phrase> multilevel analyses using a real dataset and provide articles and templates designed to facilitate your learning. You'll leave with the tools you need to analyze and interpret the <phrase>results</phrase> of the datasets you collect as a researcher.   By the end of this course, you will understand the <phrase>differences between</phrase> mediation and moderation and between moderated mediation and mediated moderation models (conditional indirect effects), and the importance of multilevel analysis. Most important, you will be able to <phrase>run</phrase> mediation, moderation, conditional indirect effect and multilevel models and interpret the <phrase>results</phrase>.  This course is supported by the BRAD Lab at the Darden <phrase>School</phrase> of <phrase>Business</phrase>, which studies <phrase>organizational behavior</phrase>, <phrase>marketing</phrase>, <phrase>business</phrase> <phrase>ethics</phrase>, judgment and <phrase>decision-making</phrase>, behavioral operations, and <phrase>entrepreneurship</phrase>, among other areas. More: http://www.darden.virginia.edu/brad-lab/
Este curso de una semana, acelerado y a pedido <phrase>se</phrase> basa <phrase>en</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals. Mediante una combinación de clases por <phrase>video</phrase>, demonstraciones y labs prácticos, aprenderá a crear canalizaciones de datos de transmisión usando <phrase>Pub</phrase>/Sub y <phrase>Dataflow</phrase> de <phrase>Google</phrase> <phrase>Cloud</phrase> a <phrase>fin</phrase> de permitir <phrase>la</phrase> toma de <phrase>decisiones</phrase> <phrase>en</phrase> tiempo real. También aprenderá a crear paneles para presentar resultados personalizados para diversas audiencias de partes interesadas.  Requisitos previos: • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals (o experiencia equivalente) • Cierto conocimiento sobre <phrase>Java</phrase>  Objetivos: • Comprender los casos prácticos de estadísticas de transmisión <phrase>en</phrase> tiempo real • Usar <phrase>el</phrase> servicio de mensajería asíncrona de PubSub de <phrase>Google</phrase> <phrase>Cloud</phrase> para administrar eventos de datos • Escribir canalizaciones de transmisión y ejecutar transformaciones cuando sea necesario • Familiarizarse con ambos extremos de una canalización de transmisión: producción y consumo • Interoperar <phrase>Dataflow</phrase>, BigQuery y <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub para lograr una transmisión y un análisis <phrase>en</phrase> tiempo real
Variability is a fact of <phrase>life</phrase> in <phrase>manufacturing</phrase> environments, impacting <phrase>product quality</phrase> and yield. Through this course, students will learn why performing advanced analysis of <phrase>manufacturing</phrase> processes is <phrase>integral</phrase> for diagnosing and correcting operational flaws in <phrase>order</phrase> to improve yields and <phrase>reduce costs</phrase>.     Gain insights into the best ways to collect, prepare and analyze <phrase>data</phrase>, as well as computational platforms that can be leveraged to collect and process <phrase>data</phrase> over sustained periods of time. Become better prepared to participate as a <phrase>member</phrase> of an advanced analysis team and share valuable inputs on effective implementation.      Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the fourth course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
Who is this course for ? This course is RESTRICTED TO LEARNERS ENROLLED IN  Strategic <phrase>Business</phrase> Analytics SPECIALIZATION as a preparation to the capstone project. During the first two MOOCs, we focused on specific techniques for <phrase>specific applications</phrase>. Instead, with this third <phrase>MOOC</phrase>, we provide you with different examples  to open your mind to different applications from different industries and sectors. The objective is to give you an <phrase>helicopter</phrase> overview on what's happening in this field. You will see how the tools presented in the two previous courses of the Specialization are used in <phrase>real life</phrase> projects.  We want to ignite your reflection process. Hence, you will best make use of the <phrase>Accenture</phrase> cases by watching first the <phrase>MOOC</phrase> and then investigate by yourself on the different concepts, industries, or challenges that are introduced during the videos.  At the end of this course learners will be able to:  - identify the possible applications of <phrase>business</phrase> analytics, - hence, reflect on the possible solutions and added-value applications that could be proposed for their capstone project.  The cases will be presented by senior practitioners from <phrase>Accenture</phrase> with different backgrounds in term of <phrase>industry</phrase>, <phrase>function</phrase>, and <phrase>country</phrase>.  Special attention will be paid to the "value <phrase>case</phrase>" of the issue raised to prepare you for the capstone project of the specialization.  About <phrase>Accenture</phrase> <phrase>Accenture</phrase> is a leading global <phrase>professional</phrase> services <phrase>company</phrase>, providing a broad <phrase>range</phrase> of services and solutions in strategy, consulting, <phrase>digital</phrase>, <phrase>technology</phrase> and operations. Combining unmatched experience and specialized skills across more than 40 industries and all <phrase>business</phrase> functions—underpinned by the world’s largest delivery network—<phrase>Accenture</phrase> works at the intersection of <phrase>business</phrase> and <phrase>technology</phrase> to help clients improve their performance and create <phrase>sustainable</phrase> value for their stakeholders. With more than 358,000 people serving clients in more than 120 countries, <phrase>Accenture</phrase> drives <phrase>innovation</phrase> to improve the way the world works and lives. Visit us at www.accenture.com.
Après avoir présenté un historique du <phrase>machine learning</phrase>, nous étudierons pourquoi <phrase>les</phrase> réseaux de neurones sont aujourd'hui parfaitement adaptés à diverses problématiques. Nous apprendrons ensuite à définir un problème d'apprentissage supervisé et à trouver une <phrase>solution</phrase> adaptée à l'aide d'une descente de <phrase>gradient</phrase>. <phrase>Ce</phrase> processus implique <phrase>la</phrase> création d'ensembles de données permettant <phrase>la</phrase> généralisation. Nous examinerons comment procéder à cette opération de façon reproductible de sorte que l'expérimentation soit possible.  Objectifs du cours : Déterminer pourquoi le <phrase>deep learning</phrase> est désormais <phrase>si</phrase> courant Optimiser et évaluer <phrase>des</phrase> modèles <phrase>en</phrase> utilisant <phrase>des</phrase> fonctions de perte et <phrase>des</phrase> statistiques de performances Corriger <phrase>les</phrase> problèmes courants liés <phrase>au</phrase> <phrase>machine learning</phrase> Créer <phrase>des</phrase> ensembles de données de formation, d'évaluation et de <phrase>test</phrase> reproductibles et évolutifs
Welcome to the specialization course <phrase>Business Intelligence</phrase> and <phrase>Data</phrase> Warehousing. This course will be completed on six weeks, it will be supported with videos and various documents that will allow you to learn in a very simple way how to identify, <phrase>design</phrase> and develop analytical <phrase>information</phrase> systems, such as <phrase>Business Intelligence</phrase> with a descriptive analysis on <phrase>data</phrase> warehouses. You will be able to understand the problem of <phrase>integration</phrase> and predictive analysis of <phrase>high</phrase> volume of <phrase>unstructured data</phrase> (<phrase>big data</phrase>) with <phrase>data mining</phrase> and the <phrase>Hadoop</phrase> framework.  After completing this course, a learner will be able to ●	Create a <phrase>Star</phrase> o <phrase>Snowflake</phrase> <phrase>data model</phrase> Diagram through the Multidimensional <phrase>Design</phrase> from analytical <phrase>business</phrase> requirements and OLTP system ●	Create a physical <phrase>database</phrase> system  ●	Extract, Transform and load <phrase>data</phrase> to a <phrase>data-warehouse</phrase>. ●	Program analytical queries with <phrase>SQL</phrase> using <phrase>MySQL</phrase> ●	Predictive analysis with RapidMiner ●	Load <phrase>relational</phrase> or <phrase>unstructured data</phrase> to Hortonworks HDFS ●	Execute Map-Reduce jobs to query <phrase>data</phrase> on HDFS for analytical purposes   <phrase>Programming languages</phrase>: For course 2 you will use the <phrase>MYSQL</phrase> <phrase>language</phrase>.  <phrase>Software</phrase> to download: Rapidminer <phrase>MYSQL</phrase> <phrase>Excel</phrase> Hortonworks <phrase>Hadoop</phrase> framework  In <phrase>case</phrase> you have a <phrase>Mac</phrase> / <phrase>IOS</phrase> <phrase>operating system</phrase> you will need to use a <phrase>virtual Machine</phrase> (<phrase>VirtualBox</phrase>, <phrase>Vmware</phrase>).
“By the end of this Course, you will be able to…”  •	Evaluate effective <phrase>leadership</phrase> styles for <phrase>leadership</phrase> in <phrase>nursing</phrase> informatics in clinical or <phrase>academic</phrase> contexts to improve <phrase>leadership</phrase> success. •	Discover core values that support effective <phrase>nursing</phrase> informatics <phrase>leadership</phrase> in <phrase>academic</phrase> and clinical contexts to inform development of a personal <phrase>leadership</phrase> mission statement. •	Discover competing values and polarities related to <phrase>knowledge</phrase> <phrase>leadership</phrase> and <phrase>management</phrase> to promote successful <phrase>leadership</phrase> collaboration. •	Determine your personal informatics <phrase>leadership</phrase> style based on <phrase>results</phrase> from the <phrase>Minnesota</phrase> Informatics <phrase>Leadership</phrase> Inventory to inform successful <phrase>leadership</phrase> practice.  •	Discuss the value and importance of <phrase>foresight</phrase> <phrase>leadership</phrase> in <phrase>nursing</phrase> informatics to anticipate trends and consequences that are likely to transform the learning <phrase>health care</phrase> system
Este curso acelerado de 1 semana profundiza <phrase>en</phrase> <phrase>el</phrase> contenido de los cursos anteriores <phrase>de la</phrase> especialización <phrase>en</phrase> <phrase>Data</phrase> <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Mediante una serie de videoconferencias, demostraciones y labs prácticos, aprenderá a crear y administrar clústeres de procesamiento para ejecutar trabajos de <phrase>Hadoop</phrase>, Spark, <phrase>Pig</phrase> o Hive <phrase>en</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. También aprenderá a acceder a las distintas opciones de <phrase>Google</phrase> <phrase>Cloud</phrase> Storage desde sus propios clústeres de procesamiento y a integrar las funciones de aprendizaje automático de <phrase>Google</phrase> <phrase>en</phrase> sus programas de estadísticas.     <phrase>En</phrase> los labs prácticos, creará y administrará clústeres de Dataproc con <phrase>la</phrase> consola web y <phrase>la</phrase> <phrase>CLI</phrase>, y los usará para ejecutar trabajos de Spark y <phrase>Pig</phrase>. Luego, creará notebooks de iPython que <phrase>se</phrase> integran <phrase>en</phrase> BigQuery y <phrase>Cloud</phrase> Storage y utilizan Spark. Por último, integrará las <phrase>API</phrase> de aprendizaje automático <phrase>en</phrase> sus análisis de datos.  Requisitos previos • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> & <phrase>Machine Learning</phrase> Fundamentals (o tener experiencia equivalente) • Cierto conocimiento sobre <phrase>Python</phrase>
The <phrase>Business</phrase> <phrase>Statistics</phrase> and Analysis Capstone is an opportunity to apply various skills developed across the four courses in the specialization to a <phrase>real life</phrase> <phrase>data</phrase>. The Capstone, in collaboration with an <phrase>industry</phrase> partner uses <phrase>publicly available</phrase> ‘Housing <phrase>Data</phrase>’ to pose various questions typically a client would pose to a <phrase>data</phrase> analyst. Your job is to do the relevant <phrase>statistical analysis</phrase> and <phrase>report</phrase> your findings in response to the questions in a way that anyone can understand. Please remember that this is a Capstone, and has a <phrase>degree</phrase> of difficulty/ambiguity higher than the previous four courses. The aim being to mimic a <phrase>real life</phrase> <phrase>application</phrase> as close as possible.
In this course you will learn a <phrase>variety</phrase> of matrix factorization and <phrase>hybrid</phrase> <phrase>machine learning</phrase> techniques for <phrase>recommender systems</phrase>.  Starting with <phrase>basic</phrase> matrix factorization, you will understand both the intuition and the practical details of building <phrase>recommender systems</phrase> based on reducing the dimensionality of the user-product preference space.  Then you will learn about techniques that combine the strengths of different <phrase>algorithms</phrase> into powerful <phrase>hybrid</phrase> recommenders.
Bienvenue dans cette formation sur l'art et <phrase>la</phrase> <phrase>science</phrase> du <phrase>machine learning</phrase>. Dans <phrase>ce</phrase> cours, vous allez acquérir <phrase>les</phrase> compétences essentielles que requiert le <phrase>ML</phrase> : intuition, discernement et capacités d'expérimentation. De cette façon, vous pourrez ajuster précisément vos modèles de <phrase>ML</phrase> et <phrase>les</phrase> améliorer pour obtenir <phrase>des</phrase> performances optimales.    <phrase>Ce</phrase> cours vous présente <phrase>les</phrase> nombreux mécanismes intervenant dans l'entraînement d'un modèle. Vous commencerez par <phrase>les</phrase> ajuster manuellement pour <phrase>observer</phrase> leurs effets sur <phrase>les</phrase> performances du modèle. Une fois que vous serez familiarisé avec ces mécanismes, également appelés "hyperparamètres", vous apprendrez à <phrase>les</phrase> régler automatiquement avec <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> sur <phrase>Google</phrase> <phrase>Cloud</phrase> Platform.
<phrase>Unlike traditional</phrase> <phrase>relational</phrase> <phrase>database management</phrase> systems, <phrase>NoSQL</phrase> <phrase>databases</phrase> are capable of storing <phrase>unstructured data</phrase>. They therefore not only meet the performance, <phrase>scalability</phrase>, and flexibility needs that <phrase>data</phrase>-intensive applications require but are essential to <phrase>big data</phrase> processing. This course covers main <phrase>NoSQL</phrase> <phrase>data management</phrase> systems topics such as key-value stores, <phrase>graph</phrase> <phrase>databases</phrase>, and document <phrase>databases</phrase>.
This course introduces simple and multiple <phrase>linear regression</phrase> models. These models allow you to assess the relationship between variables in a <phrase>data</phrase> set and a continuous response <phrase>variable</phrase>. Is there a relationship between the <phrase>physical attractiveness</phrase> of a <phrase>professor</phrase> and their <phrase>student</phrase> evaluation scores? Can we predict the <phrase>test</phrase> score for a child based on certain characteristics of his or her mother? In this course, you will learn the fundamental theory behind <phrase>linear regression</phrase> and, through <phrase>data</phrase> examples, learn to fit, examine, and utilize <phrase>regression</phrase> models to examine relationships between <phrase>multiple variables</phrase>, using the <phrase>free</phrase> statistical <phrase>software</phrase> R and RStudio.
We have all heard the phrase “correlation does not equal causation.”  What, then, does equal causation?  This course aims to answer that question and more!    Over a <phrase>period</phrase> of 5 weeks, you will learn how causal effects are defined, what assumptions about your <phrase>data</phrase> and models are necessary, and how to implement and interpret some popular <phrase>statistical methods</phrase>.  Learners will have the opportunity to apply these methods to example <phrase>data</phrase> in R (<phrase>free</phrase> statistical <phrase>software</phrase> environment).  At the end of the course, learners should be able to: 1.  Define causal effects using potential outcomes 2.  Describe the difference between association and causation 3.  Express assumptions with causal <phrase>graphs</phrase> 4.  Implement several types of causal inference methods (e.g. matching, <phrase>instrumental</phrase> variables, inverse <phrase>probability</phrase> of treatment weighting) 5.  Identify which causal assumptions are necessary for each type of statistical <phrase>method</phrase>  So join us.... and discover for yourself why modern <phrase>statistical methods</phrase> for estimating causal effects are indispensable in so many fields of study!
A good <phrase>algorithm</phrase> usually comes together with a set of good <phrase>data</phrase> structures that allow the <phrase>algorithm</phrase> to manipulate the <phrase>data</phrase> efficiently. In this course, we consider the common <phrase>data</phrase> structures that are used in various <phrase>computational problems</phrase>. You will learn how these <phrase>data</phrase> structures are implemented in different <phrase>programming</phrase> languages and will practice implementing them in our <phrase>programming</phrase> assignments. This will help you to understand what is going on inside a particular built-in implementation of a <phrase>data</phrase> structure and what to expect from it. You will also learn typical <phrase>use cases</phrase> for these <phrase>data</phrase> structures.  A few examples of questions that we are going to <phrase>cover</phrase> in this class are the following: 1. What is a good strategy of resizing a <phrase>dynamic array</phrase>? 2. How <phrase>priority queues</phrase> are implemented in <phrase>C++</phrase>, <phrase>Java</phrase>, and <phrase>Python</phrase>? 3. How to implement a <phrase>hash table</phrase> so that the amortized running time of all operations is O(1) on <phrase>average</phrase>? 4. What are good strategies to keep a <phrase>binary tree</phrase> balanced?   You will also learn how services like <phrase>Dropbox</phrase> manage to <phrase>upload</phrase> some <phrase>large files</phrase> instantly and to <phrase>save a lot</phrase> of <phrase>storage space</phrase>!  Do you have technical problems? Write to us: <phrase>coursera</phrase>@hse.ru
This course introduces you to <phrase>sampling</phrase> and exploring <phrase>data</phrase>, as well as <phrase>basic</phrase> <phrase>probability theory</phrase> and <phrase>Bayes</phrase>' rule. You will examine various types of <phrase>sampling</phrase> methods, and discuss how such methods can impact the scope of inference. A <phrase>variety</phrase> of <phrase>exploratory data analysis</phrase> techniques will be <phrase>covered</phrase>, including numeric <phrase>summary</phrase> <phrase>statistics</phrase> and <phrase>basic</phrase> <phrase>data</phrase> visualization. You will be guided through installing and using R and RStudio (<phrase>free</phrase> statistical <phrase>software</phrase>), and will use this <phrase>software</phrase> for lab exercises and a final project. The concepts and techniques in this course will serve as <phrase>building blocks</phrase> for the inference and modeling courses in the Specialization.
<phrase>Accounting</phrase> has always been about analytical thinking. From the earliest days of the profession, <phrase>Luca Pacioli</phrase> emphasized the importance of <phrase>math</phrase> and <phrase>order</phrase> for analyzing <phrase>business</phrase> transactions. The skillset that accountants have needed to perform <phrase>math</phrase> and to keep <phrase>order</phrase> has evolved from pencil and <phrase>paper</phrase>, to <phrase>typewriters</phrase> and <phrase>calculators</phrase>, then to <phrase>spreadsheets</phrase> and <phrase>accounting software</phrase>. A new skillset that is becoming more important for nearly every <phrase>aspect</phrase> of <phrase>business</phrase> is that of <phrase>big data</phrase> analytics: analyzing <phrase>large amounts of data</phrase> to find actionable insights. This course is designed to help <phrase>accounting</phrase> students develop an analytical mindset and prepare them to use <phrase>data</phrase> analytic <phrase>programming</phrase> languages like <phrase>Python</phrase> and R.   We’ve divided the course into three main sections. In the first section, we <phrase>bridge</phrase> <phrase>accountancy</phrase> to analytics. We identify how tasks in the five <phrase>major</phrase> subdomains of <phrase>accounting</phrase> (i.e., financial, managerial, <phrase>audit</phrase>, <phrase>tax</phrase>, and systems) have historically required an analytical mindset, and we then explore how those tasks can be  completed more effectively and efficiently by using <phrase>big data</phrase> analytics. We then present a FACT framework for guiding <phrase>big data</phrase> analytics: <phrase>Frame</phrase> a question, Assemble <phrase>data</phrase>, Calculate the <phrase>data</phrase>, and Tell others about the <phrase>results</phrase>.   In the second section of the course, we emphasize the importance of assembling <phrase>data</phrase>. Using <phrase>financial statement</phrase> <phrase>data</phrase>, we explain desirable characteristics of both <phrase>data</phrase> and datasets that will <phrase>lead</phrase> to effective calculations and visualizations.   In the third, and largest section of the course, we demonstrate and explore how <phrase>Excel</phrase> and Tableau can be used to analyze <phrase>big data</phrase>. We describe <phrase>visual perception</phrase> principles and then apply those principles to create effective visualizations. We then examine fundamental <phrase>data</phrase> analytic tools, such as <phrase>regression</phrase>, <phrase>linear programming</phrase> (using <phrase>Excel</phrase> Solver), and clustering in the <phrase>context</phrase> of <phrase>point of sale</phrase> <phrase>data</phrase> and loan <phrase>data</phrase>. We conclude by demonstrating the power of <phrase>data</phrase> analytic <phrase>programming</phrase> languages to assemble, visualize, and analyze <phrase>data</phrase>. We <phrase>introduce</phrase> <phrase>Visual Basic</phrase> for Applications  as an example of a <phrase>programming language</phrase>, and the <phrase>Visual Basic</phrase> <phrase>Editor</phrase> as an example of an <phrase>integrated development environment</phrase> (<phrase>IDE</phrase>).
This course offers a <phrase>rigorous mathematical</phrase> survey of causal inference at the Master’s level.  Inferences about causation are of great importance in <phrase>science</phrase>, <phrase>medicine</phrase>, policy, and <phrase>business</phrase>.  This course provides <phrase>an introduction</phrase> to the statistical <phrase>literature</phrase> on causal inference that has emerged in the last 35-40 years and that has revolutionized the way in which statisticians and applied researchers in many disciplines use <phrase>data</phrase> to make inferences about <phrase>causal relationships</phrase>.    We will study methods for collecting <phrase>data</phrase> to estimate <phrase>causal relationships</phrase>. Students will learn how to distinguish between relationships that are causal and non-causal; this is not always obvious. We shall then study and evaluate the various methods students can use — such as matching, sub-classification on the propensity score, inverse <phrase>probability</phrase> of treatment weighting, and <phrase>machine learning</phrase> — to estimate a <phrase>variety</phrase> of effects — such as the <phrase>average</phrase> treatment effect and the effect of treatment on the treated. At the end, we discuss methods for evaluating some of the assumptions we have made, and we offer a look <phrase>forward</phrase> to the extensions we take up in the <phrase>sequel</phrase> to this course.
What are the <phrase>ethical</phrase> considerations regarding the <phrase>privacy</phrase> and control of <phrase>consumer</phrase> <phrase>information</phrase> and <phrase>big data</phrase>, especially in the aftermath of recent <phrase>large-scale</phrase> <phrase>data</phrase> breaches?  This course provides a framework to analyze these concerns as you examine the <phrase>ethical</phrase> and <phrase>privacy</phrase> implications of collecting and managing <phrase>big data</phrase>. Explore the broader impact of the <phrase>data science</phrase> field on modern <phrase>society</phrase> and the principles of <phrase>fairness</phrase>, accountability and transparency as you gain a deeper understanding of the importance of a shared set of <phrase>ethical</phrase> values. You will examine the need for voluntary disclosure when leveraging <phrase>metadata</phrase> to inform <phrase>basic</phrase> <phrase>algorithms</phrase> and/or complex <phrase>artificial intelligence</phrase> systems while also learning <phrase>best practices</phrase> for responsible <phrase>data management</phrase>, understanding the significance of the <phrase>Fair</phrase> <phrase>Information</phrase> Practices Principles <phrase>Act</phrase> and the laws concerning the "right to be forgotten."  This course will help you <phrase>answer questions</phrase> such as who owns <phrase>data</phrase>, how do we value <phrase>privacy</phrase>, how to receive <phrase>informed consent</phrase> and what it means to be <phrase>fair</phrase>.  <phrase>Data</phrase> scientists and anyone beginning to use or expand their use of <phrase>data</phrase> will benefit from this course. No particular previous <phrase>knowledge</phrase> needed.
Данный курс - второй в рамках специализации «Практики анализа экономических данных: от простого к сложному». Курс посвящен «продвинутым» инструментам работы в среде MS <phrase>Excel</phrase>: применению сценариев, использованию инструмента «Подбор параметра», формированию дэшбордов и интерактивных диаграмм, использованию функций массивов, и др. Анализ данных – актуальная проблема для всех без исключения компаний. Главная цель анализа – дать информацию, полезную для принятия управленческих решений, лучшего понимания бизнеса. Научить анализировать данные сложно. Представленные курсы охватывают практики оперативной и управленческой аналитики в среде электронных таблиц MS Office. Используется подход к обучению анализу данных в среде <phrase>Microsoft Excel</phrase> через решение типовых задач, проецируемых на любую предметную область. Данный подход апробирован авторами на большом количестве групп повышения квалификации экономистов и менеджеров. В данном курсе слушатель сможет применить знания инструментов MS <phrase>Excel</phrase> для анализа деятельности компании.  По завершении курса слушатели будут:  Знать: - инструменты разработки аналитических отчетов: функции, фильтры, формулы массивов, сводные таблицы; - инструменты графического анализа экономических данных: интерактивных и других диаграмм; - правила применения инструментов ЧТО-ЕСЛИ; - инструменты разработки информационно-аналитических панелей.  Уметь: - формировать аналитические отчеты, используя весь спектр инструментов MS <phrase>Excel</phrase>; - моделировать поведение показателей компании с помощью инструментов ЧТО-ЕСЛИ; - пользоваться инструментами фильтрации при сложных условиях; - формировать дэшборды с показателями деятельности компании.  Владеть: - навыками выбора адекватных аналитических инструментов; - навыками совместной работы с экономическими данными; - инструментами работы с текстовыми данными.
The course will begin with what is familiar to many <phrase>business</phrase> managers and those who have taken the first two courses in this specialization. The first set of tools will explore <phrase>data</phrase> description, <phrase>statistical inference</phrase>, and <phrase>regression</phrase>. We will extend these concepts to other <phrase>statistical methods</phrase> used for prediction when the response <phrase>variable</phrase> is categorical such as win-don’t win an <phrase>auction</phrase>. In the next segment, students will learn about tools used for identifying <phrase>important features</phrase> in the dataset that can either reduce the complexity or help identify <phrase>important features</phrase> of the <phrase>data</phrase> or further help explain behavior. 
In this course, you'll learn how to manage big datasets, how to load them into clusters and <phrase>cloud storage</phrase>, and how to apply structure to the <phrase>data</phrase> so that you can <phrase>run</phrase> queries on it using distributed <phrase>SQL</phrase> engines like <phrase>Apache</phrase> Hive and <phrase>Apache</phrase> Impala. You’ll learn how to choose the right <phrase>data</phrase> types, <phrase>storage systems</phrase>, and <phrase>file formats</phrase> based on which tools you’ll use and what performance you need.  By the end of the course, you will be able to • use different tools to browse existing <phrase>databases</phrase> and tables in <phrase>big data</phrase> systems; • use different tools to explore files in distributed <phrase>big data</phrase> filesystems and <phrase>cloud storage</phrase>; • create and manage <phrase>big data</phrase> <phrase>databases</phrase> and tables using <phrase>Apache</phrase> Hive and <phrase>Apache</phrase> Impala; and • describe and choose among different <phrase>data</phrase> types and <phrase>file formats</phrase> for <phrase>big data</phrase> systems.  To use the hands-on environment for this course, you need to download and <phrase>install</phrase> a <phrase>virtual machine</phrase> and the <phrase>software</phrase> on which to <phrase>run</phrase> it. Before continuing, be sure that you have access to a <phrase>computer</phrase> that meets the following <phrase>hardware</phrase> and <phrase>software</phrase> requirements: • <phrase>Windows</phrase>, macOS, or <phrase>Linux</phrase> <phrase>operating system</phrase> (<phrase>iPads</phrase> and <phrase>Android</phrase> tablets will not work) • <phrase>64-bit</phrase> <phrase>operating system</phrase> (<phrase>32-bit</phrase> <phrase>operating systems</phrase> will not work) • 8 <phrase>GB</phrase> <phrase>RAM</phrase> or more • 25GB <phrase>free</phrase> <phrase>disk space</phrase> or more • <phrase>Intel</phrase> <phrase>VT</phrase>-x or <phrase>AMD</phrase>-V <phrase>virtualization</phrase> support <phrase>enabled</phrase> (on <phrase>Mac</phrase> <phrase>computers</phrase> with <phrase>Intel</phrase> processors, this is always <phrase>enabled</phrase>; on <phrase>Windows</phrase> and <phrase>Linux</phrase> <phrase>computers</phrase>, you might need to enable it in the <phrase>BIOS</phrase>) • For <phrase>Windows XP</phrase> <phrase>computers</phrase> only: You must have an unzip utility such as 7-<phrase>Zip</phrase> or <phrase>WinZip</phrase> installed (<phrase>Windows</phrase> XP’s built-in unzip utility will not work)
Welcome to the specialization course <phrase>Relational Database</phrase> Systems. This course will be completed on six weeks, it will be supported with videos and various documents that will allow you to learn in a very simple way how several types of <phrase>information</phrase> systems and <phrase>databases</phrase> are available to solve different problems and needs of the companies.   Objective:  A learner will be able to <phrase>design</phrase>, <phrase>test</phrase>, and implement analytical, transactional or <phrase>NoSQL</phrase> <phrase>database</phrase> systems according to <phrase>business</phrase> requirements by <phrase>programming</phrase> reliable, scalable and maintainable applications and resources using <phrase>SQL</phrase> and <phrase>Hadoop</phrase> <phrase>ecosystem</phrase>.  <phrase>Programming</phrase> languages:  For course 1 you will use the <phrase>MYSQL</phrase> <phrase>language</phrase>.  <phrase>Software</phrase> to download: <phrase>MySQL</phrase> <phrase>Workbench</phrase>   In <phrase>case</phrase> you have a <phrase>Mac</phrase> / <phrase>IOS</phrase> <phrase>operating system</phrase> you will need to use a <phrase>virtual Machine</phrase> (<phrase>VirtualBox</phrase>, <phrase>Vmware</phrase>).
في هذه الدورة التدريبية الأولى في تخصص مهارات برنامج <phrase>Excel</phrase> للعمل، ستتعلم المبادئ الأساسية لبرنامج <phrase>Microsoft Excel</phrase>. في غضون ستة أسابيع، ستتعلم التنقل بشكل محترف في واجهة مستخدم برنامج <phrase>Excel</phrase>، وإجراء الحسابات الأساسية باستخدام الصيغ والدالات، وتنسيق جداول البيانات بشكل باحترافي، وإنشاء تصورات للبيانات من خلال المخططات والرسوم البيانية.‏  وسواء أكنت قد تعلمت ذاتيًا وتريد أن تسد الفجوات لفعالية وإنتاجية أفضل، أو إذا لم تكن قد استخدمت برنامج <phrase>Excel</phrase> من قبل، فإن هذه الدورة التدريبية ستزودك بأساس متين لكي تصبح مستخدمًا واثقًا وتطور مهارات أكثر تقدمًا في الدورات التدريبية اللاحقة.  لقد شكّلنا فريقًا رائعًا للتدريس سيكون معك في كل خطوة على الطريق. وستمنحك مجموعة كبيرة من الاختبارات والتحديات التدريبية فرصًا رائعة لبناء مجموعة المهارات لديك. تعاون في العمل على كل تحدٍ جديد مع فريقنا، وسوف تفاجئ نفسك بمدى التقدم الذي أحرزته في وقت قصير.  ُيعد برنامج جداول البيانات أحد أكثر البرامج المستخدمة انتشارًا في مختلف أماكن العمل في جميع أنحاء العالم. وتعلُّم العمل على مثل هذا البرنامج بثقة واحترافية يمثل إضافةً ثمينة وعالية القيمة إلى مسيرتك العملية وفرص توظيفك. وفي الوقت الذي تنمو فيه الوظائف التي تعتمد على المهارات الرقمية بشكل أسرع بكثير من الوظائف غير الرقمية، احرص على أن تكون متميزًا عن الآخرين عن طريق إضافة مهارات <phrase>Excel</phrase> لمحفظتك العملية.
<phrase>Biostatistics</phrase> is an essential skill for every <phrase>public health</phrase> researcher because it provides a set of precise methods for extracting meaningful conclusions from <phrase>data</phrase>. In this second course of the <phrase>Biostatistics</phrase> in <phrase>Public Health</phrase> Specialization, you'll learn to evaluate sample variability and apply <phrase>statistical hypothesis testing</phrase> methods. Along the way, you'll perform calculations and interpret <phrase>real-world</phrase> <phrase>data</phrase> from the published <phrase>scientific literature</phrase>. Topics include sample <phrase>statistics</phrase>, the <phrase>central limit theorem</phrase>, <phrase>confidence intervals</phrase>, <phrase>hypothesis testing</phrase>, and p values.
If you want to break into competitive <phrase>data science</phrase>, then this course is for you! Participating in predictive modelling competitions can help you gain practical experience, improve and <phrase>harness</phrase> your <phrase>data</phrase> modelling skills in various domains such as credit, <phrase>insurance</phrase>, <phrase>marketing</phrase>, <phrase>natural language processing</phrase>, sales’ forecasting and <phrase>computer vision</phrase> to name a few. At the same time you get to do it in a competitive <phrase>context</phrase> against thousands of participants where each one tries to build the most predictive <phrase>algorithm</phrase>. Pushing each other to the limit can result in better performance and smaller prediction errors. Being able to achieve <phrase>high</phrase> ranks consistently can help you accelerate your career in <phrase>data science</phrase>.  In this course, you will learn to analyse and solve competitively such predictive modelling tasks.   When you finish this class, you will:  - Understand how to solve predictive modelling competitions efficiently and learn which of the skills obtained can be applicable to <phrase>real-world</phrase> tasks. - Learn how to preprocess the <phrase>data</phrase> and generate new features from various sources such as text and images. - Be taught advanced feature <phrase>engineering</phrase> techniques like generating mean-encodings, using aggregated statistical measures or finding <phrase>nearest neighbors</phrase> as a means to improve your predictions. - Be able to form reliable <phrase>cross validation</phrase> methodologies that help you benchmark your solutions and avoid <phrase>overfitting</phrase> or <phrase>underfitting</phrase> when tested with unobserved (<phrase>test</phrase>) <phrase>data</phrase>.  - Gain experience of analysing and interpreting the <phrase>data</phrase>. You will become aware of inconsistencies, <phrase>high</phrase> noise levels, errors and other <phrase>data</phrase>-<phrase>related issues</phrase> such as leakages and you will learn how to overcome them.  - Acquire <phrase>knowledge</phrase> of different <phrase>algorithms</phrase> and learn how to efficiently tune their hyperparameters and achieve top performance.  - <phrase>Master</phrase> the <phrase>art</phrase> of combining different <phrase>machine learning</phrase> models and learn how to ensemble.  - Get exposed to past (winning) solutions and codes and learn how to read them.  <phrase>Disclaimer</phrase> : This is not a <phrase>machine learning</phrase> course in the <phrase>general</phrase> sense. This course will teach you how to get <phrase>high</phrase>-rank solutions against thousands of competitors with focus on practical usage of <phrase>machine learning</phrase> methods rather than the theoretical underpinnings behind them.  Prerequisites:  - <phrase>Python</phrase>: work with DataFrames in <phrase>pandas</phrase>, plot figures in matplotlib, <phrase>import</phrase> and <phrase>train</phrase> models from <phrase>scikit-learn</phrase>, XGBoost, LightGBM. - <phrase>Machine Learning</phrase>: <phrase>basic</phrase> understanding of <phrase>linear models</phrase>, K-NN, <phrase>random forest</phrase>, <phrase>gradient</phrase> <phrase>boosting</phrase> and <phrase>neural networks</phrase>.  Do you have technical problems? Write to us: <phrase>coursera</phrase>@hse.ru
This course will <phrase>introduce</phrase> students to the basics of the <phrase>Structured Query Language</phrase> (<phrase>SQL</phrase>) as well as <phrase>basic</phrase> <phrase>database design</phrase> for <phrase>storing data</phrase> as part of a multi-<phrase>step</phrase> <phrase>data</phrase> gathering, analysis, and processing effort.  The course will use SQLite3 as its <phrase>database</phrase>.  We will also build web crawlers and multi-<phrase>step</phrase> <phrase>data</phrase> gathering and visualization processes.  We will use the D3.js <phrase>library</phrase> to do <phrase>basic</phrase> <phrase>data</phrase> visualization.  This course will <phrase>cover</phrase> Chapters 14-15 of the <phrase>book</phrase> “<phrase>Python</phrase> for Everybody”. To succeed in this course, you should be familiar with the material <phrase>covered</phrase> in Chapters 1-13 of the <phrase>textbook</phrase> and the first three courses in this specialization. This course covers <phrase>Python</phrase> 3.
This course is intended to be <phrase>an introduction</phrase> to <phrase>machine learning</phrase> for non-technical <phrase>business</phrase> professionals. There is a lot of hype around <phrase>machine learning</phrase> and many people are concerned that in <phrase>order</phrase> to use <phrase>machine learning</phrase> in <phrase>business</phrase>, you need to have a technical background. For reasons that are <phrase>covered</phrase> in this course, that's not the <phrase>case</phrase>. In actuality, your <phrase>knowledge</phrase> of your <phrase>business</phrase> is far more important than your ability to build an <phrase>ML</phrase> <phrase>model</phrase> from scratch.  By the end of this course, you will have learned how to: • Formulate <phrase>machine learning</phrase> solutions to <phrase>real-world</phrase> problems • Identify whether the <phrase>data</phrase> you have is sufficient for <phrase>ML</phrase> • Carry a project through various <phrase>ML</phrase> phases including training, evaluation, and deployment • Perform <phrase>AI</phrase> responsibly and avoid reinforcing existing bias • Discover <phrase>ML</phrase> <phrase>use cases</phrase> • Be successful at <phrase>ML</phrase>  You'll need a desktop <phrase>web browser</phrase> to <phrase>run</phrase> this course's interactive labs via Qwiklabs and <phrase>Google</phrase> <phrase>Cloud</phrase> Platform.  >>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
This course introduces you to the <phrase>basic</phrase> <phrase>biology</phrase> of modern <phrase>genomics</phrase> and the <phrase>experimental</phrase> tools that we use to measure it. We'll <phrase>introduce</phrase> the <phrase>Central Dogma of Molecular Biology</phrase> and <phrase>cover</phrase> how <phrase>next-generation sequencing</phrase> can be used to measure <phrase>DNA</phrase>, <phrase>RNA</phrase>, and <phrase>epigenetic</phrase> patterns. You'll also get <phrase>an introduction</phrase> to the <phrase>key concepts</phrase> in <phrase>computing</phrase> and <phrase>data science</phrase> that you'll need to understand how <phrase>data</phrase> from <phrase>next-generation sequencing</phrase> experiments are generated and analyzed.    This is the first course in the <phrase>Genomic</phrase> <phrase>Data Science</phrase> Specialization.
Get started learning about the fascinating and useful world of <phrase>geographic information systems</phrase> (<phrase>GIS</phrase>)! In this first course of the specialization <phrase>GIS</phrase>, Mapping, and <phrase>Spatial Analysis</phrase>, you'll learn about what a <phrase>GIS</phrase> is, how to get started with the <phrase>software</phrase> yourself, how things we find in the <phrase>real world</phrase> can be represented on a map, how we record locations using coordinates, and how we can make a <phrase>two-dimensional</phrase> map from a <phrase>three-dimensional</phrase> <phrase>Earth</phrase>. In the course project, you will create your own <phrase>GIS</phrase> <phrase>data</phrase> by tracing geographic features from a <phrase>satellite</phrase> image for a location and theme of your choice. This course will give you a strong foundation in mapping and <phrase>GIS</phrase> that will give you the understanding you need to start working with <phrase>GIS</phrase>, and to succeed in the other courses in this specialization.  This course is for anyone who wants to learn about mapping and <phrase>GIS</phrase>. You don't have to have any previous experience - just your curiosity! The course includes both practical <phrase>software</phrase> training and explanations of the concepts you need to know to make informed decisions as you start your journey to becoming a <phrase>GIS</phrase> analyst.  You will need a <phrase>Windows</phrase> <phrase>computer</phrase> with <phrase>ArcGIS</phrase> Desktop installed.
Apresentaremos o TensorFlow de baixo nível <phrase>e</phrase> trabalharemos com <phrase>os</phrase> conceitos <phrase>e</phrase> <phrase>APIs</phrase> necessários para gravar modelos de aprendizado de máquina distribuídos. Levando <phrase>em</phrase> consideração <phrase>os</phrase> modelos do TensorFlow, explicaremos como fazer o escalonamento horizontal do treinamento desse modelo <phrase>e</phrase> oferecer previsões de <phrase>alto</phrase> desempenho com o <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase>.  Objetivos do curso: Criar modelos de aprendizado de máquina no TensorFlow Usar as bibliotecas do TensorFlow para solucionar problemas numéricos Resolver problemas <phrase>e</phrase> <phrase>lidar</phrase> com dificuldades comuns do código do TensorFlow Usar o tf_estimator para criar, treinar <phrase>e</phrase> avaliar modelos de aprendizado de máquina Treinar, implantar <phrase>e</phrase> produzir modelos de aprendizado de máquina <phrase>em</phrase> escala com o <phrase>Cloud</phrase> <phrase>ML</phrase> <phrase>Engine</phrase>
This course teaches you the fundamentals of clinical <phrase>natural language processing</phrase>. In this course you will learn practical techniques for extracting <phrase>information</phrase> stored in <phrase>text-based</phrase> portions of <phrase>electronic</phrase> <phrase>medical</phrase> records.
<phrase>Ce</phrase> cours présente l'approche TensorFlow de bas niveau et dresse <phrase>la</phrase> liste <phrase>des</phrase> concepts et <phrase>API</phrase> nécessaires pour <phrase>la</phrase> rédaction de modèles de <phrase>machine learning</phrase> distribués. Nous verrons comment appliquer une évolutivité horizontale à l'entraînement d'un modèle TensorFlow afin d'offrir <phrase>des</phrase> prédictions très pertinentes avec <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase>.  Objectifs du cours : Créer <phrase>des</phrase> modèles de <phrase>machine learning</phrase> dans TensorFlow Utiliser <phrase>les</phrase> bibliothèques TensorFlow pour résoudre <phrase>des</phrase> problèmes numériques Résoudre <phrase>les</phrase> problèmes et déboguer <phrase>les</phrase> erreurs de code courantes sur TensorFlow Utiliser tf.estimator pour créer, entraîner et évaluer un modèle de <phrase>ML</phrase> Entraîner et déployer <phrase>les</phrase> modèles de <phrase>ML</phrase> <phrase>avant</phrase> de <phrase>les</phrase> envoyer <phrase>en</phrase> <phrase>production</phrase> à grande échelle avec <phrase>Cloud</phrase> <phrase>ML</phrase> <phrase>Engine</phrase>
في هذه الدورة التدريبية، ستتعلم أفضل الممارسات لكيفية استخدام تحليلات البيانات لتجعل أي شركة لها قدرة أكبر على التنافس والربح. سيمكنك التعرف على أهم مقاييس الأعمال وتمييزها عن البيانات العادية.   وستكون لديك صورة واضحة عن الأدوار الحيوية المختلفة التي يضطلع بها كل من محللي الأعمال، ومحللي بيانات الأعمال، وعلماء البيانات في مختلف أنواع الشركات. وستعرف بالضبط أي المهارات مطلوبة للتوظيف في هذه الأعمال التي يرتفع الطلب عليها والنجاح فيها.   وفي النهاية، سيمكنك الاستعانة بالقائمة المرجعية التي توفرها الدورة التدريبية؛ لتقييم أي شركة بناءً على كيفية تبنّيها لثقافة البيانات الضخمة بفعاليّة. تُحدث الشركات الرقمية مثل <phrase>Amazon</phrase>، وUber، وAirbnb تحوّلًا في الصناعات بالكامل من خلال استخدامها الإبداعي للبيانات الضخمة.. وستدرك لِمَ تكون هذه الشركات معرقلة للغاية، وكيفية استخدامها لتقنيات تحليل البيانات؛ لكي تتفوق في قدرتها التنافسية على الشركات التقليدية.
In this course, you will learn the fundamental techniques for making personalized recommendations through <phrase>nearest-neighbor</phrase> techniques.  First you will learn user-user <phrase>collaborative filtering</phrase>, an <phrase>algorithm</phrase> that identifies other people with similar tastes to a <phrase>target</phrase> user and combines their ratings to make recommendations for that user.  You will explore and implement variations of the user-user <phrase>algorithm</phrase>, and will explore the benefits and drawbacks of the <phrase>general</phrase> approach.  Then you will learn the widely-practiced <phrase>item</phrase>-<phrase>item</phrase> <phrase>collaborative filtering</phrase> <phrase>algorithm</phrase>, which identifies global product associations from user ratings, but uses these product associations to provide personalized recommendations based on a user's own product ratings.
This course focuses on one of the most important tools in your <phrase>data</phrase> analysis <phrase>arsenal</phrase>: <phrase>regression analysis</phrase>. Using either <phrase>SAS</phrase> or <phrase>Python</phrase>, you will begin with <phrase>linear regression</phrase> and then learn how to adapt when two variables do not present a clear linear relationship. You will examine multiple predictors of your outcome and be able to identify <phrase>confounding</phrase> variables, which can tell a more compelling story about your <phrase>results</phrase>. You will learn the assumptions underlying <phrase>regression analysis</phrase>, how to interpret <phrase>regression</phrase> coefficients, and how to use <phrase>regression</phrase> diagnostic plots and other tools to evaluate the quality of your <phrase>regression</phrase> <phrase>model</phrase>. Throughout the course, you will share with others the <phrase>regression</phrase> models you have developed and the stories they tell you.
The Capstone project will allow you to continue to apply and refine the <phrase>data</phrase> analytic techniques learned from the previous courses in the Specialization to <phrase>address</phrase> an important issue in <phrase>society</phrase>. You will use <phrase>real world</phrase> <phrase>data</phrase> to complete a project with our <phrase>industry</phrase> and <phrase>academic</phrase> partners. For example, you can work with our <phrase>industry</phrase> partner, DRIVENDATA, to help them solve some of the world's biggest social challenges! DRIVENDATA at www.drivendata.org, is committed to bringing <phrase>cutting-edge</phrase> practices in <phrase>data science</phrase> and <phrase>crowdsourcing</phrase> to some of the world's biggest social challenges and the organizations taking them on.   Or, you can work with our other <phrase>industry</phrase> partner, The Connection (www.theconnectioninc.org) to help them better understand <phrase>recidivism</phrase> <phrase>risk</phrase> for people on <phrase>parole</phrase> seeking substance use treatment. For more than 40 years, The Connection has been one of Connecticut’s leading <phrase>private</phrase>, <phrase>nonprofit</phrase> <phrase>human</phrase> service and <phrase>community</phrase> development agencies. Each month, thousands of people are assisted by The Connection’s diverse behavioral <phrase>health</phrase>, <phrase>family</phrase> support and <phrase>community</phrase> <phrase>justice</phrase> programs. The Connection’s Institute for Innovative Practice was created in 2010 to <phrase>bridge</phrase> the gap between researchers and practitioners in the behavioral <phrase>health</phrase> and <phrase>criminal justice</phrase> fields with the goal of developing maximally effective, <phrase>evidence-based</phrase> treatment programs.   A <phrase>major</phrase> component of the Capstone project is for you to be able to choose the <phrase>information</phrase> from your analyses that best conveys <phrase>results</phrase> and implications, and to tell a compelling story with this <phrase>information</phrase>. By the end of the course, you will have a <phrase>professional</phrase> quality <phrase>report</phrase> of your findings that can be shown to colleagues and potential employers to demonstrate the skills you learned by completing the Specialization.
This course presents the principles of <phrase>evolution</phrase> and <phrase>ecology</phrase> for citizens and students interested in studying <phrase>biology</phrase> and environmental sciences. It discusses <phrase>major</phrase> ideas and <phrase>results</phrase>. <phrase>Recent advances</phrase> have energised these fields with evidence that has implications beyond their boundaries: ideas, mechanisms, and processes that should form part of the <phrase>toolkit</phrase> of all <phrase>biologists</phrase> and educated citizens.  <phrase>Major</phrase> topics <phrase>covered</phrase> by the course include fundamental principles of <phrase>ecology</phrase>, how organisms interact with each other and their environment, <phrase>evolutionary</phrase> processes, <phrase>population</phrase> dynamics, communities, <phrase>energy</phrase> flow and <phrase>ecosystems</phrase>, <phrase>human</phrase> influences on <phrase>ecosystems</phrase>, and the <phrase>integration</phrase> and scaling of <phrase>ecological</phrase> processes through <phrase>systems ecology</phrase>.   This course will also review <phrase>major</phrase> <phrase>ecological</phrase> concepts, identify the techniques used by <phrase>ecologists</phrase>, provide an overview of <phrase>local and global</phrase> environmental issues, and examine individual, group and governmental activities important for protecting natural <phrase>ecosystems</phrase>. The course has been designed to provide <phrase>information</phrase>, to direct the <phrase>student</phrase> toward pertinent <phrase>literature</phrase>, to identify problems and issues, to utilise <phrase>research</phrase> methodology for the study of <phrase>ecology</phrase> and <phrase>evolution</phrase>, and to consider appropriate solutions and analytical techniques.   Needed Learner Background: <phrase>general</phrase> <phrase>biology</phrase> and a good understanding of <phrase>English</phrase>.  This course has the following expectations and <phrase>results</phrase>: 1) covers the theoretical and <phrase>practical issues</phrase> involved in <phrase>ecology</phrase> and <phrase>evolution</phrase>, 2) <phrase>conducting</phrase> surveys and inventories in <phrase>ecology</phrase>,  3) analyzing the <phrase>information</phrase> gathered,  4) and applying their analysis to <phrase>ecological</phrase> and conservation problems.
Oдним из условий применимости обычных линейных моделей является независимость наблюдений друг от друга, на основе которых подбирается модель. Однако на  практике часто встречаются ситуации, когда дизайн сбора материала таков, что нарушение этого условия неизбежно. Представьте, что вы решили построить модель, описывающую связь успеваемости по физкультуре и величины IQ теста у студентов. Для решения этой задачи вы сделали многочисленные выборки в нескольких институтах. Можно ли объединить такие данные в один анализ, построенной по традиционной схеме? Конечно нет. Студенты в каждом вузе могут быть в чем-то сходными друг с другом. Даже характер связи между изучаемыми величинами может быть несколько разным. Такого рода данные, в которых присутствуют внутригрупповые корреляции, стоит анализировать при помощи смешанных линейных моделей. Мы покажем, что некоторые предикторы стоит включать в модель в качестве так называемых “случайных факторов”. Вы узнаете, что случайные факторы могут быть иерархически соподчинены. Мы обсудим, как такие смешанные модели могут быть построены для зависимых переменных подчиняющихся разным типам распределений. Кроме того, мы покажем, что случайная часть модели может быть устроена еще сложнее - в ней может быть компонент, моделирующий поведение дисперсии в ответ на влияние ковариаты. В конце курса вас ждет проект, в котором вы сможете потренироваться в построении смешанных моделей, выбрав один из нескольких датасетов. На основе анализа этих данных вы сможете создать отчет, выдержанный в традициях воспроизводимого исследования.  Этот курс поможет научиться строить модели со случайными факторами для величин с разными типами распределений. Чтобы легче осваивать материалы курса, вам пригодятся базовые представления о линейных моделях (общих и обобщенных), базовые знания R и умение создавать простейшие .<phrase>html</phrase> документы при помощи rmarkdown и knitr.
A partir de una historia del aprendizaje automático, analizamos por qué las redes neuronales, <phrase>en</phrase> <phrase>la</phrase> actualidad, ofrecen un <phrase>alto</phrase> rendimiento ante una variedad de problemas. Luego, analizaremos cómo configurar un problema de aprendizaje supervisado y encontrar una solución adecuada mediante <phrase>el</phrase> descenso de gradientes. Esto incluye crear conjuntos de datos que permitan <phrase>la</phrase> generalización; hablaremos sobre los métodos para hacerlo de una manera repetible que admita <phrase>la</phrase> experimentación.  Objetivos del curso: Identificar por qué <phrase>el</phrase> aprendizaje profundo <phrase>es</phrase> popular <phrase>en</phrase> <phrase>la</phrase> actualidad Optimizar y evaluar los modelos mediante las funciones de pérdida y las métricas de rendimiento Mitigar los problemas comunes que surgen <phrase>en</phrase> <phrase>el</phrase> aprendizaje automático Crear conjuntos de datos de entrenamiento, evaluación y prueba, repetibles y escalables
<phrase>Machine learning</phrase> is transforming the world around us. To become successful, you’d better know what kinds of problems can be solved with <phrase>machine learning</phrase>, and how they can be solved. Don’t know where to start? The answer is one button away.   During this course you will: - Identify <phrase>practical problems</phrase> which can be solved with <phrase>machine learning</phrase> - Build, tune and apply <phrase>linear models</phrase> with Spark MLLib - Understand methods of <phrase>text processing</phrase> - Fit <phrase>decision trees</phrase> and boost them with <phrase>ensemble learning</phrase> - Construct your own <phrase>recommender system</phrase>.   As a practical assignment, you will  - build and apply <phrase>linear models</phrase> for classification and <phrase>regression</phrase> tasks;  - learn how to work with texts;  - automatically construct <phrase>decision trees</phrase> and improve their performance with <phrase>ensemble learning</phrase>;  - finally, you will build your own <phrase>recommender system</phrase>!  With these skills, you will be able to tackle many practical <phrase>machine learning</phrase> tasks.   We provide the tools, you choose the <phrase>place</phrase> of <phrase>application</phrase> to make this world of machines more intelligent.  Special thanks to: - Prof. Mikhail Roytberg, APT <phrase>dept</phrase>., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the <phrase>road</phrase>. - Oleg Sukhoroslov (<phrase>PhD</phrase>, Senior Researcher at IITP RAS), who has been teaching  <phrase>MapReduce</phrase>, <phrase>Hadoop</phrase> and <phrase>friends</phrase> since 2008. Now he is leading the <phrase>infrastructure</phrase> team. - Oleg Ivchenko (<phrase>PhD</phrase> <phrase>student</phrase> APT <phrase>dept</phrase>., MIPT), Pavel Akhtyamov (<phrase>MSc</phrase>. <phrase>student</phrase> at APT <phrase>dept</phrase>., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl <phrase>State</phrase> <phrase>University</phrase>), superbrains who have developed and now maintain the <phrase>infrastructure</phrase> used for practical assignments in this course. - Asya Roitberg, <phrase>Eugene</phrase> Baulin, <phrase>Marina</phrase> Sudarikova. These people never <phrase>sleep</phrase> to babysit this course day and night, to make your learning experience productive, smooth and exciting.
Bienvenido al curso The <phrase>Art</phrase> and <phrase>Science</phrase> of <phrase>Machine Learning</phrase>. <phrase>En</phrase> este curso, adquirirá las competencias básicas de intuición de <phrase>AA</phrase>, evaluación de modelos de <phrase>AA</phrase> y experimentación con ellos para realizar los ajustes y optimizaciones correspondientes a <phrase>fin</phrase> de obtener <phrase>el</phrase> mejor rendimiento posible.     También aprenderá los mecanismos necesarios para entrenar modelos. Primero, los ajustará manualmente para observar los efectos <phrase>en</phrase> <phrase>el</phrase> rendimiento. Una vez que <phrase>se</phrase> familiarice con los valores que puede cambiar, denominados hiperparámetros, aprenderá a ajustarlos automáticamente con <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> <phrase>en</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> Platform.
In most areas of <phrase>health</phrase>, <phrase>data</phrase> is being used to make important decisions.  As a <phrase>health</phrase> <phrase>population</phrase> <phrase>manager</phrase>, you will have the opportunity to use <phrase>data</phrase> to answer interesting questions. In this course, we will discuss <phrase>data</phrase> analysis from a responsible perspective, which will help you to extract useful <phrase>information</phrase> from <phrase>data</phrase> and enlarge your <phrase>knowledge</phrase> about specific aspects of interest of the <phrase>population</phrase>.   First, you will learn how to obtain, safely gather, clean and explore <phrase>data</phrase>. Then, we will discuss that because <phrase>data</phrase> are usually obtained from a sample of a limited number of individuals, <phrase>statistical methods</phrase> are needed to make claims about the whole <phrase>population</phrase> of interest. You will discover how <phrase>statistical inference</phrase>, <phrase>hypothesis testing</phrase> and <phrase>regression</phrase> techniques will help you to make the connection between samples and populations.  A final important <phrase>aspect</phrase> is interpreting and reporting. How can we transform <phrase>information</phrase> into <phrase>knowledge</phrase>? How can we separate trustworthy <phrase>information</phrase> from noise? In the last part of the course, we will <phrase>cover</phrase> the critical assessment of the <phrase>results</phrase>, and we will discuss challenges and dangers of <phrase>data analysis</phrase> in the <phrase>era</phrase> of <phrase>big data</phrase> and <phrase>massive amounts</phrase> of <phrase>information</phrase>.    In this course, we will emphasize the concepts and we will also teach you how to effectively perform your analysis using R. You do not need to <phrase>install</phrase> R on your <phrase>computer</phrase> to follow the course, you will be able to access R and all the example <phrase>data</phrase> sets within the <phrase>Coursera</phrase> environment.    This course will become part of the to-be-developed  <phrase>Leiden University</phrase> <phrase>master</phrase> program <phrase>Population</phrase> <phrase>Health</phrase> <phrase>Management</phrase>. If you wish to find out more about this program see the last <phrase>reading</phrase> of this Course!
Sie möchten erfahren, wie Sie die Genauigkeit Ihrer maschinellen Lernmodelle verbessern <phrase>oder</phrase> wie Sie herausfinden, welche Datenspalten die nützlichsten Funktionen ergeben? Willkommen zum Feature <phrase>Engineering</phrase> <phrase>mit</phrase> der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Wir erörtern in diesem Kurs nützliche und nutzlose Funktionen und wie Sie diese für die optimale Nutzung in Ihren maschinellen Lernmodellen vorverarbeiten und umwandeln.  In praktischen, interaktiven Labs lernen Sie, Funktionen auszuwählen und <phrase>mit</phrase> der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform vorzuverarbeiten. Unsere Kursleiter präsentieren Ihnen die Code-Lösungen, die zu Referenzzwecken <phrase>auch</phrase> öffentlich gemacht werden, während Sie an Ihren eigenen zukünftigen <phrase>ML</phrase>-Projekten arbeiten.
Once you’ve identified a <phrase>big data</phrase> issue to analyze, how do you collect, store and organize your <phrase>data</phrase> using <phrase>Big Data</phrase> solutions?  In this course, you will experience various <phrase>data</phrase> genres and <phrase>management</phrase> tools appropriate for each.  You will be able to describe the reasons behind the evolving plethora of new <phrase>big data</phrase> platforms from the perspective of <phrase>big data</phrase> <phrase>management</phrase> systems and analytical tools.  Through guided hands-on tutorials, you will become familiar with techniques using <phrase>real-time</phrase> and <phrase>semi-structured data</phrase> examples.  Systems and tools discussed include: AsterixDB, <phrase>HP</phrase> Vertica, Impala, Neo4j, <phrase>Redis</phrase>, SparkSQL. This course provides techniques to extract value from existing untapped <phrase>data</phrase> sources and discovering new <phrase>data</phrase> sources.  At the end of this course, you will be able to:  * Recognize different <phrase>data</phrase> elements in your own work and in <phrase>everyday life</phrase> problems  * Explain why your team needs to <phrase>design</phrase> a <phrase>Big Data</phrase> <phrase>Infrastructure</phrase> Plan and <phrase>Information</phrase> System <phrase>Design</phrase>  * Identify the frequent <phrase>data</phrase> operations required for various types of <phrase>data</phrase>  * Select a <phrase>data model</phrase> to suit the characteristics of your <phrase>data</phrase>   * Apply techniques to <phrase>handle</phrase> <phrase>streaming</phrase> <phrase>data</phrase>  * Differentiate between a traditional <phrase>Database Management</phrase> System and a <phrase>Big Data</phrase> <phrase>Management</phrase> System  * Appreciate why there are so many <phrase>data management</phrase> systems  * <phrase>Design</phrase> a <phrase>big data</phrase> <phrase>information system</phrase> for an <phrase>online game</phrase> <phrase>company</phrase>  This course is for those new to <phrase>data science</phrase>.  Completion of Intro to <phrase>Big Data</phrase> is recommended.  No prior <phrase>programming</phrase> experience is needed, although the ability to <phrase>install</phrase> applications and utilize a <phrase>virtual machine</phrase> is necessary to complete the hands-on assignments.  Refer to the specialization <phrase>technical requirements</phrase> for complete <phrase>hardware</phrase> and <phrase>software</phrase> specifications.  <phrase>Hardware</phrase> Requirements:  (A) <phrase>Quad Core</phrase> Processor (<phrase>VT</phrase>-x or <phrase>AMD</phrase>-V support recommended), <phrase>64-bit</phrase>; (B) 8 <phrase>GB</phrase> <phrase>RAM</phrase>; (C) 20 <phrase>GB</phrase> disk <phrase>free</phrase>. How to find your <phrase>hardware</phrase> <phrase>information</phrase>: (<phrase>Windows</phrase>): <phrase>Open System</phrase> by clicking the Start button, right-clicking <phrase>Computer</phrase>, and then clicking Properties; (<phrase>Mac</phrase>): Open Overview by clicking on the <phrase>Apple</phrase> menu and clicking “About This <phrase>Mac</phrase>.” Most <phrase>computers</phrase> with 8 <phrase>GB</phrase> <phrase>RAM</phrase> purchased in the last 3 years will meet the minimum requirements.You will need a <phrase>high</phrase> speed <phrase>internet</phrase> connection because you will be downloading files up to 4 <phrase>Gb</phrase> in size.   <phrase>Software</phrase> Requirements:  This course relies on several <phrase>open-source software</phrase> tools, including <phrase>Apache Hadoop</phrase>. All required <phrase>software</phrase> can be downloaded and installed <phrase>free</phrase> of charge (except for <phrase>data</phrase> charges from your <phrase>internet provider</phrase>). <phrase>Software</phrase> requirements include: <phrase>Windows</phrase> 7+, <phrase>Mac OS</phrase> X 10.10+, <phrase>Ubuntu</phrase> 14.04+ or <phrase>CentOS</phrase> 6+ <phrase>VirtualBox</phrase> 5+.
This course aims at providing an introductory and broad overview of the field of <phrase>ML</phrase> with the focus on applications on <phrase>Finance</phrase>. Supervised <phrase>Machine Learning</phrase> methods are used in the capstone project to predict <phrase>bank</phrase> closures. Simultaneously, while this course can be taken as a separate course, it serves as a preview of topics that are <phrase>covered</phrase> in more details in subsequent modules of the specialization <phrase>Machine Learning</phrase> and <phrase>Reinforcement Learning</phrase> in <phrase>Finance</phrase>. The goal  of Guided <phrase>Tour</phrase> of <phrase>Machine Learning</phrase> in <phrase>Finance</phrase> is to get a sense of what <phrase>Machine Learning</phrase> is, what it is for and in how many different financial problems it can be applied to.  The course is designed for three categories of students: Practitioners working at <phrase>financial institutions</phrase> such as <phrase>banks</phrase>, <phrase>asset management</phrase> firms or <phrase>hedge funds</phrase> Individuals interested in applications of <phrase>ML</phrase> for personal day trading Current full-time students pursuing a <phrase>degree</phrase> in <phrase>Finance</phrase>, <phrase>Statistics</phrase>, <phrase>Computer Science</phrase>, <phrase>Mathematics</phrase>, <phrase>Physics</phrase>, <phrase>Engineering</phrase> or other <phrase>related disciplines</phrase> who want to learn about <phrase>practical applications</phrase> of <phrase>ML</phrase> in <phrase>Finance</phrase>    Experience with <phrase>Python</phrase> (including <phrase>numpy</phrase>, <phrase>pandas</phrase>, and IPython/Jupyter notebooks), <phrase>linear algebra</phrase>, <phrase>basic</phrase> <phrase>probability theory</phrase> and <phrase>basic</phrase> <phrase>calculus</phrase> is necessary to complete assignments in this course.
We <phrase>introduce</phrase> <phrase>low-level</phrase> TensorFlow and work our way through the necessary concepts and <phrase>APIs</phrase> so as to be able to write distributed <phrase>machine learning</phrase> models. Given a TensorFlow <phrase>model</phrase>, we explain how to scale out the training of that <phrase>model</phrase> and offer <phrase>high</phrase>-performance predictions using <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase>.  Course Objectives: Create <phrase>machine learning</phrase> models in TensorFlow Use the TensorFlow <phrase>libraries</phrase> to solve numerical problems Troubleshoot and debug common TensorFlow code pitfalls Use tf.estimator to create, <phrase>train</phrase>, and evaluate an <phrase>ML</phrase> <phrase>model</phrase> <phrase>Train</phrase>, deploy, and productionalize <phrase>ML</phrase> models at scale with <phrase>Cloud</phrase> <phrase>ML</phrase> <phrase>Engine</phrase>
Despite the recent increase in <phrase>computing</phrase> power and access to <phrase>data</phrase> over the last couple of decades, our ability to use the <phrase>data</phrase> within the <phrase>decision making</phrase> process is either <phrase>lost</phrase> or not maximized at all too often, we don't have a solid understanding of the questions being asked and how to apply the <phrase>data</phrase> correctly to the problem at hand.  This course has one purpose, and that is to share a methodology that can be used within <phrase>data science</phrase>, to ensure that the <phrase>data</phrase> used in <phrase>problem solving</phrase> is relevant and properly manipulated to <phrase>address</phrase> the question at hand.  Accordingly, in this course, you will learn:     - The <phrase>major</phrase> steps involved in tackling a <phrase>data science</phrase> problem.     - The <phrase>major</phrase> steps involved in practicing <phrase>data science</phrase>, from forming a <phrase>concrete</phrase> <phrase>business</phrase> or <phrase>research</phrase> problem, to collecting and <phrase>analyzing data</phrase>, to building a <phrase>model</phrase>, and understanding the <phrase>feedback</phrase> after <phrase>model</phrase> deployment.     - How <phrase>data</phrase> scientists think!  LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
<phrase>Reinforcement Learning</phrase> is a subfield of <phrase>Machine Learning</phrase>, but is also a <phrase>general</phrase> purpose formalism for automated <phrase>decision-making</phrase> and <phrase>AI</phrase>. This course introduces you to <phrase>statistical learning</phrase> techniques where an agent explicitly takes actions and interacts with the world. Understanding the importance and challenges of learning agents that make decisions is of vital importance today, with more and more companies interested in interactive agents and <phrase>intelligent decision</phrase>-making.   This course introduces you to the fundamentals of <phrase>Reinforcement Learning</phrase>. When you finish this course, you will: - Formalize problems as <phrase>Markov</phrase> Decision Processes  - Understand <phrase>basic</phrase> exploration methods and the exploration/exploitation tradeoff - Understand value functions, as a <phrase>general</phrase>-purpose tool for <phrase>optimal decision</phrase>-making - Know how to implement <phrase>dynamic programming</phrase> as an efficient <phrase>solution</phrase> approach to an <phrase>industrial</phrase> control problem  This course teaches you the <phrase>key concepts</phrase> of <phrase>Reinforcement Learning</phrase>, underlying classic and modern <phrase>algorithms</phrase> in <phrase>RL</phrase>. After completing this course, you will be able to start using <phrase>RL</phrase> for real problems, where you have or can specify the MDP.   This is the first course of the <phrase>Reinforcement Learning</phrase> Specialization.
This course provides an analytical framework to help you evaluate key problems in a structured <phrase>fashion</phrase> and will equip you with tools to better manage the uncertainties that pervade and complicate <phrase>business</phrase> processes. Specifically, you will be introduced to <phrase>statistics</phrase> and how to summarize <phrase>data</phrase> and learn concepts of <phrase>frequency</phrase>, <phrase>normal distribution</phrase>, statistical studies, <phrase>sampling</phrase>, and <phrase>confidence intervals</phrase>.  While you will be introduced to some of the <phrase>science</phrase> of what is being taught, the focus will be on applying the methodologies. This will be accomplished through the use of <phrase>Excel</phrase> and <phrase>data</phrase> sets from many different disciplines, allowing you to see the use of <phrase>statistics</phrase> in very diverse settings. The course will focus not only on explaining these concepts, but also understanding the meaning of the <phrase>results</phrase> obtained.  Upon successful completion of this course, you will be able to:  •	Summarize <phrase>large data sets</phrase> in <phrase>graphical</phrase>, tabular, and numerical forms. •	Understand the significance of proper <phrase>sampling</phrase> and why you can rely on sample <phrase>information</phrase>. •	Understand why <phrase>normal distribution</phrase> can be used in so many settings. •	Use sample <phrase>information</phrase> to infer about the <phrase>population</phrase> with a certain level of confidence about the accuracy of the estimations. •	Use <phrase>Excel</phrase> for <phrase>statistical analysis</phrase>.  This course is part of the iMBA offered by the <phrase>University</phrase> of <phrase>Illinois</phrase>, a flexible, fully-accredited online <phrase>MBA</phrase> at an incredibly competitive price. For more <phrase>information</phrase>, please see the Resource page in this course and onlinemba.illinois.edu.
This course is <phrase>an introduction</phrase> to <phrase>sequence</phrase> models and their applications, including an overview of <phrase>sequence</phrase> <phrase>model</phrase> architectures and how to <phrase>handle</phrase> inputs of <phrase>variable</phrase> length.  • Predict future values of a <phrase>time-series</phrase> • Classify <phrase>free</phrase> form text • <phrase>Address</phrase> <phrase>time-series</phrase> and text problems with <phrase>recurrent neural</phrase> networks • Choose between RNNs/LSTMs and simpler models • <phrase>Train</phrase> and reuse <phrase>word embeddings</phrase> in text problems  You will get hands-on practice building and optimizing your own text classification and <phrase>sequence</phrase> models on a <phrase>variety</phrase> of <phrase>public</phrase> datasets in the labs we’ll work on together.    Prerequisites: <phrase>Basic</phrase> <phrase>SQL</phrase>, familiarity with <phrase>Python</phrase> and TensorFlow
<phrase>Biostatistics</phrase> is the <phrase>application</phrase> of statistical reasoning to the <phrase>life</phrase> sciences, and it's the key to unlocking the <phrase>data</phrase> gathered by researchers and the evidence presented in the scientific <phrase>public health</phrase> <phrase>literature</phrase>. In this course, we'll focus on the use of simple <phrase>regression</phrase> methods to determine the relationship between an outcome of interest and a <phrase>single</phrase> predictor via a <phrase>linear equation</phrase>. Along the way, you'll be introduced to a <phrase>variety</phrase> of methods, and you'll practice interpreting <phrase>data</phrase> and performing calculations on real <phrase>data</phrase> from published studies.  Topics include <phrase>logistic regression</phrase>, <phrase>confidence intervals</phrase>, p-values, Cox <phrase>regression</phrase>, <phrase>confounding</phrase>, adjustment, and effect modification.
Are you interested in predicting future outcomes using your <phrase>data</phrase>? This course helps you do just that! <phrase>Machine learning</phrase> is the process of developing, testing, and applying predictive <phrase>algorithms</phrase> to achieve this goal. Make sure to familiarize yourself with course 3 of this specialization before <phrase>diving</phrase> into these <phrase>machine learning</phrase> concepts. Building on Course 3, which introduces students to <phrase>integral</phrase> supervised <phrase>machine learning</phrase> concepts, this course will provide an overview of many additional concepts, techniques, and <phrase>algorithms</phrase> in <phrase>machine learning</phrase>, from <phrase>basic</phrase> classification to <phrase>decision trees</phrase> and clustering. By completing this course, you will learn how to apply, <phrase>test</phrase>, and interpret <phrase>machine learning</phrase> <phrase>algorithms</phrase> as <phrase>alternative</phrase> methods for <phrase>addressing</phrase> your <phrase>research</phrase> questions.
The explosion in <phrase>digital media</phrase> - web, social and now <phrase>mobile</phrase> - represents a departure from how things were like in the last century. This proliferation of <phrase>digital media</phrase> is both a threat and an opportunity for many businesses. <phrase>Business</phrase> Analytics can be leveraged to process <phrase>data</phrase>, sentiment, buzz, contacts, <phrase>context</phrase> and other aspects of <phrase>business</phrase> interest in <phrase>real time</phrase>, for <phrase>business</phrase> performance and impact. The course picks and uses <phrase>use-cases</phrase> from a <phrase>variety</phrase> of industries and geographies, to showcase the potential and impact that <phrase>business</phrase> analytics done properly (or not) can have on <phrase>business</phrase> performance.
本課程第二部分著重在和人工智慧密不可分的機器學習。課程內容包含了機器學習基礎理論（包含 1990 年代發展的<phrase>VC</phrase>理論）、分類器（包含決策樹及支援向量機）、神經網路（包含深度學習）及增強式學習（包含深度增強式學習。         此部份技術包含最早追溯至 1950 年代直到最近 2016 年附近的最新發展。此課程從基礎理論開始，簡介了各機器學習主流技法以及從淺層學習架構演變到最近深度架構的轉換。  本課程之核心目標為： （一）使同學對人工智慧相關的機器學習技術有基礎概念 （二）同學能夠理解機器學習基礎理論、分類器、神經網路、增強式學習 （三）同學能將相關技術應用到自己的問題上  修課前，基礎背景知識： 需要的先備知識：計算機概論 建議的先備知識：資料結構與演算法
In the second course of this specialization, we will dive into the components and <phrase>best practices</phrase> of a <phrase>high</phrase>-performing <phrase>ML</phrase> system in <phrase>production</phrase> environments.   Prerequisites: <phrase>Basic</phrase> <phrase>SQL</phrase>, familiarity with <phrase>Python</phrase> and TensorFlow
This course provides <phrase>an introduction</phrase> to <phrase>Deep Learning</phrase>, a field that aims to <phrase>harness</phrase> the enormous amounts of <phrase>data</phrase> that we are surrounded by with <phrase>artificial neural networks</phrase>, allowing for the development of <phrase>self-driving cars</phrase>, speech interfaces, <phrase>genomic</phrase> <phrase>sequence analysis</phrase> and <phrase>algorithmic trading</phrase>.   You will explore important concepts in <phrase>Deep Learning</phrase>, <phrase>train</phrase> deep networks using <phrase>Intel</phrase> Nervana <phrase>Neon</phrase>, apply <phrase>Deep Learning</phrase> to various applications and explore new and emerging <phrase>Deep Learning</phrase> topics.
In this course you will get <phrase>an introduction</phrase> to the main tools and ideas in the <phrase>data</phrase> scientist's toolbox. The course gives an overview of the <phrase>data</phrase>, questions, and tools that <phrase>data</phrase> analysts and <phrase>data</phrase> scientists work with. There are two components to this course. The first is a conceptual introduction to the ideas behind turning <phrase>data</phrase> into actionable <phrase>knowledge</phrase>. The second is a practical introduction to the tools that will be used in the program like <phrase>version control</phrase>, <phrase>markdown</phrase>, git, <phrase>GitHub</phrase>, R, and RStudio.
<phrase>Spreadsheet</phrase> <phrase>software</phrase> remains one of the most ubiquitous pieces of <phrase>software</phrase> used in workplaces across the world. Learning to confidently operate this <phrase>software</phrase> means adding a highly valuable asset to your employability portfolio. In the <phrase>United States</phrase> alone, millions of job advertisements requiring <phrase>Excel</phrase> skills are posted every day. <phrase>Research</phrase> by Burning <phrase>Glass</phrase> Technologies and <phrase>Capital</phrase> One shows that digitals skills <phrase>lead</phrase> to higher income and better <phrase>employment</phrase> opportunities at a time when <phrase>digital</phrase> skills job are growing much faster than non-<phrase>digital</phrase> jobs.  In this second course of our <phrase>Excel</phrase> specialization <phrase>Excel</phrase> Skills for <phrase>Business</phrase> you will build on the strong foundations of the Essentials course. Intermediate Skills I will expand your <phrase>Excel</phrase> <phrase>knowledge</phrase> to new horizons. You are going to discover a whole <phrase>range</phrase> of skills and techniques that will become a standard component of your everyday use of <phrase>Excel</phrase>. In this course, you will build a solid layer of more advanced skills so you can manage <phrase>large datasets</phrase> and create meaningful reports. These key techniques and tools will allow you to add a sophisticated layer of <phrase>automation</phrase> and efficiency to your everyday tasks in <phrase>Excel</phrase>.  Once again, we have brought together a great teaching team that will be with you every <phrase>step</phrase> of the way. Prashan and Nicky will guide you through each week (and I am even going to make a <phrase>guest appearance</phrase> in Week 5 to help you learn about my favourite tool in <phrase>Excel</phrase> - shh, no spoilers!). Work through each new challenge <phrase>step</phrase>-by-<phrase>step</phrase> and in no time you will surprise yourself by how far you have come. This time around, we are going to follow Uma's trials and tribulations as she is trying to find her feet in a new position in the fictitious <phrase>company</phrase> PushPin. For those of you who have done the Essentials course, you will already be familiar with the <phrase>company</phrase>. Working through her challenges which are all too common ones that we encounter everyday, will help you to more easily relate to the skills and techniques learned in each week and apply them to familiar and new contexts.
If you are a <phrase>software developer</phrase> who wants to build scalable <phrase>AI</phrase>-powered <phrase>algorithms</phrase>, you need to understand how to use the tools to build them. This Specialization will teach you <phrase>best practices</phrase> for using TensorFlow, a popular <phrase>open-source</phrase> framework for <phrase>machine learning</phrase>.  In Course 3 of the deeplearning.ai TensorFlow Specialization, you will build <phrase>natural language processing</phrase> systems using TensorFlow. You will learn to process text, including tokenizing and representing sentences as vectors, so that they can be input to a <phrase>neural network</phrase>. You’ll also learn to apply RNNs, GRUs, and LSTMs in TensorFlow. Finally, you’ll get to <phrase>train</phrase> an  <phrase>LSTM</phrase> on existing text to create original <phrase>poetry</phrase>!  The <phrase>Machine Learning</phrase> course and <phrase>Deep Learning</phrase> Specialization from Andrew Ng teach the most important and foundational principles of <phrase>Machine Learning</phrase> and <phrase>Deep Learning</phrase>. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to <phrase>real-world</phrase> problems. To develop a deeper understanding of how <phrase>neural networks</phrase> work, we recommend that you take the <phrase>Deep Learning</phrase> Specialization.
Welcome to this course on <phrase>Data</phrase> Analytics for Lean <phrase>Six Sigma</phrase>.   In this course you will learn <phrase>data</phrase> analytics techniques that are typically useful within Lean <phrase>Six Sigma</phrase> improvement projects. At the end of this course you are able to analyse and interpret <phrase>data</phrase> gathered within such a project. You will be able to use Minitab to analyse the <phrase>data</phrase>. I will also briefly explain what Lean <phrase>Six Sigma</phrase> is.  I will emphasize on use of <phrase>data</phrase> analytics tools and the interpretation of the outcome. I will use many different examples from actual Lean <phrase>Six Sigma</phrase> projects to illustrate all tools. I will not discuss any <phrase>mathematical</phrase> background.   The setting we chose for our <phrase>data</phrase> example is a Lean <phrase>Six Sigma</phrase> improvement project. However <phrase>data</phrase> analytics tools are very widely applicable. So you will find that you will learn techniques that you can use in a broader setting apart from improvement projects.   I hope that you enjoy this course and good luck! Dr. Inez Zwetsloot & the <phrase>IBIS</phrase> <phrase>UvA</phrase> team
This course builds on the theory and foundations of <phrase>marketing</phrase> analytics and focuses on <phrase>practical application</phrase> by demystifying the use of <phrase>data</phrase> in <phrase>marketing</phrase> and helping you realize the power of visualizing <phrase>data</phrase> with artful use of numbers found in the <phrase>digital</phrase> space.  This course is part of the iMBA offered by the <phrase>University</phrase> of <phrase>Illinois</phrase>, a flexible, fully-accredited online <phrase>MBA</phrase> at an incredibly competitive price. For more <phrase>information</phrase>, please see the Resource page in this course and onlinemba.illinois.edu.
Welcome to Introduction to <phrase>Statistics</phrase> & <phrase>Data</phrase> Analysis in <phrase>Public Health</phrase>!  This course will teach you the core <phrase>building blocks</phrase> of <phrase>statistical analysis</phrase> - types of variables, common <phrase>distributions</phrase>, <phrase>hypothesis testing</phrase> - but, more than that, it will enable you to take a <phrase>data</phrase> set you've never seen before, describe its keys features, get to know its strengths and quirks, <phrase>run</phrase> some vital <phrase>basic</phrase> analyses and then formulate and <phrase>test</phrase> hypotheses based on means and proportions. You'll then have a solid grounding to move on to more sophisticated analysis and take the other courses in the series. You'll learn the popular, flexible and completely <phrase>free software</phrase> R, used by <phrase>statistics</phrase> and <phrase>machine learning</phrase> practitioners everywhere. It's hands-on, so you'll first learn about how to phrase a testable <phrase>hypothesis</phrase> via examples of <phrase>medical</phrase> <phrase>research</phrase> as reported by the <phrase>media</phrase>. Then you'll work through a <phrase>data</phrase> set on <phrase>fruit</phrase> and <phrase>vegetable</phrase> eating habits: <phrase>data</phrase> that are realistically messy, because that's what <phrase>public health</phrase> <phrase>data</phrase> sets are like in <phrase>reality</phrase>. There will be <phrase>mini</phrase>-quizzes with <phrase>feedback</phrase> along the way to check your understanding. The course will sharpen your ability to think critically and not take things for granted: in this age of uncontrolled <phrase>algorithms</phrase> and <phrase>fake news</phrase>, these skills are more important than ever.  Prerequisites  Some formulae are given to aid understanding, but this is not one of those courses where you need a <phrase>mathematics</phrase> <phrase>degree</phrase> to follow it. You will need only <phrase>basic</phrase> <phrase>numeracy</phrase> (for example, we will not use <phrase>calculus</phrase>) and familiarity with <phrase>graphical</phrase> and tabular ways of presenting <phrase>results</phrase>. No <phrase>knowledge</phrase> of R or <phrase>programming</phrase> is assumed.
<phrase>Google</phrase> <phrase>Cloud</phrase> チームが提供する <phrase>Coursera</phrase> 専門講座、「From <phrase>Data</phrase> to Insights with <phrase>Google</phrase> <phrase>Cloud</phrase> Platform」へようこそ。データ エンスージアストの Evan Jones です。このコースのガイドを務めます。    本専門講座のこの最初のコースは、「Exploring and Preparing your <phrase>Data</phrase> with BigQuery」です。ここでは、データ アナリストが共通して直面する課題と、その課題を <phrase>Google</phrase> <phrase>Cloud</phrase> Platform のビッグデータ ツールを使用して解決する方法を取り上げます。その過程で <phrase>SQL</phrase> を学習しながら、BigQuery と <phrase>Cloud</phrase> Dataprep を使用してデータセットを分析し、変換する方法について理解を深めます。    このコースは完了までに約 1 週間、合計 5～7 時間の作業時間を必要とします。修了すると、BigQuery 一般公開データセット内の何百万ものレコードに対してクエリを行い、分析情報を引き出せるようになります。データセットの品質を評価し、BigQuery に出力される自動データ クレンジング パイプラインを開発する方法を学習します。さらに、実際の <phrase>Google</phrase> アナリティクスの <phrase>e</phrase> コマース データセットで、マーケティング分析情報を得るための <phrase>SQL</phrase> の作成とトラブルシューティングの演習を行います。    >>> この専門講座に登録すると、よくある質問に記載されているとおり Qwiklabs の利用規約に同意したことになります。詳細については、https://qwiklabs.com/terms_of_service をご覧ください。<<<
In this final course, you will put together your <phrase>knowledge</phrase> from Courses 1, 2 and 3 to implement a complete <phrase>RL</phrase> <phrase>solution</phrase> to a problem. This capstone will let you see how each component---problem formulation, <phrase>algorithm</phrase> selection, parameter selection and representation <phrase>design</phrase>---fits together into a complete <phrase>solution</phrase>, and how to make appropriate choices when deploying <phrase>RL</phrase> in the <phrase>real world</phrase>. This project will require you to implement both the environment to stimulate your problem, and a control agent with <phrase>Neural Network</phrase> <phrase>function</phrase> approximation. In addition, you will conduct a scientific study of your learning system to develop your ability to assess the robustness of <phrase>RL</phrase> agents. To use <phrase>RL</phrase> in the <phrase>real world</phrase>, it is critical to (a) appropriately formalize the problem as an MDP, (b) select appropriate <phrase>algorithms</phrase>, (c ) identify what choices in your implementation will have large impacts on performance and (d) validate the expected behaviour of your <phrase>algorithms</phrase>. This capstone is valuable for anyone who is planning on using <phrase>RL</phrase> to solve real problems.  By the end of this course, you will be able to:   Complete an <phrase>RL</phrase> <phrase>solution</phrase> to a problem, starting from problem formulation, appropriate <phrase>algorithm</phrase> selection and implementation and <phrase>empirical study</phrase> into the effectiveness of the <phrase>solution</phrase>.
In this course you will learn how to evaluate <phrase>recommender systems</phrase>.  You will gain familiarity with several families of metrics, including ones to measure prediction accuracy, rank accuracy, <phrase>decision-support</phrase>, and other factors such as diversity, product coverage, and <phrase>serendipity</phrase>.  You will learn how different metrics relate to different user goals and <phrase>business</phrase> goals.  You will also learn how to rigorously conduct offline evaluations (i.e., how to prepare and sample <phrase>data</phrase>, and how to aggregate <phrase>results</phrase>).  And you will learn about online (<phrase>experimental</phrase>) evaluation.  At the completion of this course you will have the tools you need to compare different <phrase>recommender system</phrase> alternatives for a wide <phrase>variety</phrase> of uses.
Результаты подсчета чего-либо или вероятности возникновения событий - это величины, практически не поддающиеся моделированию при помощи обычных линейных моделей, так как не подчиняются нормальному распределению. Обобщенные линейные модели (<phrase>Generalized Linear</phrase> Models, GLM) позволяют обойти это ограничение. В этом курсе мы постараемся с минимальным количеством математики  рассказать об устройстве GLM и многочисленных подводных камнях, связанных с анализом. GLM для счетных данных основаны на распределении Пуассона или отрицательном биномиальном распределении. Модели для бинарных данных (например, логистическая регрессия) - на биномиальном распределении. Мы обсудим особенности диагностики моделей, возникающие в зависимости от выбранного распределения. Параметры GLM подбирают при помощи метода максимального правдоподобия, поэтому и аппарат тестирования гипотез и техники упрощения моделей довольно сильно отличаются от привычного для простых линейных моделей. Для анализа данных мы будем использовать язык R, чтобы вы могли лучше разобраться в тонкостях работы с обобщенными линейными моделями. Вас ждут интерактивные задания на платформе Stepic и проект по анализу данных в конце курса.  Этот курс для всех, кто хочет научиться строить модели для счетных или бинарных величин. Для успешного прохождения пригодятся базовые представления о регрессионном анализе, умение создавать простейшие .<phrase>html</phrase> документы при помощи rmarkdown и knitr.
This course is intended as a first <phrase>step</phrase> for learners who seek to become producers of <phrase>social science</phrase> <phrase>research</phrase>. It is organized as <phrase>an introduction</phrase> to the <phrase>design</phrase> and execution of a <phrase>research</phrase> study. It introduces the <phrase>key elements</phrase> of a proposal for a <phrase>research</phrase> study, and explains the role of each. It reviews the <phrase>major</phrase> types of qualitative and <phrase>quantitative data</phrase> used in <phrase>social science</phrase> <phrase>research</phrase>, and then introduces some of the most important sources of existing <phrase>data</phrase> available freely or by <phrase>application</phrase>, worldwide and for <phrase>China</phrase>. The course offers an overview of <phrase>basic</phrase> principles in the <phrase>design</phrase> of surveys, including a brief introduction to <phrase>sampling</phrase>. <phrase>Basic</phrase> techniques for <phrase>quantitative analysis</phrase> are also introduced, along with a review of common challenges that arise in the interpretation of <phrase>results</phrase>. <phrase>Professional</phrase> and <phrase>ethical</phrase> issues that often arise in the conduct of <phrase>research</phrase> are also discussed.  The course concludes with <phrase>an introduction</phrase> to the options for further study available to the interested <phrase>student</phrase>, and an overview of the key steps involved in selecting postgraduate programs and applying for admission. Learners who complete the course will be able to make an informed decision about whether to pursue advanced studies, and should be adequately prepared to write an <phrase>application</phrase> for postgraduate study that exhibits <phrase>basic</phrase> understanding of key aspects of <phrase>social science</phrase> <phrase>research</phrase> paradigms and methodologies.  Explore the big questions in <phrase>social science</phrase> and learn how you can be a <phrase>producer</phrase> of <phrase>social science</phrase> <phrase>research</phrase>.   Course Overview <phrase>video</phrase>: https://youtu.be/QuMOAlwhpvU  Part 1 should be completed before taking this course:  https://www.coursera.org/learn/<phrase>social-science</phrase>-study-<phrase>chinese</phrase>-<phrase>society</phrase>
Enterprises that seek to become proficient in advanced <phrase>manufacturing</phrase> must incorporate <phrase>manufacturing</phrase> <phrase>management</phrase> tools and integrate <phrase>data</phrase> throughout the <phrase>supply chain</phrase> to be successful. This course will make students aware of what a digitally connected enterprise is, as they learn about the operational complexity of enterprises, <phrase>business process</phrase> optimization and the concept of an integrated product-process-value chain.   Students will become acquainted with the available tools, technologies and techniques for <phrase>aggregation</phrase> and <phrase>integration</phrase> of <phrase>data</phrase> throughout the <phrase>manufacturing</phrase> <phrase>supply chain</phrase> and entire <phrase>product life-cycle</phrase>. They will receive foundational <phrase>knowledge</phrase> to assist in efforts to facilitate <phrase>design</phrase>, planning, and <phrase>production</phrase> scheduling of <phrase>goods and services</phrase> by applying <phrase>product life cycle</phrase> <phrase>data</phrase>.    Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the sixth course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
この専門講座の 3 番目のコースは、「Achieving Advanced Insights with BigQuery」です。ここでは、高度なファンクションと、複雑なクエリを管理可能なステップに分割する方法を学びながら、<phrase>SQL</phrase> に関する知識を深めます。    BigQuery の内部アーキテクチャ（列ベースのシャーディング ストレージ）についてや、<phrase>ARRAY</phrase> と STRUCT を使用した、ネストされたフィールドと繰り返しフィールドなどの高度な <phrase>SQL</phrase> トピックについて説明します。最後に、クエリのパフォーマンスを最適化する方法と、承認済みビューを使用してデータを保護する方法について説明します。    >>> この専門講座に登録すると、よくある質問に記載されているとおり Qwiklabs の利用規約に同意したことになります。詳細については、https://qwiklabs.com/terms_of_service をご覧ください。<<<
Este curso intensivo de <phrase>uma</phrase> semana é <phrase>uma</phrase> continuação <phrase>dos</phrase> cursos anteriores <phrase>da</phrase> especialização <phrase>Data</phrase> <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Por meio de <phrase>uma</phrase> combinação de palestras <phrase>em</phrase> vídeo, demonstrações <phrase>e</phrase> laboratórios práticos, você aprenderá a criar <phrase>e</phrase> gerenciar clusters de computação para executar jobs do <phrase>Hadoop</phrase>, Spark, <phrase>Pig</phrase> <phrase>e</phrase>/ou Hive no <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Você também verá como acessar várias opções do <phrase>Cloud</phrase> Storage a partir <phrase>dos</phrase> seus clusters de computação, além de integrar recursos de aprendizado de máquina do <phrase>Google</phrase> aos seus programas de análise.    <phrase>Nos</phrase> laboratórios práticos, você poderá criar <phrase>e</phrase> gerenciar clusters do Dataproc usando o <phrase>console</phrase> <phrase>da</phrase> Web <phrase>e</phrase> a <phrase>CLI</phrase>, além de usar um cluster para executar jobs do Spark <phrase>e</phrase> do <phrase>Pig</phrase>. <phrase>Em</phrase> seguida, você criará blocos de notas do Jupyter que <phrase>se</phrase> integram <phrase>ao</phrase> BigQuery <phrase>e</phrase> <phrase>ao</phrase> <phrase>Cloud</phrase> Storage <phrase>e</phrase> utilizam Spark. Por fim, você integrará as <phrase>APIs</phrase> de aprendizado de máquina à sua análise de dados.  Pré-requisitos • curso <phrase>Google</phrase> <phrase>Cloud</phrase> Platform Fundamentals: <phrase>Big Data</phrase> & <phrase>Machine Learning</phrase> (ou experiência equivalente) • conhecimentos básicos de <phrase>Python</phrase>
This course will enable you <phrase>mastering</phrase> <phrase>machine-learning</phrase> approaches in the <phrase>area</phrase> of <phrase>investment management</phrase>. It has been designed by two thought leaders in their field, Lionel Martellini from EDHEC-<phrase>Risk</phrase> Institute and John Mulvey from <phrase>Princeton University</phrase>. Starting from the basics, they will help you build practical skills to understand <phrase>data science</phrase> so you can make the best portfolio decisions.  The course will start with <phrase>an introduction</phrase> to the fundamentals of <phrase>machine learning</phrase>, followed by an in-depth discussion of the <phrase>application</phrase> of these techniques to portfolio <phrase>management</phrase> decisions, including the <phrase>design</phrase> of more robust factor models, the <phrase>construction</phrase> of portfolios with improved diversification benefits, and the implementation of more efficient <phrase>risk management</phrase> models.   We have designed a 3-<phrase>step</phrase> learning process: first, we will <phrase>introduce</phrase> a meaningful <phrase>investment</phrase> problem and see how this problem can be addressed using <phrase>statistical techniques</phrase>. Then, we will see how this new insight from <phrase>Machine learning</phrase> can complete and improve the relevance of the analysis.  You will have the opportunity to capitalize on videos and recommended readings to level up your financial <phrase>expertise</phrase>, and to use the quizzes and <phrase>Jupiter</phrase> notebooks to ensure grasp of concept.  At the end of this course, you will <phrase>master</phrase> the various <phrase>machine learning</phrase> techniques in <phrase>investment management</phrase>.
In this course, you will develop and <phrase>test</phrase> hypotheses about your <phrase>data</phrase>. You will learn a <phrase>variety</phrase> of statistical <phrase>tests</phrase>, as well as strategies to know how to apply the appropriate one to your specific <phrase>data</phrase> and question. Using your choice of two powerful statistical <phrase>software</phrase> packages (<phrase>SAS</phrase> or <phrase>Python</phrase>), you will explore <phrase>ANOVA</phrase>, <phrase>Chi</phrase>-Square, and Pearson correlation analysis. This course will guide you through <phrase>basic</phrase> statistical principles to give you the tools to <phrase>answer questions</phrase> you have developed. Throughout the course, you will share your progress with others to gain valuable <phrase>feedback</phrase> and provide insight to other learners about their work.
In this course you will learn how to apply <phrase>satisfiability</phrase> (<phrase>SAT</phrase>/SMT) tools to solve a wide <phrase>range</phrase> of problems. Several <phrase>basic</phrase> examples are given to get the flavor of the applications: fitting rectangles to be applied for <phrase>printing</phrase> posters, <phrase>scheduling problems</phrase>, solving puzzles, and <phrase>program correctness</phrase>. Also underlying theory is presented: resolution as a <phrase>basic</phrase> approach for <phrase>propositional</phrase> <phrase>satisfiability</phrase>, the CDCL framework to scale up for big formulas, and the <phrase>simplex</phrase> <phrase>method</phrase> to deal with linear inequallities.  The <phrase>light</phrase> weight approach to following this course is just watching the lectures and do the corresponding quizzes. To get a flavor of the topic this may work out fine. However, the much more interesting approach is to use this as a basis to apply <phrase>SAT</phrase>/SMT yourself on several problems, for <phrase>instance</phrase> on the problems presented in the honor's assignment.
In diesem einwöchigen <phrase>On-Demand</phrase>-Intensivkurs erhalten die Teilnehmer eine Einführung in die Funktionen der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform (GCP) für <phrase>Big Data</phrase> und maschinelles Lernen. Dabei wird ein kurzer Überblick über die <phrase>Google</phrase> <phrase>Cloud</phrase> Platform geboten, während die Funktionen für die Datenverarbeitung eingehender behandelt werden.  Nach Abschluss dieses Kurses sind die Teilnehmer in der Lage: • den Zweck und den Nutzen der wichtigsten Produkte für <phrase>Big Data</phrase> und maschinelles Lernen in der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform zu beschreiben, • vorhandene <phrase>MySQL</phrase>- und <phrase>Hadoop</phrase>-/<phrase>Pig</phrase>-/Spark-/Hive-Arbeitslasten <phrase>mit</phrase> <phrase>Cloud</phrase> <phrase>SQL</phrase> und <phrase>Cloud</phrase> Dataproc zur <phrase>Google</phrase> <phrase>Cloud</phrase> Platform zu migrieren, • <phrase>mit</phrase> BigQuery und <phrase>Cloud</phrase> Datalab interaktive Datenanalysen auszuführen, • eine Auswahl zwischen <phrase>Cloud</phrase> <phrase>SQL</phrase>, <phrase>BigTable</phrase> und Datastore zu treffen, • <phrase>mit</phrase> TensorFlow ein neuronales Netzwerk zu trainieren und zu verwenden, • eine Auswahl zwischen verschiedenen Datenverarbeitungsprodukten in der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform zu treffen.  Wenn Sie sich zu diesem Kurs anmelden möchten, sollten Sie ungefähr ein (1) Jahr Erfahrung in einem <phrase>oder</phrase> mehreren der folgenden Bereiche haben: • Gängige Abfragesprache wie <phrase>SQL</phrase> • Extraktions-, Transformations-, Ladeaktivitäten • Datenmodellierung • Maschinelles Lernen und/<phrase>oder</phrase> Statistik • Programmierung in <phrase>Python</phrase>  Hinweise zum <phrase>Google</phrase>-Konto: • Sie benötigen ein <phrase>Google</phrase>-/<phrase>Gmail</phrase>-Konto und eine Kreditkarte <phrase>oder</phrase> ein Bankkonto, um sich für den kostenlosen <phrase>Test</phrase> der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform zu registrieren (in <phrase>China</phrase> stehen die Dienste von <phrase>Google</phrase> derzeit nicht zur Verfügung). • Wenn Sie <phrase>Google</phrase> <phrase>Cloud</phrase> Platform-Kunde <phrase>mit</phrase> Rechnungsadresse in der Europäischen <phrase>Union</phrase> (<phrase>EU</phrase>) <phrase>oder</phrase> Russland sind, lesen Sie sich die Dokumentation zur Mehrwertsteuer unter https://cloud.google.com/billing/docs/resources/vat-overview durch. • Weitere häufig gestellte Fragen zum kostenlosen <phrase>Test</phrase> der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform finden Sie unter https://cloud.google.com/<phrase>free</phrase>-trial/.  Buscando <phrase>la</phrase> versión <phrase>en</phrase> español de este curso? Visita https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>es</phrase>/ このコースの日本語版をお探しですか？https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>jp</phrase>/
The majority of <phrase>data</phrase> in the world is unlabeled and unstructured. Shallow <phrase>neural networks</phrase> cannot easily capture relevant structure in, for <phrase>instance</phrase>, images, <phrase>sound</phrase>, and <phrase>textual data</phrase>. Deep networks are capable of discovering hidden structures within this type of <phrase>data</phrase>. In this course you’ll use TensorFlow <phrase>library</phrase> to apply <phrase>deep learning</phrase> to different <phrase>data</phrase> types in <phrase>order</phrase> to solve <phrase>real world</phrase> problems. <phrase>Learning Outcomes</phrase>: After completing this course, learners will be able to: •	explain foundational TensorFlow concepts such as the main functions, operations and the execution pipelines.  •	describe how TensorFlow can be used in <phrase>curve fitting</phrase>, <phrase>regression</phrase>, classification and minimization of error functions.  •	understand different types of Deep Architectures, such as Convolutional Networks, Recurrent Networks and Autoencoders. •	apply TensorFlow for <phrase>backpropagation</phrase> to tune the weights and biases while the <phrase>Neural Networks</phrase> are being trained.
Dies ist eine Einführung in die Grundlagen von TensorFlow. Darin werden die Konzepte und <phrase>APIs</phrase> erläutert, die Sie zum Schreiben verteilter Modelle für maschinelles Lernen benötigen. Außerdem wird anhand eines TensorFlow-Modells erklärt, wie Sie Modelle in großem Umfang trainieren und <phrase>mit</phrase> <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> effektive Vorhersagen treffen können.  Lernziele: Modelle für maschinelles Lernen in TensorFlow erstellen Diverse Herausforderungen <phrase>mit</phrase> TensorFlow-Bibliotheken lösen Gängige Codefehler in TensorFlow beheben <phrase>Mit</phrase> tf.estimator ein <phrase>ML</phrase>-Modell erstellen, trainieren und bewerten <phrase>ML</phrase>-Modelle im großen Umfang <phrase>mit</phrase> <phrase>Cloud</phrase> <phrase>ML</phrase> <phrase>Engine</phrase> trainieren, bereitstellen und in der Produktion verwenden
<phrase>Machine learning</phrase> is the study that allows <phrase>computers</phrase> to adaptively improve their performance with experience accumulated from the <phrase>data</phrase> observed. Our two sister courses teach the most fundamental algorithmic, theoretical and practical tools that any user of <phrase>machine learning</phrase> needs to know. This first course of the two would focus more on <phrase>mathematical</phrase> tools, and the other course would focus more on algorithmic tools. [機器學習旨在讓電腦能由資料中累積的經驗來自我進步。我們的兩項姊妹課程將介紹各領域中的機器學習使用者都應該知道的基礎演算法、理論及實務工具。本課程將較為著重數學類的工具，而另一課程將較為著重方法類的工具。]
This course describes <phrase>Bayesian statistics</phrase>, in which one's inferences about parameters or hypotheses are updated as evidence accumulates. You will learn to use <phrase>Bayes</phrase>’ rule to transform prior <phrase>probabilities</phrase> into posterior <phrase>probabilities</phrase>, and be introduced to the underlying theory and perspective of the <phrase>Bayesian</phrase> <phrase>paradigm</phrase>. The course will apply <phrase>Bayesian</phrase> methods to several <phrase>practical problems</phrase>, to show <phrase>end-to-end</phrase> <phrase>Bayesian</phrase> analyses that move from framing the question to building models to eliciting prior <phrase>probabilities</phrase> to implementing in R (<phrase>free</phrase> statistical <phrase>software</phrase>) the final <phrase>posterior distribution</phrase>. Additionally, the course will <phrase>introduce</phrase> credible regions, <phrase>Bayesian</phrase> comparisons of means and proportions, <phrase>Bayesian</phrase> <phrase>regression</phrase> and inference using multiple models, and discussion of <phrase>Bayesian</phrase> prediction.  We assume learners in this course have <phrase>background knowledge</phrase> equivalent to what is <phrase>covered</phrase> in the earlier three courses in this specialization: "Introduction to <phrase>Probability</phrase> and <phrase>Data</phrase>," "Inferential <phrase>Statistics</phrase>," and "<phrase>Linear Regression</phrase> and Modeling."
By the end of this course, learners will understand what <phrase>computer vision</phrase> is, as well as its mission of making <phrase>computers</phrase> see and interpret the world as humans do, by learning <phrase>core concepts</phrase> of the field and receiving <phrase>an introduction</phrase> to <phrase>human</phrase> vision capabilities. They are equipped to identify some key <phrase>application</phrase> areas of <phrase>computer vision</phrase> and understand the <phrase>digital</phrase> imaging process. The course covers crucial elements that enable <phrase>computer vision</phrase>: <phrase>digital signal processing</phrase>, <phrase>neuroscience</phrase> and <phrase>artificial intelligence</phrase>. Topics include color, <phrase>light</phrase> and image formation; early, mid- and <phrase>high</phrase>-level vision; and <phrase>mathematics</phrase> essential for <phrase>computer vision</phrase>. Learners will be able to apply <phrase>mathematical</phrase> techniques to complete <phrase>computer vision</phrase> tasks.   This course is ideal for anyone curious about or interested in exploring the concepts of <phrase>computer vision</phrase>. It is also useful for those who desire a refresher course in <phrase>mathematical</phrase> concepts of <phrase>computer vision</phrase>. Learners should have <phrase>basic</phrase> <phrase>programming</phrase> skills and experience (understanding of for loops, if/else statements), specifically in <phrase>MATLAB</phrase> (<phrase>Mathworks</phrase> provides the basics here: https://www.mathworks.com/learn/tutorials/<phrase>matlab</phrase>-onramp.html). Learners should also be familiar with the following: <phrase>basic</phrase> <phrase>linear algebra</phrase> (matrix <phrase>vector</phrase> operations and <phrase>notation</phrase>), 3D co-ordinate systems and transformations, <phrase>basic</phrase> <phrase>calculus</phrase> (derivatives and <phrase>integration</phrase>) and <phrase>basic</phrase> <phrase>probability</phrase> (<phrase>random variables</phrase>).    Material includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing <phrase>computer vision</phrase> programs through online labs using <phrase>MATLAB</phrase>* and supporting toolboxes.  This is the first course in the <phrase>Computer</phrase> Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a <phrase>video</phrase> overview at https://youtu.be/OfxVUSCPXd0.   * A <phrase>free</phrase> license to <phrase>install</phrase> <phrase>MATLAB</phrase> for the duration of the course is available from <phrase>MathWorks</phrase>.
"A picture is worth a thousand words". We are all familiar with this expression. It especially applies when trying to explain the insight obtained from the analysis of increasingly <phrase>large datasets</phrase>. <phrase>Data visualization</phrase> plays an essential role in the representation of both small and <phrase>large-scale</phrase> <phrase>data</phrase>.  One of the key skills of a <phrase>data</phrase> <phrase>scientist</phrase> is the ability to tell a compelling story, visualizing <phrase>data</phrase> and findings in an approachable and stimulating way. Learning how to leverage a <phrase>software</phrase> tool to visualize <phrase>data</phrase> will also enable you to extract <phrase>information</phrase>, better understand the <phrase>data</phrase>, and make more effective decisions.  The main goal of this <phrase>Data</phrase> Visualization with <phrase>Python</phrase> course is to teach you how to take <phrase>data</phrase> that at first glance has little meaning and present that <phrase>data</phrase> in a form that <phrase>makes sense</phrase> to people. Various techniques have been developed for presenting <phrase>data</phrase> visually but in this course, we will be using several <phrase>data</phrase> visualization <phrase>libraries</phrase> in <phrase>Python</phrase>, namely Matplotlib, Seaborn, and Folium.  LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
Este curso <phrase>te</phrase> proporcionará las bases del lenguaje de programación estadística R, <phrase>la</phrase> lengua franca <phrase>de la</phrase> estadística, <phrase>el</phrase> cual <phrase>te</phrase> permitirá escribir programas que lean, manipulen y analicen datos cuantitativos. <phrase>Te</phrase> explicaremos <phrase>la</phrase> instalación del lenguaje; también verás una introducción a los sistemas base de gráficos y al paquete para graficar ggplot2, para visualizar estos datos. Además también abordarás <phrase>la</phrase> utilización de uno de los IDEs más populares entre <phrase>la</phrase> comunidad de usuarios de R, llamado RStudio.  Objetivo  Al término del curso:  Utilizarás <phrase>el</phrase> lenguaje de programación R con <phrase>el</phrase> <phrase>fin</phrase> de manipular datos, generar análisis estadísticos y representación gráfica, a través del procesamiento de datos cuantitativos.  Forma de trabajo  Este curso busca introducirte <phrase>en</phrase> <phrase>el</phrase> lenguaje de programación estadística R, un lenguaje computacional diseñado para <phrase>el</phrase> análisis estadístico de datos. Este curso está dirigido a estudiantes y profesionales que tienen interés <phrase>en</phrase> poder utilizar esta herramienta, para leer, manipular, analizar y graficar datos.   Utilizarás un <phrase>IDE</phrase> (Ambiente de Desarrollo Integrado) muy popular para trabajar con <phrase>el</phrase> lenguaje R, llamado RStudio, que <phrase>se</phrase> <phrase>ha</phrase> vuelto <phrase>el</phrase> <phrase>IDE</phrase> de facto para programar <phrase>en</phrase> R.  <phrase>En</phrase> cada módulo encontrarás videos que <phrase>te</phrase> guiarán <phrase>en</phrase> <phrase>la</phrase> instalación de las herramientas a utilizar, así como explicaciones de las operaciones básicas y los elementos específicos que ofrecen un manejo más profundo del lenguaje. También hallarás algunas referencias bibliográficas para ahondar <phrase>en</phrase> <phrase>el</phrase> tema que sea de <phrase>tu</phrase> interés.  Para complementar las lecciones, realizarás prácticas con <phrase>el</phrase> lenguaje, las cuales tendrán valor para <phrase>la</phrase> evaluación.
This 2-week accelerated <phrase>on-demand</phrase> course introduces participants to the <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> capabilities of <phrase>Google</phrase> <phrase>Cloud</phrase> Platform (GCP). It provides a quick overview of the <phrase>Google</phrase> <phrase>Cloud</phrase> Platform and a deeper dive of the <phrase>data processing</phrase> capabilities.  At the end of this course, participants will be able to: • Identify the purpose and value of the key <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> <phrase>products</phrase> in the <phrase>Google</phrase> <phrase>Cloud</phrase> Platform • Use CloudSQL and <phrase>Cloud</phrase> Dataproc to migrate existing <phrase>MySQL</phrase> and <phrase>Hadoop</phrase>/<phrase>Pig</phrase>/Spark/Hive workloads to <phrase>Google</phrase> <phrase>Cloud</phrase> Platform • Employ BigQuery and <phrase>Cloud</phrase> Datalab to <phrase>carry out</phrase> interactive <phrase>data</phrase> analysis • Choose between <phrase>Cloud</phrase> <phrase>SQL</phrase>, <phrase>BigTable</phrase> and Datastore • <phrase>Train</phrase> and use a <phrase>neural network</phrase> using TensorFlow • Choose between different <phrase>data</phrase> processing <phrase>products</phrase> on the <phrase>Google</phrase> <phrase>Cloud</phrase> Platform  Before enrolling in this course, participants should have roughly one (1) year of experience with one or more of the following: • A common <phrase>query language</phrase> such as <phrase>SQL</phrase> • Extract, transform, load activities • <phrase>Data modeling</phrase> • <phrase>Machine learning</phrase> and/or <phrase>statistics</phrase> • <phrase>Programming</phrase> in <phrase>Python</phrase>  <phrase>Google</phrase> Account Notes: • <phrase>Google</phrase> services are currently unavailable in <phrase>China</phrase>.
This course provides you with an understanding of what <phrase>Enterprise Systems</phrase> (also commonly termed as <phrase>Enterprise Resource Planning</phrase> Systems, ERPs) are. After learning about what these systems are, we would touch upon why these systems are useful to companies, through which you would get to see the various jobs and positions that are associated with the use and deployment of ERPs.   In this course, you would also develop an appreciation of the managerial aspects related to the selection and implementation of ERPs. Specifically, we would touch on the important points to consider when shortlisting and purchasing an <phrase>ERP</phrase>, the approaches taken in <phrase>ERP</phrase> implementation, and <phrase>change management</phrase> techniques to utilize when an <phrase>organization</phrase> is undergoing <phrase>ERP</phrase> implementation. At the end of this class, you will be endowed with practical <phrase>knowledge</phrase> that would help you to <phrase>address</phrase> <phrase>real world</phrase> <phrase>business</phrase> problems associated with <phrase>ERP</phrase> usage and implementation.
This project completer has <phrase>proven</phrase> a deep understanding on massive parallel <phrase>data</phrase> processing, <phrase>data</phrase> exploration and visualization, advanced <phrase>machine learning</phrase> and <phrase>deep learning</phrase> and how to apply his <phrase>knowledge</phrase> in a <phrase>real-world</phrase> practical <phrase>use case</phrase> where he justifies <phrase>architectural</phrase> decisions, proves understanding the characteristics of different <phrase>algorithms</phrase>, frameworks and technologies and how they impact <phrase>model</phrase> performance and <phrase>scalability</phrase>.   Please note: You are requested to create a <phrase>short</phrase> <phrase>video</phrase> presentation at the end of the course. This is <phrase>mandatory</phrase> to <phrase>pass</phrase>. You don't need to share the <phrase>video</phrase> in <phrase>public</phrase>.
Курс посвящен работе в надстройках Power Query и Power Pivot системы MS <phrase>Excel</phrase>. Данный курс является третьим в серии курсов "Практики анализа экономических данных. От простого к сложному". Надстройка Power Query предназначена для того, чтобы собирать данные из разных источников и создавать шаблоны для их обработки, позволяющие не проводить многократно одни и те же манипуляции с данными. Надстройка Power Pivot позволяет создавать модели данных и отчеты сложной структуры на их основе. В данном курсе слушатель сможет применить надстройки <phrase>Microsoft</phrase> Power Query и Power Pivot для сборки данных из разных источников для анализа деятельности компании.  По завершении курса слушатели будут:  Знать: - инструменты работы с данными надстроек Power Query и Power Pivot системы MS <phrase>Excel</phrase>; - правила, возможности и особенности соединения данных из разных таблиц в одну с помощью надстройки Power Query; - правила и особенности создания модели данных с помощью надстройки Power Pivot; - возможности и ограничения DAX-формул.  Уметь: - загружать в MS <phrase>Excel</phrase> данные из таблиц, внешних файлов и папок, собирая их в единый отчет; - преобразовывать данные разной структуры в вид, удобный для составления аналитических отчетов; - создавать модель данных с помощью надстройки Power Pivot; - создавать отчеты на основе данных из нескольких источников.  Владеть: - навыками работы с надстройками Power Query и Power Pivot системы MS <phrase>Excel</phrase>; - навыками создания отчетов с использованием возможностей Power Query и Power Pivot; - навыками создания мер с помощью DAX-формул.
Этот курс - первый в специализации "Анализ данных". Курс будет особенно полезен тем, кто имеет небольшой опыт работы с данными, или хочет освежить знания по теории вероятностей, математической статистике и типах данных.  Сначала мы вспомним основы теории вероятностей и поговорим о случайных величинах и их свойствах, об основных распределениях случайных величин.  Затем перейдем к основным характеристикам распределений: мерам центра и мерам вариативности. Далее обсудим основные типы шкал измерения признаков, а также основные ограничения, которые тип шкалы накладывает на применимые методы анализа данных.  Третья неделя курса посвящена графическому анализу данных и способам визуализации распределений, индивидуальных или совместных. Завершающий модуль курса посвящен выборкам и способам их формирования, а также принципам и инструментам работы с пропущенными и неопределенными значениями.  Вы сможете применить полученные знания, выполнив небольшой проект на реальных данных, предоставленных компанией 2GIS.  Присоединяйтесь!
Este curso é indicado para profissionais que desejam entender de forma fácil o que é <phrase>Big Data</phrase>, conhecer algumas tecnologias de <phrase>Big Data</phrase>, ter acesso a algumas aplicações de Analytics, <phrase>Internet</phrase> das Coisas - <phrase>IOT</phrase> <phrase>e</phrase> de <phrase>Big Data</phrase>. <phrase>Ao</phrase> final do curso você será capaz de participar de um projeto de <phrase>Big Data</phrase> contribuindo com estratégias <phrase>e</phrase> direcionando o projeto para a escolha <phrase>da</phrase> adequada técnica de análise de dados.
This course aims to provide a succinct overview of the <phrase>emerging discipline</phrase> of Materials Informatics at the intersection of <phrase>materials science</phrase>, <phrase>computational science</phrase>, and <phrase>information science</phrase>. Attention is drawn to specific opportunities afforded by this new field in accelerating materials <phrase>development and deployment</phrase> efforts. A particular emphasis is placed on materials exhibiting hierarchical internal structures spanning multiple length/structure scales and the impediments involved in establishing invertible process-structure-<phrase>property</phrase> (<phrase>PSP</phrase>) linkages for these materials. More specifically, it is argued that modern <phrase>data</phrase> sciences (including advanced <phrase>statistics</phrase>, <phrase>dimensionality reduction</phrase>, and formulation of metamodels) and innovative <phrase>cyberinfrastructure</phrase> tools (including <phrase>integration</phrase> platforms, <phrase>databases</phrase>, and customized tools for enhancement of collaborations among cross-disciplinary <phrase>team members</phrase>) are likely to <phrase>play</phrase> a critical and pivotal role in <phrase>addressing</phrase> the above challenges.
This course will provide learners with <phrase>an introduction</phrase> to <phrase>research</phrase> <phrase>data management</phrase> and sharing. After completing this course, learners will understand the diversity of <phrase>data</phrase> and their <phrase>management</phrase> needs across the <phrase>research</phrase> <phrase>data</phrase> lifecycle, be able to identify the components of good <phrase>data management</phrase> plans, and be familiar with <phrase>best practices</phrase> for working with <phrase>data</phrase> including the <phrase>organization</phrase>, documentation, and storage and <phrase>security</phrase> of <phrase>data</phrase>. Learners will also understand the impetus and importance of archiving and sharing <phrase>data</phrase> as well as how to assess the trustworthiness of repositories.   Today, an increasing number of funding agencies, <phrase>journals</phrase>, and other stakeholders are requiring <phrase>data</phrase> producers to share, <phrase>archive</phrase>, and plan for the <phrase>management</phrase> of their <phrase>data</phrase>. In <phrase>order</phrase> to respond to these requirements, researchers and <phrase>information</phrase> professionals will need the <phrase>data management</phrase> and curation <phrase>knowledge</phrase> and skills that support the <phrase>long</phrase>-term preservation, access, and reuse of <phrase>data</phrase>. Effectively managing <phrase>data</phrase> can also help optimize <phrase>research</phrase> outputs, increase the impact of <phrase>research</phrase>, and support open <phrase>scientific inquiry</phrase>. After completing this course, learners will be better equipped to manage <phrase>data</phrase> throughout the entire <phrase>research</phrase> <phrase>data</phrase> lifecycle from <phrase>project planning</phrase> to the end of the project when <phrase>data</phrase> ideally are shared and made available within a trustworthy repository.  This course was developed by the Curating <phrase>Research</phrase> Assets and <phrase>Data</phrase> Using Lifecycle <phrase>Education</phrase> (CRADLE) Project in collaboration with EDINA at the <phrase>University</phrase> of <phrase>Edinburgh</phrase>.   This course was made possible in part by the Institute of <phrase>Museum</phrase> and <phrase>Library</phrase> Services under <phrase>award</phrase> #RE-06-13-0052-13. The views, findings, conclusions or recommendations expressed in this <phrase>Research</phrase> <phrase>Data Management</phrase> and Sharing <phrase>MOOC</phrase> do not necessarily represent those of the Institute of <phrase>Museum</phrase> and <phrase>Library</phrase> Services.  <phrase>Hashtag</phrase>: #RDMSmooc
In this course you will learn how to use D3.js to create powerful visualizations for web. Learning D3.js will enable you to create many different types of visualization and to visualize many different <phrase>data</phrase> types. It will give you the freedom to create something as simple as a <phrase>bar chart</phrase> as well your own new revolutionary technique.   In this course we will <phrase>cover</phrase> the basics of creating visualizations with D3 as well as how to deal with tabular <phrase>data</phrase>, <phrase>geography</phrase> and networks. By the end of this course you will be able to:  - Create bar and line charts - Create choropleth and <phrase>symbol</phrase> maps - Create node-link diagrams and <phrase>tree</phrase> maps - Implement zooming and brushing - Link two or more views through interaction  The course mixes theoretical and practical lectures. We will show you <phrase>step</phrase> by <phrase>step</phrase> how to use the <phrase>library</phrase> to build actual visualizations and what theoretical concepts lie behind them. Throughout the course you will learn skills that will <phrase>lead</phrase> you to building a whole <phrase>application</phrase> by the end of the lectures (a fully working visualization system to visualize <phrase>airlines</phrase> routes).  This course is the third one of the “Specialization in <phrase>Information</phrase> Visualization". The course expects you to have some <phrase>basic</phrase> <phrase>knowledge</phrase> of <phrase>programming</phrase> as well as some <phrase>basic</phrase> visualization skills.
This course will provide you with an overview over existing <phrase>data</phrase> <phrase>products</phrase> and a good understanding of the <phrase>data</phrase> collection <phrase>landscape</phrase>. With the help of various examples you will learn how to identify which <phrase>data</phrase> sources likely matches your <phrase>research</phrase> question, how to turn your <phrase>research</phrase> question into measurable pieces, and how to think about an analysis plan. Furthermore this course will provide you with a <phrase>general</phrase> framework that allows you to not only understand each <phrase>step</phrase> required for a successful <phrase>data</phrase> collection and analysis, but also help you to identify errors associated with different <phrase>data</phrase> sources. You will learn some metrics to quantify each potential error, and thus you will have tools at hand to describe the quality of a <phrase>data</phrase> source. Finally we will <phrase>introduce</phrase> different <phrase>large scale</phrase> <phrase>data</phrase> collection efforts done by <phrase>private</phrase> <phrase>industry</phrase> and <phrase>government</phrase> agencies, and review the learned concepts through these examples. This course is suitable for beginners as well as those that know about one particular <phrase>data</phrase> source, but not others, and are looking for a <phrase>general</phrase> framework to evaluate <phrase>data</phrase> <phrase>products</phrase>.
If you are a <phrase>software developer</phrase> who wants to build scalable <phrase>AI</phrase>-powered <phrase>algorithms</phrase>, you need to understand how to use the tools to build them. This course is part of the upcoming <phrase>Machine Learning</phrase> in Tensorflow Specialization and will teach you <phrase>best practices</phrase> for using TensorFlow, a popular <phrase>open-source</phrase> framework for <phrase>machine learning</phrase>.  In Course 2 of the deeplearning.ai TensorFlow Specialization, you will learn advanced techniques to improve the <phrase>computer vision</phrase> <phrase>model</phrase> you built in Course 1. You will explore how to work with <phrase>real-world</phrase> images in different shapes and sizes, visualize the journey of an image through convolutions to understand how a <phrase>computer</phrase> “sees” <phrase>information</phrase>, plot loss and accuracy, and explore strategies to prevent <phrase>overfitting</phrase>, including augmentation and dropout. Finally, Course 2 will <phrase>introduce</phrase> you to transfer learning and how learned features can be extracted from models.   The <phrase>Machine Learning</phrase> course and <phrase>Deep Learning</phrase> Specialization from Andrew Ng teach the most important and foundational principles of <phrase>Machine Learning</phrase> and <phrase>Deep Learning</phrase>. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to <phrase>real-world</phrase> problems. To develop a deeper understanding of how <phrase>neural networks</phrase> work, we recommend that you take the <phrase>Deep Learning</phrase> Specialization.
<phrase>Data</phrase> about our browsing and buying patterns are everywhere.  From <phrase>credit card</phrase> transactions and <phrase>online shopping</phrase> carts, to <phrase>customer loyalty</phrase> programs and <phrase>user-generated</phrase> ratings/reviews, there is a staggering amount of <phrase>data</phrase> that can be used to describe our past buying behaviors, predict future ones, and prescribe new ways to influence future purchasing decisions. In this course, four of Wharton’s top <phrase>marketing</phrase> professors will provide an overview of key areas of customer analytics: descriptive analytics, <phrase>predictive analytics</phrase>, prescriptive analytics, and their <phrase>application</phrase> to <phrase>real-world</phrase> <phrase>business</phrase> practices including <phrase>Amazon</phrase>, <phrase>Google</phrase>, and <phrase>Starbucks</phrase> to name a few. This course provides an overview of the field of analytics so that you can make informed <phrase>business</phrase> decisions. It is <phrase>an introduction</phrase> to the theory of customer analytics, and is not intended to prepare learners to perform customer analytics.   Course <phrase>Learning Outcomes</phrase>:   After completing the course learners will be able to...  Describe the <phrase>major</phrase> methods of <phrase>customer data</phrase> collection used by companies and understand how this <phrase>data</phrase> can inform <phrase>business</phrase> decisions  Describe the main tools used to predict customer behavior and identify the appropriate uses for each tool   Communicate key ideas about customer analytics and how the field informs <phrase>business</phrase> decisions  Communicate the <phrase>history</phrase> of customer analytics and latest <phrase>best practices</phrase> at top firms
This course focuses on the concepts and tools behind reporting modern <phrase>data</phrase> analyses in a reproducible manner. Reproducible <phrase>research</phrase> is the idea that <phrase>data</phrase> analyses, and more generally, scientific claims, are published with their <phrase>data</phrase> and <phrase>software</phrase> code so that others may verify the findings and build upon them.  The need for <phrase>reproducibility</phrase> is increasing dramatically as <phrase>data</phrase> analyses become more complex, involving larger datasets and more sophisticated computations. <phrase>Reproducibility</phrase> allows for people to focus on the actual content of a <phrase>data</phrase> analysis, rather than on superficial details reported in a written <phrase>summary</phrase>. In addition, <phrase>reproducibility</phrase> makes an analysis more useful to others because the <phrase>data</phrase> and code that actually conducted the analysis are available. This course will focus on literate <phrase>statistical analysis</phrase> tools which allow one to publish <phrase>data</phrase> analyses in a <phrase>single</phrase> document that allows others to easily execute the same analysis to obtain the same <phrase>results</phrase>.
Want to understand your <phrase>data</phrase> network structure and how it changes under different conditions? Curious to know how to identify closely interacting clusters within a <phrase>graph</phrase>? Have you heard of the fast-growing <phrase>area</phrase> of <phrase>graph</phrase> analytics and want to learn more? This course gives you a broad overview of the field of <phrase>graph</phrase> analytics so you can learn new ways to <phrase>model</phrase>, store, retrieve and analyze <phrase>graph</phrase>-<phrase>structured data</phrase>.  After completing this course, you will be able to <phrase>model</phrase> a problem into a <phrase>graph database</phrase> and perform analytical tasks over the <phrase>graph</phrase> in a scalable manner.  Better yet, you will be able to apply these techniques to understand the significance of your <phrase>data</phrase> sets for your own projects.
In this capstone, learners will apply their <phrase>deep learning</phrase> <phrase>knowledge</phrase> and <phrase>expertise</phrase> to a <phrase>real world</phrase> challenge.  They will use a <phrase>library</phrase> of their choice to develop and <phrase>test</phrase> a <phrase>deep learning</phrase> <phrase>model</phrase>. They will load and pre-process <phrase>data</phrase> for a real problem, build the <phrase>model</phrase> and validate it. Learners  will then present a project <phrase>report</phrase> to demonstrate the validity of their <phrase>model</phrase> and their proficiency in the field of <phrase>Deep Learning</phrase>. <phrase>Learning Outcomes</phrase>: •	determine what kind of <phrase>deep learning</phrase> <phrase>method</phrase> to use in which situation •	know how to build a <phrase>deep learning</phrase> <phrase>model</phrase> to solve a real problem  •	<phrase>master</phrase> the process of creating  a <phrase>deep learning</phrase> <phrase>pipeline</phrase>  •	apply <phrase>knowledge</phrase> of <phrase>deep learning</phrase> to improve models using real <phrase>data</phrase> •	demonstrate ability to present and communicate outcomes of <phrase>deep learning</phrase> projects
Dieser einwöchige <phrase>On-Demand</phrase>-Schnellkurs bietet Teilnehmern eine praktische Einführung in die Entwicklung und Erstellung von Modellen für maschinelles Lernen (<phrase>Machine Learning</phrase>, <phrase>ML</phrase>) mithilfe der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. <phrase>Mit</phrase> einem Mix aus Präsentationen, Demos und praxisorientierten Labs machen Sie sich <phrase>mit</phrase> <phrase>ML</phrase>- und TensorFlow-Konzepten vertraut und lernen, wie Sie <phrase>ML</phrase>-Modelle entwickeln, bewerten und zur Bereitstellung vorbereiten.  LERNZIELE  Dieser Kurs vermittelt den Teilnehmern die folgenden Kompetenzen:    ● Anwendungsfälle für maschinelles Lernen bestimmen    ● <phrase>ML</phrase>-Modelle <phrase>mit</phrase> TensorFlow erstellen    ● Skalierbare, bereitstellbare <phrase>ML</phrase>-Modelle <phrase>mit</phrase> <phrase>Cloud</phrase> <phrase>ML</phrase> erstellen    ● Bedeutung der Vorverarbeitung und Kombination von Funktionen erkennen    ● Erweiterte <phrase>ML</phrase>-Konzepte in Modelle integrieren    ● Trainierte <phrase>ML</phrase>-Modelle für die Bereitstellung vorbereiten   VORAUSSETZUNGEN  Für maximale Lernerfolge sollten die Teilnehmer Folgendes mitbringen:    ● Abgeschlossener Kurs "<phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals" <phrase>ODER</phrase> gleichwertige Kenntnisse    ● Grundkenntnisse in gängigen Abfragesprachen wie <phrase>SQL</phrase>    ● Erfahrung <phrase>mit</phrase> Datenmodellierung, Extraktion, Transformation und Ladeaktivitäten    ● Kenntnisse im Entwickeln von Anwendungen <phrase>mit</phrase> einer gängigen Programmiersprache wie <phrase>Python</phrase>    ● Vertrautheit <phrase>mit</phrase> maschinellem Lernen und/<phrase>oder</phrase> Statistik  Hinweise zum <phrase>Google</phrase>-Konto: • Wenn Sie sich für die kostenlose Testversion der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform registrieren möchten, benötigen Sie ein <phrase>Google</phrase>-/<phrase>Gmail</phrase>-Konto und eine Kreditkarte <phrase>oder</phrase> ein Bankkonto. In <phrase>China</phrase> sind <phrase>Google</phrase>-Dienste derzeit nicht verfügbar. • Kunden der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>mit</phrase> Rechnungsadresse in der Europäischen <phrase>Union</phrase> (<phrase>EU</phrase>) <phrase>oder</phrase> Russland finden hilfreiche Hinweise zur Mehrwertsteuer unter: https://cloud.google.com/billing/docs/resources/vat-overview • Weitere FAQs zur kostenlosen Testversion der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform finden Sie unter: https://cloud.google.com/<phrase>free</phrase>-trial/.
How can we know if the differences in wages between <phrase>men and women</phrase> are caused by <phrase>discrimination</phrase> or differences in background characteristics? In this <phrase>PhD</phrase>-level course we look at causal effects as opposed to spurious relationships. We will discuss how they can be identified in the <phrase>social sciences</phrase> using <phrase>quantitative data</phrase>, and describe how this can help us understand social mechanisms.
低レベルの TensorFlow を導入し、分散型機械学習モデルを作成するために必要なコンセプトと <phrase>API</phrase> を開発します。TensorFlow モデルのトレーニングをスケールアウトし、<phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> を使った高性能な予測を提供する方法について説明します。  コースの目的: TensorFlow で機械学習モデルを作成する TensorFlow ライブラリを使用して数値の問題を解決する TensorFlow コードによくある問題のトラブルシューティングとデバッグを行う tf.estimator を使用して <phrase>ML</phrase> モデルを作成、トレーニング、評価する <phrase>Cloud</phrase> <phrase>ML</phrase> <phrase>Engine</phrase> を使用して <phrase>ML</phrase> モデルの大規模なトレーニング、デプロイ、本稼働を行う
R <phrase>Programming</phrase> Capstone
Choosing an appropriate <phrase>study design</phrase> is a critical decision that can largely determine whether your study will successfully answer your <phrase>research</phrase> question. A quick look at the contents page of a biomedical <phrase>journal</phrase> or even at the <phrase>health</phrase> <phrase>news</phrase> section of a <phrase>news</phrase> <phrase>website</phrase> is enough to tell you that there are many different ways to conduct <phrase>epidemiological</phrase> <phrase>research</phrase>.  In this course, you will learn about the main <phrase>epidemiological</phrase> study designs, including <phrase>cross-sectional</phrase> and <phrase>ecological</phrase> studies, <phrase>case</phrase>-control and cohort studies, as well as the more complex nested <phrase>case</phrase>-control and <phrase>case</phrase>-cohort designs. The final module is dedicated to randomised controlled trials, which is often considered the optimal <phrase>study design</phrase>, especially in <phrase>clinical research</phrase>. You will also develop the skills to identify strengths and limitations of the various study designs. By the end of this course, you will be able to choose the most suitable <phrase>study design</phrase> considering the <phrase>research</phrase> question, the available time, and resources.
In this project-based course, you will <phrase>design</phrase> and execute a complete <phrase>GIS</phrase>-based analysis – from identifying a concept, question or issue you wish to develop, all the way to final <phrase>data</phrase> <phrase>products</phrase> and maps that you can add to your portfolio. Your completed project will demonstrate your mastery of the content in the <phrase>GIS</phrase> Specialization and is broken up into four phases:  <phrase>Milestone</phrase> 1: Project Proposal - Conceptualize and <phrase>design</phrase> your project in the abstract, and write a <phrase>short</phrase> proposal that includes the project description, expected <phrase>data</phrase> needs, <phrase>timeline</phrase>, and how you expect to complete it.  <phrase>Milestone</phrase> 2: <phrase>Workflow</phrase> <phrase>Design</phrase> - Develop the analysis <phrase>workflow</phrase> for your project, which will typically involve creating at least one core <phrase>algorithm</phrase> for processing your <phrase>data</phrase>. The <phrase>model</phrase> need not be complex or complicated, but it should allow you to analyze <phrase>spatial data</phrase> for a new output or to create a new analytical map of some type.  <phrase>Milestone</phrase> 3: <phrase>Data</phrase> Analysis – Obtain and preprocess <phrase>data</phrase>, <phrase>run</phrase> it through your models or other workflows in <phrase>order</phrase> to get your rough <phrase>data</phrase> <phrase>products</phrase>, and begin creating your final map <phrase>products</phrase> and/or analysis.  <phrase>Milestone</phrase> 4: Web and Print Map Creation – Complete your project by submitting usable and attractive maps and your <phrase>data</phrase> and <phrase>algorithm</phrase> for <phrase>peer review</phrase> and <phrase>feedback</phrase>.
Финальный проект даст вам возможность применить полученные в рамках специализации знания к задаче из реального мира. Под руководством успешных специалистов в науке о данных вы сможете поработать над актуальным проектом в одной из областей: электронная коммерция, социальные медиа, информационный поиск, бизнес-аналитика и др.  В отличие от задач, основанных на модельных данных, работа над проектом из реальной жизни даст вам возможность самостоятельно пройти все этапы анализа данных — от подготовки данных до построения финальной модели и оценки её качества. В результате в вашем арсенале появится проект, который вы сможете использовать на практике и самостоятельно развивать в дальнейшем.  Наличие такого проекта станет вашим конкурентным преимуществом, ведь вы всегда сможете продемонстрировать успешный проект потенциальному работодателю.  Задания и видео курса разработаны на <phrase>Python</phrase> 2.
Необходимость описать закономерности изменения количественной переменной в нескольких группах возникает часто, например, если перед вами данные эксперимента. Дисперсионный анализ - это один из методов, который позволяет эффективно решать такие задачи, особенно, если таких групп больше чем две (однофакторный дисперсионный анализ), или группировка задается несколькими факторами (многофакторный дисперсионный анализ). В этом курсе вы узнаете, что в рамках парадигмы линейных моделей вполне можно работать не только с непрерывными, но и с дискретными предикторами - а иногда это даже приходится делать одновременно (как в ковариационном анализе и др.). Мы разберем несколько вариантов представления дискретных предикторов в линейных моделях и последствия разных способов кодирования для интерпретации модели. Вы научитесь подбирать линейные модели со взаимодействием факторов, чтобы описывать ситуации, когда характер действия фактора меняется в разных группах. В этом курсе для анализа и визуализации линейных моделей мы будем использовать язык статистического программирования R. Его богатейшие возможности позволят вам разобраться в тонкостях внутреннего устройства линейных моделей с дискретными и непрерывными предикторами.  Этот курс ориентирован на людей, которые хотят научиться описывать закономерности поведения количественных величин в зависимости от дискретных факторов. Курс рассчитан на тех, кто освоил базовые приемы регрессионного анализа с использованием языка R и  создание простейших .<phrase>html</phrase> документов при помощи rmarkdown и knitr.
This join course created by SPSU and ETU  includes 5 modules dedicated to different stages of the <phrase>system development</phrase>. Its modules represent several widely separated fields of <phrase>biomedical engineering</phrase>. We interconnect them by applying the <phrase>knowledge</phrase> from them all to a common <phrase>task</phrase> – the development of a <phrase>prototype</phrase> of an <phrase>mHealth</phrase> <phrase>ECG</phrase> system with built-in <phrase>data</phrase>-driven <phrase>signal processing</phrase> and analysis. Working on this <phrase>task</phrase> throughout the course, you will acquire a <phrase>knowledge</phrase> on how these branches of <phrase>science</phrase>, including <phrase>electronics</phrase>, <phrase>mathematics</phrase>, <phrase>data science</phrase> and <phrase>programming</phrase> are applied together in a real project. Pieces of <phrase>hardware</phrase> and <phrase>software</phrase>, as well as the <phrase>data</phrase> sets that we utilize in this course are the same components that we use in our work developing <phrase>prototypes</phrase> of devices and <phrase>algorithms</phrase> for our tasks in <phrase>science</phrase> and <phrase>engineering</phrase>. The course is a joint work of <phrase>Saint Petersburg State University</phrase> and <phrase>Saint Petersburg</phrase> Electrotechnical <phrase>University</phrase> ETU ("LETI"). Note that the goal of the course is not to provide you with fundamental <phrase>knowledge</phrase> on any of the topics highlighted in the modules, but to give you some useful skills on implementing them in practical tasks.
This course will teach you how to perform <phrase>data analysis</phrase> using MongoDB's powerful <phrase>Aggregation</phrase> Framework.  You'll begin this course by building a foundation of essential <phrase>aggregation</phrase> <phrase>knowledge</phrase>. By understanding these features of the <phrase>Aggregation</phrase> Framework you will learn how to ask complex questions of your <phrase>data</phrase>. This will lay the groundwork for the remainder of the course where you'll dive deep and learn about schema <phrase>design</phrase>, <phrase>relational</phrase> <phrase>data</phrase> migrations, and <phrase>machine learning</phrase> with  <phrase>MongoDB</phrase>.  By the end of this course you'll understand how to best use <phrase>MongoDB</phrase> and its <phrase>Aggregation</phrase> Framework in your own <phrase>data science</phrase> <phrase>workflow</phrase>.
Este curso terminal del programa de especialidad “Análisis de datos para <phrase>la</phrase> toma <phrase>decisiones</phrase>” <phrase>te</phrase> presenta un caso con datos <phrase>e</phrase> información <phrase>de la</phrase> vida real, <phrase>en</phrase> <phrase>el</phrase> que podrás aplicar <phrase>e</phrase> implementar los conceptos y herramientas que has adquirido <phrase>en</phrase> las últimas semanas. Dicho caso <phrase>ha</phrase> sido desarrollado por <phrase>el</phrase> grupo de instructores, así como un grupo de expertos de <phrase>IBM</phrase> asociados al <phrase>software</phrase> computacional ‘Watson Analytics'.   Encontrarás no solo datos <phrase>e</phrase> información asociados a este reto, <phrase>sino</phrase> también videos, lecturas <phrase>e</phrase> información clave que enriquecerán tus conocimientos y habilidades <phrase>en</phrase> esta área de conocimiento.   ¿Estás listo para poner <phrase>en</phrase> marcha todo lo aprendido hasta <phrase>el</phrase> día de hoy?   ¡Adelante!
Dieser einwöchige <phrase>On-Demand</phrase>-Schnellkurs baut auf dem Kurs "<phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals" auf. <phrase>Mit</phrase> einem Mix aus Lernvideos, Demos und praxisorientierten Labs lernen Sie, wie Sie <phrase>mit</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub und <phrase>Dataflow</phrase> Streamingdaten-Pipelines erstellen und die gewonnenen Erkenntnisse für Entscheidungen in Echtzeit nutzen können. Sie lernen <phrase>auch</phrase>, Dashboards zu erstellen, die gezielt für verschiedene Stakeholder-Gruppen Informationen abbilden.  Voraussetzungen: • Abgeschlossener Kurs "<phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals" (<phrase>oder</phrase> gleichwertige Kenntnisse) • Kenntnisse in <phrase>Java</phrase>  Lernziele: • Anwendungsmöglichkeiten für Echtzeit-Streaminganalysen kennenlernen • Den asynchronen <phrase>Messaging</phrase>-Dienst <phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub zur Verwaltung von Datenereignissen verwenden • <phrase>Streaming</phrase>-Pipelines schreiben und Transformationen ausführen, <phrase>falls</phrase> nötig • Die <phrase>Streaming</phrase>-<phrase>Pipeline</phrase> von beiden Seiten kennenlernen: Produktion und Nutzung • <phrase>Dataflow</phrase>, BigQuery und <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub für Echtzeit-<phrase>Streaming</phrase> und -Analysen einsetzen
Are you ready to close the loop on your <phrase>Big Data</phrase> skills? Do you want to apply all your <phrase>knowledge</phrase> you got from the previous courses in practice? Finally, in the Capstone project, you will integrate all the <phrase>knowledge</phrase> acquired earlier to build a <phrase>real application</phrase> leveraging the power of <phrase>Big Data</phrase>.  You will be given a <phrase>task</phrase> to combine <phrase>data</phrase> from different sources of different types (static distributed dataset, <phrase>streaming</phrase> <phrase>data</phrase>, <phrase>SQL</phrase> or <phrase>NoSQL</phrase> storage). Combined, this <phrase>data</phrase> will be used to build a predictive <phrase>model</phrase> for a <phrase>financial market</phrase> (as an example). First, you <phrase>design</phrase> a system from scratch and share it with your <phrase>peers</phrase> to get valuable <phrase>feedback</phrase>. Second, you can make it <phrase>public</phrase>, so get ready to receive the <phrase>feedback</phrase> from your service users. <phrase>Real-world</phrase> experience without any 3G-glasses or mock interviews.
This course will provide you a foundational understanding of <phrase>machine learning</phrase> models (<phrase>logistic regression</phrase>, multilayer perceptrons, <phrase>convolutional neural networks</phrase>, <phrase>natural language processing</phrase>, etc.) as well as demonstrate how these models can solve <phrase>complex problems</phrase> in a <phrase>variety</phrase> of industries, from <phrase>medical</phrase> diagnostics to <phrase>image recognition</phrase> to text prediction. In addition, we have designed practice exercises that will give you hands-on experience implementing these <phrase>data science</phrase> models on <phrase>data</phrase> sets. These practice exercises will teach you how to implement <phrase>machine learning</phrase> <phrase>algorithms</phrase> with TensorFlow, <phrase>open source</phrase> <phrase>libraries</phrase> used by leading tech companies in the <phrase>machine learning</phrase> field (e.g., <phrase>Google</phrase>, <phrase>NVIDIA</phrase>, CocaCola, <phrase>eBay</phrase>, <phrase>Snapchat</phrase>, Uber and many more).
In this course you will be introduced to the <phrase>basic</phrase> ideas behind the <phrase>qualitative research</phrase> in <phrase>social science</phrase>. You will learn about <phrase>data</phrase> collection, description, analysis and interpretation in <phrase>qualitative research</phrase>. <phrase>Qualitative research</phrase> often involves an <phrase>iterative process</phrase>. We will focus on the ingredients required for this process: <phrase>data</phrase> collection and analysis. You won't learn how to use qualitative methods by just watching video's, so we put much stress on collecting <phrase>data</phrase> through observation and interviewing and on analysing and interpreting the <phrase>collected data</phrase> in other assignments. Obviously, the most important concepts in <phrase>qualitative research</phrase> will be discussed, just as we will discuss quality criteria, good practices, <phrase>ethics</phrase>, writing some methods of analysis, and <phrase>mixing</phrase> methods. We hope to take away some <phrase>prejudice</phrase>, and enthuse many students for <phrase>qualitative research</phrase>.
What is <phrase>machine learning</phrase>, and what kinds of problems can it solve? <phrase>Google</phrase> thinks about <phrase>machine learning</phrase> slightly differently -- of being about <phrase>logic</phrase>, rather than just <phrase>data</phrase>. We <phrase>talk</phrase> about why such a framing is useful for <phrase>data</phrase> scientists when thinking about building a <phrase>pipeline</phrase> of <phrase>machine learning</phrase> models.   Then, we discuss the five phases of converting a candidate <phrase>use case</phrase> to be driven by <phrase>machine learning</phrase>, and consider why it is important the phases not be skipped. We end with a recognition of the biases that <phrase>machine learning</phrase> can amplify and how to recognize this.  >>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
Understanding <phrase>statistics</phrase> is essential to understand <phrase>research</phrase> in the social and <phrase>behavioral sciences</phrase>. In this course you will learn the basics of <phrase>statistics</phrase>; not just how to calculate them, but also how to evaluate them. This course will also prepare you for the next course in the specialization - the course Inferential <phrase>Statistics</phrase>.   In the first part of the course we will discuss methods of <phrase>descriptive statistics</phrase>. You will learn what cases and variables are and how you can compute measures of <phrase>central tendency</phrase> (mean, median and mode) and dispersion (<phrase>standard deviation</phrase> and <phrase>variance</phrase>). Next, we discuss how to assess relationships between variables, and we <phrase>introduce</phrase> the concepts correlation and <phrase>regression</phrase>.   The second part of the course is concerned with the basics of <phrase>probability</phrase>: calculating <phrase>probabilities</phrase>, <phrase>probability distributions</phrase> and <phrase>sampling</phrase> <phrase>distributions</phrase>. You need to know about these things in <phrase>order</phrase> to understand how inferential <phrase>statistics</phrase> work.   The third part of the course consists of <phrase>an introduction</phrase> to methods of inferential <phrase>statistics</phrase> - methods that help us decide whether the patterns we see in our <phrase>data</phrase> are strong enough to draw conclusions about the underlying <phrase>population</phrase> we are interested in. We will discuss <phrase>confidence intervals</phrase> and significance <phrase>tests</phrase>.  You will not only learn about all these statistical concepts, you will also be trained to calculate and generate these <phrase>statistics</phrase> yourself using <phrase>freely available</phrase> statistical <phrase>software</phrase>.
This course (The <phrase>English</phrase> copy of "用<phrase>Python</phrase>玩转数据" <https://www.coursera.org/learn/hipython/home/welcome>)  is mainly for non-<phrase>computer</phrase> <phrase>majors</phrase>. It starts with the <phrase>basic</phrase> <phrase>syntax</phrase> of <phrase>Python</phrase>, to how to acquire <phrase>data</phrase> in <phrase>Python</phrase> locally and from network, to how to present <phrase>data</phrase>, then to how to conduct <phrase>basic</phrase> and advanced statistic analysis and visualization of <phrase>data</phrase>, and finally to how to <phrase>design</phrase> a simple <phrase>GUI</phrase> to present and process <phrase>data</phrase>, advancing level by level.   This course, as a whole, based on <phrase>Finance</phrase> <phrase>data</phrase> and through the establishment of popular cases one after another, enables learners to more vividly feel the simplicity, elegance, and robustness of <phrase>Python</phrase>. Also, it discusses the fast, convenient and efficient <phrase>data</phrase> processing capacity of <phrase>Python</phrase> in <phrase>humanities</phrase> and <phrase>social sciences</phrase> fields like <phrase>literature</phrase>, <phrase>sociology</phrase> and <phrase>journalism</phrase> and <phrase>science</phrase> and <phrase>engineering</phrase> fields like <phrase>mathematics</phrase> and <phrase>biology</phrase>, in addition to <phrase>business</phrase> fields. Similarly, it may also be flexibly applied into other fields.  The course has been updated. Updates in the new version are :   1) the whole course has moved from <phrase>Python</phrase> 2.x to <phrase>Python</phrase> 3.x  2) Added manual webpage fetching and <phrase>parsing</phrase>. <phrase>Web API</phrase> is also added.  3) Improve the content <phrase>order</phrase> and enrich details of some content especially for some practice projects.  Note: videos are in <phrase>Chinese</phrase> (Simplified) with <phrase>English</phrase> subtitles. All other materials are in <phrase>English</phrase>.
Welcome to <phrase>Linear Regression</phrase> in R for <phrase>Public Health</phrase>!  <phrase>Public Health</phrase> has been defined as “the <phrase>art</phrase> and <phrase>science</phrase> of preventing <phrase>disease</phrase>, prolonging <phrase>life</phrase> and promoting <phrase>health</phrase> through the organized efforts of <phrase>society</phrase>”. Knowing what causes <phrase>disease</phrase> and what makes it worse are clearly vital parts of this. This requires the development of <phrase>statistical models</phrase> that describe how patient and <phrase>environmental factors</phrase> affect our chances of getting ill. This course will show you how to create such models from scratch, beginning with introducing you to the concept of correlation and  <phrase>linear regression</phrase> before walking you through importing and examining your <phrase>data</phrase>, and then showing you how to fit models. Using the example of <phrase>respiratory disease</phrase>, these models will describe how patient and other factors affect outcomes such as <phrase>lung</phrase> <phrase>function</phrase>.   <phrase>Linear regression</phrase> is one of a <phrase>family</phrase> of <phrase>regression</phrase> models, and the other courses in this series will <phrase>cover</phrase> two further members. <phrase>Regression</phrase> models have many things in common with each other, though the <phrase>mathematical</phrase> details differ.  This course will show you how to prepare the <phrase>data</phrase>, assess how well the <phrase>model</phrase> fits the <phrase>data</phrase>, and <phrase>test</phrase> its underlying assumptions – vital tasks with any type of <phrase>regression</phrase>.  You will use the <phrase>free</phrase> and versatile <phrase>software</phrase> package R, used by statisticians and <phrase>data</phrase> scientists in <phrase>academia</phrase>, governments and <phrase>industry</phrase> worldwide.
Welcome to <phrase>Survival Analysis</phrase> in R for <phrase>Public Health</phrase>!  The three earlier courses in this series <phrase>covered</phrase> statistical thinking, correlation, <phrase>linear regression</phrase> and <phrase>logistic regression</phrase>. This one will show you how to <phrase>run</phrase> survival – or “time to event” – analysis, explaining what’s meant by familiar-sounding but deceptive terms like hazard and censoring, which have specific meanings in this <phrase>context</phrase>. Using the popular and completely <phrase>free software</phrase> R, you’ll learn how to take a <phrase>data</phrase> set from scratch, <phrase>import</phrase> it into R, <phrase>run</phrase> essential descriptive analyses to get to know the data’s features and quirks, and progress from Kaplan-Meier plots through to multiple Cox <phrase>regression</phrase>. You’ll use <phrase>data</phrase> simulated from real, messy patient-level <phrase>data</phrase> for patients admitted to <phrase>hospital</phrase> with <phrase>heart failure</phrase> and learn how to explore which factors predict their subsequent mortality. You’ll learn how to <phrase>test</phrase> <phrase>model</phrase> assumptions and fit to the <phrase>data</phrase> and some simple tricks to get round <phrase>common problems</phrase> that real <phrase>public health</phrase> <phrase>data</phrase> have. There will be <phrase>mini</phrase>-quizzes on the videos and the R exercises with <phrase>feedback</phrase> along the way to check your understanding.  Prerequisites  Some formulae are given to aid understanding, but this is not one of those courses where you need a <phrase>mathematics</phrase> <phrase>degree</phrase> to follow it. You will need <phrase>basic</phrase> <phrase>numeracy</phrase> (for example, we will not use <phrase>calculus</phrase>) and familiarity with <phrase>graphical</phrase> and tabular ways of presenting <phrase>results</phrase>. The three previous courses in the series explained concepts such as <phrase>hypothesis testing</phrase>, p values, <phrase>confidence intervals</phrase>, correlation and <phrase>regression</phrase> and showed how to <phrase>install</phrase> R and <phrase>run</phrase> <phrase>basic</phrase> commands. In this course, we will recap all these core ideas in brief, but if you are unfamiliar with them, then you may prefer to take the first course in particular, Statistical Thinking in <phrase>Public Health</phrase>, and perhaps also the second, on <phrase>linear regression</phrase>, before embarking on this one.
Businesses <phrase>run</phrase> on <phrase>data</phrase>, and <phrase>data</phrase> offers little value without analytics. The ability to process <phrase>data</phrase> to make predictions about the behavior of individuals or markets, to diagnose systems or situations, or to prescribe actions for people or processes drives <phrase>business</phrase> today. Increasingly many businesses are striving to become “<phrase>data</phrase>-driven”, proactively relying more on cold hard <phrase>information</phrase> and sophisticated <phrase>algorithms</phrase> than upon the gut instinct or slow reactions of humans.   This course will focus on understanding key analytics concepts and the breadth of analytic possibilities. Together, the class will explore dozens of <phrase>real-world</phrase> analytics problems and solutions across most <phrase>major</phrase> industries and <phrase>business</phrase> functions. The course will also touch on analytic technologies, architectures, and roles from <phrase>business intelligence</phrase> to <phrase>data science</phrase>, and from <phrase>data</phrase> warehouses to <phrase>data</phrase> <phrase>lakes</phrase>. And the course will wrap up with a discussion of analytics trends and futures.
This course enables learners to develop 3D vision applications using a <phrase>stereo</phrase> imaging system. They are introduced to <phrase>stereo</phrase> vision theory, dense motion and visual <phrase>tracking</phrase>. They are able to discuss techniques used to obtain the 3D structure of objects. Topics include epipolar <phrase>geometry</phrase>, <phrase>optical flow</phrase>, structure from motion, multi-object <phrase>tracking</phrase>, 3D vision and visual odometry.      This course is ideal for anyone curious about or interested in exploring the concepts of <phrase>computer vision</phrase>. It is also useful for those who desire a refresher course in <phrase>mathematical</phrase> concepts of <phrase>computer vision</phrase>. Learners should have <phrase>basic</phrase> <phrase>programming</phrase> skills and experience (understanding of for loops, if/else statements), specifically in <phrase>MATLAB</phrase> (<phrase>Mathworks</phrase> provides the basics here: https://www.mathworks.com/learn/tutorials/<phrase>matlab</phrase>-onramp.html).  Learners should also be familiar with the following: <phrase>basic</phrase> <phrase>linear algebra</phrase> (matrix <phrase>vector</phrase> operations and <phrase>notation</phrase>), 3D co-ordinate systems and transformations, <phrase>basic</phrase> <phrase>calculus</phrase> (derivatives and <phrase>integration</phrase>) and <phrase>basic</phrase> <phrase>probability</phrase> (<phrase>random variables</phrase>).       Material includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing <phrase>computer vision</phrase> programs through online labs using <phrase>MATLAB</phrase>* and supporting toolboxes.  This is the third course in the <phrase>Computer</phrase> Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a <phrase>video</phrase> overview at https://youtu.be/OfxVUSCPXd0.    * A <phrase>free</phrase> license to <phrase>install</phrase> <phrase>MATLAB</phrase> for the duration of the course is available from <phrase>MathWorks</phrase>.
In this project-based course, you will follow your own interests to create a portfolio worthy <phrase>single</phrase>-<phrase>frame</phrase> viz or multi-<phrase>frame</phrase> <phrase>data</phrase> story that will be shared on Tableau <phrase>Public</phrase>. You will use all the skills taught in this Specialization to complete this project <phrase>step</phrase>-by-<phrase>step</phrase>, with guidance from your instructors along the way. You will first create a project proposal to identify your goals for the project, including the question you wish to answer or explore with <phrase>data</phrase>. You will then find <phrase>data</phrase> that will provide the <phrase>information</phrase> you are seeking. You will then <phrase>import</phrase> that <phrase>data</phrase> into Tableau and  prepare it for analysis. Next you will create a <phrase>dashboard</phrase> that will allow you to explore the <phrase>data</phrase> in depth and identify meaningful insights. You will then give structure to your <phrase>data</phrase> story by writing the <phrase>story arc</phrase> in <phrase>narrative</phrase> form. Finally, you will consult your <phrase>design</phrase> checklist to craft the final viz or <phrase>data</phrase> story in Tableau. This is your opportunity to show the world what you’re capable of - so think big, and have confidence in your skills!
Probabilistic <phrase>graphical</phrase> models (PGMs) are a rich framework for encoding <phrase>probability distributions</phrase> over complex domains: joint (multivariate) <phrase>distributions</phrase> over <phrase>large numbers</phrase> of <phrase>random variables</phrase> that interact with each other. These representations sit at the intersection of <phrase>statistics</phrase> and <phrase>computer science</phrase>, relying on concepts from <phrase>probability theory</phrase>, <phrase>graph</phrase> <phrase>algorithms</phrase>, <phrase>machine learning</phrase>, and more. They are the basis for the <phrase>state</phrase>-of-the-<phrase>art</phrase> methods in a wide <phrase>variety</phrase> of applications, such as <phrase>medical</phrase> diagnosis, <phrase>image understanding</phrase>, <phrase>speech recognition</phrase>, <phrase>natural language processing</phrase>, and many, many more. They are also a foundational tool in formulating many <phrase>machine learning</phrase> problems.   This course is the second in a <phrase>sequence</phrase> of three. Following the first course, which focused on representation, this course addresses the question of <phrase>probabilistic inference</phrase>: how a PGM can be used to <phrase>answer questions</phrase>. Even though a PGM generally describes a very <phrase>high</phrase> dimensional distribution, its structure is designed so as to allow questions to be answered efficiently. The course presents both exact and approximate <phrase>algorithms</phrase> for different types of inference tasks, and discusses where each could best be applied. The (highly recommended) honors <phrase>track</phrase> contains two hands-on <phrase>programming</phrase> assignments, in which key routines of the most commonly used exact and approximate <phrase>algorithms</phrase> are implemented and applied to a <phrase>real-world</phrase> problem.
Welcome to <phrase>Data</phrase>-driven <phrase>Decision Making</phrase>. In this course you'll get <phrase>an introduction</phrase> to <phrase>Data</phrase> Analytics and its role in <phrase>business</phrase> decisions. You'll learn why <phrase>data</phrase> is important and how it has evolved. You'll be introduced to “<phrase>Big Data</phrase>” and how it is used. You'll also be introduced to a framework for <phrase>conducting</phrase> <phrase>Data</phrase> Analysis and what tools and techniques are commonly used. Finally, you'll have a chance to put your <phrase>knowledge</phrase> to work in a simulated <phrase>business</phrase> setting.  This course was created by <phrase>PricewaterhouseCoopers</phrase> LLP with an <phrase>address</phrase> at 300 <phrase>Madison</phrase> Avenue, <phrase>New York</phrase>, <phrase>New York</phrase>, 10017.
Important: The focus of this course is on <phrase>math</phrase> - specifically, <phrase>data</phrase>-analysis concepts and methods - not on <phrase>Excel</phrase> for its own <phrase>sake</phrase>. We use <phrase>Excel</phrase> to do our calculations, and all <phrase>math</phrase> formulas are given as <phrase>Excel</phrase> <phrase>Spreadsheets</phrase>, but we do not attempt to <phrase>cover</phrase> <phrase>Excel</phrase> Macros, <phrase>Visual Basic</phrase>, Pivot Tables, or other intermediate-to-advanced <phrase>Excel</phrase> functionality.  This course will prepare you to <phrase>design</phrase> and implement realistic predictive models based on <phrase>data</phrase>. In the Final Project (module 6) you will assume the role of a <phrase>business</phrase> <phrase>data</phrase> analyst for a <phrase>bank</phrase>, and develop two different predictive models to determine which applicants for <phrase>credit cards</phrase> should be accepted and which rejected. Your first <phrase>model</phrase> will focus on minimizing default <phrase>risk</phrase>, and your second on maximizing <phrase>bank</phrase> profits. The two models should demonstrate to you in a practical, hands-on way the idea that your choice of <phrase>business</phrase> metric drives your choice of an optimal <phrase>model</phrase>.  The second big idea this course seeks to demonstrate is that your <phrase>data</phrase>-analysis <phrase>results</phrase> cannot and should not aim to eliminate all uncertainty. Your role as a <phrase>data</phrase>-analyst is to reduce uncertainty for <phrase>decision-makers</phrase> by a financially valuable increment, while quantifying how much uncertainty remains. You will learn to calculate and apply to <phrase>real-world</phrase> examples the most important uncertainty measures used in <phrase>business</phrase>, including classification <phrase>error rates</phrase>, <phrase>entropy</phrase> of <phrase>information</phrase>, and <phrase>confidence intervals</phrase> for <phrase>linear regression</phrase>.  All the <phrase>data</phrase> you need is provided within the course, all assignments are designed to be done in MS <phrase>Excel</phrase>, and you will learn enough <phrase>Excel</phrase> to complete all assignments. The course will give you enough practice with <phrase>Excel</phrase> to become fluent in its most commonly used <phrase>business</phrase> functions, and you’ll be ready to learn any other <phrase>Excel</phrase> functionality you might need in the future (module 1).  The course does not <phrase>cover</phrase> <phrase>Visual Basic</phrase> or Pivot Tables and you will not need them to complete the assignments. All <phrase>advanced concepts</phrase> are demonstrated in individual <phrase>Excel</phrase> <phrase>spreadsheet</phrase> templates that you can use to answer relevant questions. You will emerge with substantial <phrase>vocabulary</phrase> and practical <phrase>knowledge</phrase> of how to apply <phrase>business</phrase> <phrase>data</phrase> analysis methods based on <phrase>binary</phrase> classification (module 2), <phrase>information theory</phrase> and <phrase>entropy</phrase> measures (module 3), and <phrase>linear regression</phrase> (module 4 and 5), all using no <phrase>software</phrase> tools more complex than <phrase>Excel</phrase>.
<phrase>Statistical inference</phrase> is the process of <phrase>drawing</phrase> conclusions about populations or scientific truths from <phrase>data</phrase>. There are many modes of performing inference including <phrase>statistical modeling</phrase>, <phrase>data</phrase> oriented strategies and explicit use of designs and <phrase>randomization</phrase> in analyses. Furthermore, there are broad theories (frequentists, <phrase>Bayesian</phrase>, likelihood, <phrase>design</phrase> based, …) and numerous complexities (<phrase>missing data</phrase>, observed and unobserved <phrase>confounding</phrase>, biases) for performing inference. A practitioner can often be left in a debilitating <phrase>maze</phrase> of techniques, philosophies and nuance. This course presents the fundamentals of inference in a <phrase>practical approach</phrase> for getting things done. After taking this course, students will understand the broad directions of <phrase>statistical inference</phrase> and use this <phrase>information</phrase> for making informed choices in <phrase>analyzing data</phrase>.
This course is <phrase>an introduction</phrase> to how to use <phrase>relational databases</phrase> in <phrase>business analysis</phrase>.  You will learn how <phrase>relational databases</phrase> work, and how to use <phrase>entity-relationship</phrase> diagrams to display the structure of the <phrase>data</phrase> held within them.  This <phrase>knowledge</phrase> will help you understand how <phrase>data</phrase> needs to be collected in <phrase>business</phrase> contexts, and help you identify features you want to consider if you are involved in implementing new <phrase>data</phrase> collection efforts.  You will also learn how to execute the most useful query and table <phrase>aggregation</phrase> statements for <phrase>business</phrase> analysts, and practice using them with real <phrase>databases</phrase>. No more waiting 48 hours for someone else in the <phrase>company</phrase> to provide <phrase>data</phrase> to you – you will be able to get the <phrase>data</phrase> by yourself!  By the end of this course, you will have a clear understanding of how <phrase>relational databases</phrase> work, and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of <phrase>information</phrase> with the hope that <phrase>data</phrase> will yield novel insights into how to improve businesses. Analysts that understand how to access this <phrase>data</phrase> – this means you! – will have a strong <phrase>competitive advantage</phrase> in this <phrase>data</phrase>-smitten <phrase>business</phrase> world.
Welcome to the <phrase>Coursera</phrase> specialization, From <phrase>Data</phrase> to Insights with <phrase>Google</phrase> <phrase>Cloud</phrase> Platform brought to you by the <phrase>Google</phrase> <phrase>Cloud</phrase> team. I’m Evan Jones (a <phrase>data</phrase> enthusiast) and I’m going to be your guide.  This first course in this specialization is Exploring and Preparing your <phrase>Data</phrase> with BigQuery. Here we will see what the common challenges faced by <phrase>data</phrase> analysts are and how to solve them with the <phrase>big data</phrase> tools on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. You’ll pick up some <phrase>SQL</phrase> along the way and become very familiar with using BigQuery and <phrase>Cloud</phrase> Dataprep to analyze and transform your datasets.  This course should take about one week to complete, 5-7 total hours of work.  By the end of this course, you’ll be able to query and draw insight from millions of records in our BigQuery <phrase>public</phrase> datasets. You’ll learn how to assess the quality of your datasets and develop an automated <phrase>data</phrase> cleansing <phrase>pipeline</phrase> that will output to BigQuery. Lastly, you’ll get to practice writing and <phrase>troubleshooting</phrase> <phrase>SQL</phrase> on a real <phrase>Google</phrase> Analytics <phrase>e-commerce</phrase> dataset to drive <phrase>marketing</phrase> insights.  >>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
This course is designed to show you how use quantitative models to transform <phrase>data</phrase> into better <phrase>business</phrase> decisions. You’ll learn both how to use models to facilitate <phrase>decision-making</phrase> and also how to structure <phrase>decision-making</phrase> for optimum <phrase>results</phrase>. Two of Wharton’s most acclaimed professors will show you the <phrase>step</phrase>-by-<phrase>step</phrase> processes of modeling common <phrase>business</phrase> and financial scenarios, so you can significantly improve your ability to structure <phrase>complex problems</phrase> and derive useful insights about alternatives. Once you’ve created models of existing realities, possible risks, and <phrase>alternative</phrase> scenarios, you can determine the best <phrase>solution</phrase> for your <phrase>business</phrase> or enterprise, using the <phrase>decision-making</phrase> tools and techniques you’ve learned in this course.
Курс предназначен для пользователей, которым необходимо проводить анализ экономических данных. Данный курс является первым в серии курсов "Практики анализа экономических данных. От простого к сложному". Первый курс посвящен правилам правильной организации данных, профессиональным приемам организации расчетов и визуализации данных. Слушатель сможет применить знания базовых инструментов MS <phrase>Excel</phrase> для решения бизнес-кейса. Все инструменты показываются через призму типовых примеров, в которых каждый слушатель узнает свои профессиональные проблемы. Используется подход к обучению анализу данных в среде <phrase>Microsoft Excel</phrase> через решение типовых задач, проецируемых на любую предметную область. Данный подход апробирован авторами на большом количестве групп повышения квалификации экономистов и менеджеров. В конце курса слушателям предлагается выполнить большой практический проект. По окончании курса Вы будете  Знать: - категории задач, решаемые в среде электронных таблиц; - базовые правила организации расчетов при решении экономических задач; - правила агрегирования данных; - методы выборки данных в соответствии с потребностями аналитика; - базовые концепции и инструменты визуализации данных.  Уметь: - выбирать адекватные инструменты для решения задач; - рассчитывать операционные, агрегированные показатели деятельности компании; - представлять диаграммы, адекватно отражающие и интерпретирующие данные таблиц; - применять инструменты фильтрации в соответствии с поставленной задачей.  Владеть: - навыками решения аналитических задач в среде MS <phrase>Excel</phrase>; - навыками выбора адекватных инструментов графического анализа данных; - типовыми инструментами фильтрации данных; - технологиями формирования агрегированных показателей.
Want to know how to query and process petabytes of <phrase>data</phrase> in seconds? Curious about <phrase>data analysis</phrase> that scales automatically as your <phrase>data</phrase> grows? Welcome to the <phrase>Data</phrase> Insights course!  This 1-week, accelerated <phrase>online course</phrase> teaches participants how to derive insights through <phrase>data analysis</phrase> and visualization using the <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. The course features interactive scenarios and hands-on labs where participants explore, mine, load, visualize, and extract insights from diverse <phrase>Google</phrase> BigQuery datasets. The course covers <phrase>data</phrase> loading, querying, schema modeling, optimizing performance, query pricing,  and <phrase>data</phrase> visualization.  PREREQUISITES To get the most out of this course, participants must complete the prior courses in this specialization: • Exploring and Preparing your <phrase>Data</phrase> • Storing and Visualizing your <phrase>Data</phrase> • <phrase>Architecture</phrase> and Performance  >>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
In this course, we will explore <phrase>basic</phrase> principles behind using <phrase>data</phrase> for estimation and for assessing theories. We will analyze both categorical <phrase>data</phrase> and <phrase>quantitative data</phrase>, starting with one <phrase>population</phrase> techniques and expanding to <phrase>handle</phrase> comparisons of two populations. We will learn how to construct <phrase>confidence intervals</phrase>. We will also use sample <phrase>data</phrase> to assess whether or not a theory about the value of a parameter is consistent with the <phrase>data</phrase>. A <phrase>major</phrase> focus will be on interpreting inferential <phrase>results</phrase> appropriately.    At the end of each week, learners will apply what they’ve learned using <phrase>Python</phrase> within the course environment. During these lab-based sessions, learners will work through tutorials focusing on specific <phrase>case</phrase> studies to help solidify the week’s statistical concepts, which will include further deep dives into <phrase>Python</phrase> <phrase>libraries</phrase> including Statsmodels, <phrase>Pandas</phrase>, and Seaborn. This course utilizes the Jupyter Notebook environment within <phrase>Coursera</phrase>.
This four-module course introduces users to Julia as a first <phrase>language</phrase>.  Julia is a <phrase>high</phrase>-level, <phrase>high</phrase>-performance <phrase>dynamic programming language</phrase> developed specifically for <phrase>scientific computing</phrase>. This <phrase>language</phrase> will be particularly useful for applications in <phrase>physics</phrase>, <phrase>chemistry</phrase>, <phrase>astronomy</phrase>, <phrase>engineering</phrase>, <phrase>data science</phrase>, <phrase>bioinformatics</phrase> and many more. As <phrase>open source software</phrase>, you will always have it available throughout your working <phrase>life</phrase>. It can also be used from the <phrase>command line</phrase>, <phrase>program files</phrase> or a new type of interface known as a Jupyter notebook (which is <phrase>freely available</phrase> as a service from JuliaBox.com).  Julia is designed to <phrase>address</phrase> the requirements of <phrase>high</phrase>-performance numerical and <phrase>scientific computing</phrase> while also being effective for <phrase>general</phrase>-purpose <phrase>programming</phrase>. You will be able to access all the available processors and <phrase>memory</phrase>, scrape <phrase>data</phrase> from anywhere on the web, and have it always accessible through any device you care to use as <phrase>long</phrase> as it has a <phrase>browser</phrase>.  Join us to discover new <phrase>computing</phrase> possibilities. Let's get started on learning Julia.  By the end of the course you will be able to: - Programme using the Julia <phrase>language</phrase> by practising through assignments - Write your own simple Julia programs from scratch - Understand the advantages and capacities of Julia as a <phrase>computing</phrase> <phrase>language</phrase> - Work in Jupyter notebooks using the Julia <phrase>language</phrase> - Use various Julia packages such as  Plots, DataFrames and <phrase>Stats</phrase>  The course is delivered through <phrase>video</phrase> lectures, on-screen demonstrations, quizzes and practical <phrase>peer-reviewed</phrase> projects designed to give you an opportunity to work with the packages.
<phrase>Data</phrase> repositories in which cases are related to subcases are identified as hierarchical. This course covers the representation schemes of hierarchies and <phrase>algorithms</phrase> that enable analysis of <phrase>hierarchical data</phrase>, as well as provides opportunities to apply several methods of analysis.
Курс  рассматривает способы и инструменты исследования статистических взаимосвязей между признаками. Вы научитесь оценивать, связаны ли признаки,  а также делать обоснованные выводы о том, значима ли эта связь статистически. Связаны ли богатство и счастье, как связана потребительская активность людей с днем недели, способствует ли наличие аккаунта в социальных сетях популярности корпоративного сайта? На вопросы такого рода вы сможете ответить, пройдя этот курс.  В первом модуле курса мы поговорим о статистических гипотезах, о способах их проверки и об основных статистических критериях, которые для этого разработаны. После этого мы рассмотрим практические инструменты выявления статистических взаимосвязей признаков, измеренных разными типами шкал, а также способы оценки значимости этих связей. Мы поговорим об основных коэффициентах взаимосвязи признаков, о том, как правильно выбрать коэффициент для решения конкретной задачи и покажем, как рассчитывать коэффициенты связи в статистических пакетах.  В заключении мы подробно рассмотрим модель линейной регрессии, которая позволяет не только выявлять взаимосвязи между признаками, но и строить прогноз, и попрактикуемся в её построении.
This course is <phrase>an introduction</phrase> to 3D <phrase>scientific data</phrase> visualization, with an emphasis on <phrase>science communication</phrase> and cinematic <phrase>design</phrase> for appealing to broad audiences. You will develop visualization <phrase>literacy</phrase>, through being able to interpret/analyze (read) visualizations and create (write) your own visualizations.  By the end of this course, you will: -Develop visualization <phrase>literacy</phrase>. -Learn the practicality of working with <phrase>spatial data</phrase>. -Understand what makes a <phrase>scientific visualization</phrase> meaningful. -Learn how to create educational visualizations that maintain scientific accuracy. -Understand what makes a <phrase>scientific visualization</phrase> cinematic. -Learn how to create visualizations that <phrase>appeal</phrase> to broad audiences. -Learn how to work with image-making <phrase>software</phrase>. (for those completing the Honors <phrase>track</phrase>)
Interprofessional <phrase>Healthcare Informatics</phrase> is a graduate-level, hands-on interactive exploration of real informatics tools and techniques offered by the <phrase>University</phrase> of <phrase>Minnesota</phrase> and the <phrase>University</phrase> of Minnesota's National <phrase>Center</phrase> for Interprofessional Practice and <phrase>Education</phrase>. We will be incorporating <phrase>technology</phrase>-<phrase>enabled</phrase> educational innovations to bring the <phrase>subject matter</phrase> to <phrase>life</phrase>. Over the 10 modules, we will create a vital <phrase>online learning community</phrase> and a working <phrase>healthcare</phrase> informatics network.   We will explore perspectives of clinicians like dentists, <phrase>physical therapists</phrase>, nurses, and <phrase>physicians</phrase> in all sorts of practice settings worldwide. <phrase>Emerging technologies</phrase>, <phrase>telehealth</phrase>, gaming, simulations, and eScience are just some of the topics that we will consider.   Throughout the course, we’ll focus on <phrase>creativity</phrase>, controversy, and collaboration - as we collectively imagine and create the future within the rapidly evolving <phrase>healthcare</phrase> informatics milieu. All <phrase>healthcare</phrase> professionals and IT geeks are welcome!
This capstone project course for the <phrase>Recommender Systems</phrase> Specialization brings together everything you've learned about <phrase>recommender systems</phrase> <phrase>algorithms</phrase> and evaluation into a <phrase>comprehensive</phrase> recommender <phrase>analysis and design</phrase> project.  You will be given a <phrase>case</phrase> study to complete where you have to select and justify the <phrase>design</phrase> of a <phrase>recommender system</phrase> through analysis of recommender goals and <phrase>algorithm</phrase> performance.    Learners in the honors <phrase>track</phrase> will focus on <phrase>experimental</phrase> evaluation of the <phrase>algorithms</phrase> against <phrase>medium sized</phrase> datasets.  The standard <phrase>track</phrase> will include a mix of provided <phrase>results</phrase> and <phrase>spreadsheet</phrase> exploration.  Both groups will produce a capstone <phrase>report</phrase> documenting the analysis, the selected <phrase>solution</phrase>, and the justification for that <phrase>solution</phrase>.
In this capstone project we’ll combine  all of the skills from all four specialization courses to do something really fun: analyze <phrase>social networks</phrase>!    The opportunities for learning are practically endless in a <phrase>social network</phrase>.  Who are the “influential” members of the network?  What are the sub-communities in the network?   Who is connected to whom, and by how many links?   These are just some of the questions you can explore in this project.  We will provide you with a <phrase>real-world</phrase> <phrase>data</phrase> set and some <phrase>infrastructure</phrase> for <phrase>getting started</phrase>, as well as some warm up tasks and <phrase>basic</phrase> project requirements, but then it’ll be up to you where you want to take the project.  If you’re running <phrase>short</phrase> on ideas, we’ll have several suggested directions that can help get your <phrase>creativity</phrase> and imagination going.  Finally, to integrate the skills you acquired in course 4 (and to show off your project!) you will be asked to create a <phrase>video</phrase> showcase of your final product.
Apprendre à réaliser <phrase>des</phrase> cartes ? C’est <phrase>ce</phrase> que propose <phrase>ce</phrase> cours grâce à <phrase>des</phrase> vidéos, <phrase>des</phrase> exercices, un forum. A <phrase>la</phrase> <phrase>fin</phrase> <phrase>de la</phrase> formation, vous maîtriserez <phrase>les</phrase> <phrase>principes</phrase> <phrase>de la</phrase> sémiologie graphique, et vous saurez <phrase>les</phrase> mettre <phrase>en</phrase> œuvre pour construire <phrase>des</phrase> cartes thématiques simples et de qualité. Ces cartes personnelles donneront <phrase>de la</phrase> force à vos rapports, synthèses, articles, mémoires universitaires, sites web... <phrase>Les</phrase> participants seront guidés <phrase>pas</phrase> à <phrase>pas</phrase> pour dessiner un fond de carte, construire <phrase>des</phrase> cartes statistiques, avec <phrase>des</phrase> figurés proportionnels, <phrase>des</phrase> gammes de couleurs… De multiples exercices seront proposés : quizz, QCM, entraînements sur <phrase>les</phrase> logiciels. Deux logiciels libres seront utilisés : Inkscape, un logiciel de dessin vectoriel, et Philcarto, un logiciel de cartographie automatique.  <phrase>Ce</phrase> <phrase>MOOC</phrase> est aussi une invitation <phrase>au</phrase> voyage : de nombreux exemples à toutes <phrase>les</phrase> échelles et de toutes <phrase>les</phrase> régions du monde seront présentés.
Even decades into the <phrase>Information</phrase> Age, <phrase>accounting</phrase> practices yet fail to recognize the financial value of <phrase>information</phrase>. Moreover, traditional <phrase>asset management</phrase> practices fail to recognize <phrase>information</phrase> as an asset to be managed with earnest discipline. This has <phrase>led</phrase> to a <phrase>business</phrase> <phrase>culture</phrase> of complacence, and the inability for most organizations to fully leverage available <phrase>information</phrase> assets.   This second course in the two-part Infonomics series explores how and why to adapt well-honed <phrase>asset management</phrase> <phrase>principles and practices</phrase> to <phrase>information</phrase>, and how to apply accepted and new valuation models to gauge information’s potential and realized economic benefits. In addition, the course will enlighten students on the critical but <phrase>confounding</phrase> issues of <phrase>information</phrase> ownership, <phrase>property</phrase> rights, and <phrase>sovereignty</phrase>. The course will wrap up with an overview of emergent roles for the <phrase>information</phrase>-savvy <phrase>organization</phrase> of the <phrase>21st century</phrase>.
In the final capstone project you will apply the skills you learned by building a large <phrase>data</phrase>-intensive <phrase>application</phrase> using <phrase>real-world</phrase> <phrase>data</phrase>.  You will implement a complete <phrase>application</phrase> processing several gigabytes of <phrase>data</phrase>. This <phrase>application</phrase> will show interactive visualizations of the <phrase>evolution</phrase> of temperatures over time all over the world.  The development of such an <phrase>application</phrase> will involve:  — transforming <phrase>data</phrase> provided by <phrase>weather</phrase> <phrase>stations</phrase> into meaningful <phrase>information</phrase> like, for <phrase>instance</phrase>, the <phrase>average</phrase> <phrase>temperature</phrase> of each point of the globe over the last <phrase>ten years</phrase> ;  — then, making images from this <phrase>information</phrase> by using spatial and <phrase>linear interpolation</phrase> techniques ;  — finally, implementing how the <phrase>user interface</phrase> will react to users’ actions.
This course answers the questions, What is <phrase>data visualization</phrase> and What is the power of visualization? It also introduces <phrase>core concepts</phrase> such as dataset elements, <phrase>data</phrase> warehouses and exploratory querying, and combinations of visual variables for <phrase>graphic</phrase> usefulness, as well as the types of statistical <phrase>graphs</phrase>, tools that are essential to <phrase>exploratory data analysis</phrase>.
The analytics process is a collection of interrelated activities that <phrase>lead</phrase> to better decisions and to a higher <phrase>business</phrase> performance. The capstone of this specialization is designed with the goal of allowing you to experience this process. The capstone project will take you from <phrase>data</phrase> to analysis and models, and ultimately to presentation of insights.   In this capstone project, you will analyze the <phrase>data</phrase> on financial loans to help with the <phrase>investment</phrase> decisions of an <phrase>investment</phrase> <phrase>company</phrase>. You will go through all typical steps of a <phrase>data</phrase> analytics project, including <phrase>data</phrase> understanding and cleanup, <phrase>data</phrase> analysis, and presentation of analytical <phrase>results</phrase>.  For the first week, the goal is to understand the <phrase>data</phrase> and prepare the <phrase>data</phrase> for analysis. As we discussed  in this specialization, <phrase>data</phrase> preprocessing and cleanup is often the first <phrase>step</phrase> in <phrase>data</phrase> analytics projects. Needless to say, this <phrase>step</phrase> is crucial for the success of this project.    In the second week, you will perform some <phrase>predictive analytics</phrase> tasks, including classifying loans and predicting losses from defaulted loans. You will try a <phrase>variety</phrase> of tools and techniques  this week, as the predictive accuracy of different tools can vary quite a <phrase>bit</phrase>. It is rarely the <phrase>case</phrase> that the default <phrase>model</phrase> <phrase>produced</phrase> by <phrase>ASP</phrase> is the best <phrase>model</phrase> possible. Therefore, it is important for you to tune the different models in <phrase>order</phrase> to improve the performance.  Beginning in the third week, we turn our attention to prescriptive analytics, where you will provide some <phrase>concrete</phrase> suggestions on how to allocate <phrase>investment</phrase> funds using analytics tools, including clustering and <phrase>simulation</phrase> based optimization. You will see that allocating funds wisely is crucial for the financial return of the <phrase>investment</phrase> portfolio.  In the <phrase>last week</phrase>, you are expected to present your analytics <phrase>results</phrase> to your clients. Since you will obtain many <phrase>results</phrase> in your project, it is important for you to judiciously choose what to include in your presentation. You are also expected to follow the principles we <phrase>covered</phrase> in the courses in preparing your presentation.
The goal of this course is to understand the foundations of <phrase>Big Data</phrase> and the <phrase>data</phrase> that is being generated in the <phrase>health</phrase> <phrase>domain</phrase> and how the use of <phrase>technology</phrase> would help to integrate and exploit all those <phrase>data</phrase> to extract meaningful <phrase>information</phrase> that can be later used in different sectors of the <phrase>health</phrase> <phrase>domain</phrase> from <phrase>physicians</phrase> to <phrase>management</phrase>, from patients to care givers, etc. The course will offer to the <phrase>student</phrase> a <phrase>high</phrase>-level perspective of the importance of the <phrase>medical</phrase> <phrase>context</phrase> within the <phrase>European</phrase> <phrase>context</phrase>, the types of <phrase>data</phrase> that are managed in the <phrase>health</phrase> (clinical) <phrase>context</phrase>, the challenges to be addressed in the <phrase>mining</phrase> of unstructured <phrase>medical</phrase> <phrase>data</phrase> (text and image) as well as the opportunities from the analytical <phrase>point of view</phrase> with <phrase>an introduction</phrase> to the basics of <phrase>data</phrase> analytics field.
“Visualización de datos” <phrase>es</phrase> <phrase>el</phrase> cuarto curso <phrase>de la</phrase> especialización “Biga <phrase>Data</phrase>- Uso práctico de datos masivos. Organizado <phrase>en</phrase> cuatro semanas, tiene por objetivo motivar <phrase>e</phrase> introducir los conceptos clave <phrase>de la</phrase> visualización de datos así como mostrar ejemplos <phrase>en</phrase> diferentes contextos. Además, <phrase>se</phrase> proporcionan criterios para formular <phrase>el</phrase> problema y elegir las herramientas más adecuadas para obtener una correcta visualización. Este debe ser un curso introductorio, motivador <phrase>e</phrase> inspirador para <phrase>la</phrase> narración de historias a través <phrase>de la</phrase> visualización de sus datos.  Los cuatro módulos <phrase>en</phrase> los que <phrase>se</phrase> estructura <phrase>el</phrase> curso son los siguientes: MÓDULO 1: Contexto para <phrase>la</phrase> visualización de datos hoy MÓDULO 2: Herramientas de análisis y visualización de datos MÓDULO 3: <phrase>El</phrase> proceso de creación de una visualización de datos MÓDULO 4: Otros aspectos <phrase>de la</phrase> visualización de datos
In this final course you will complete a Capstone Project using <phrase>data</phrase> analysis to recommend a <phrase>method</phrase> for improving profits for your <phrase>company</phrase>, <phrase>Watershed</phrase> <phrase>Property Management</phrase>, Inc. <phrase>Watershed</phrase> is responsible for managing thousands of residential rental properties throughout the <phrase>United States</phrase>. Your job is to persuade Watershed’s <phrase>management</phrase> team to pursue a new strategy for managing its properties that will increase their profits. To do this, you will: (1) Elicit <phrase>information</phrase> about important variables relevant to your analysis; (2) Draw upon your new <phrase>MySQL</phrase> <phrase>database</phrase> skills to extract relevant <phrase>data</phrase> from a <phrase>real estate</phrase> <phrase>database</phrase>; (3) Implement <phrase>data</phrase> analysis in <phrase>Excel</phrase> to identify the best opportunities for <phrase>Watershed</phrase> to increase <phrase>revenue</phrase> and maximize profits, while managing any new risks; (4) Create a Tableau <phrase>dashboard</phrase> to show <phrase>Watershed</phrase> executives the <phrase>results</phrase> of a <phrase>sensitivity analysis</phrase>; and (5) Articulate a significant and innovative <phrase>business process</phrase> change for <phrase>Watershed</phrase> based on your <phrase>data</phrase> analysis, that you will recommend to <phrase>company</phrase> executives.   <phrase>Airbnb</phrase>, our Capstone’s official Sponsor, provided input on the project <phrase>design</phrase>. The top 10 Capstone completers each year will have the opportunity to present their work directly to senior <phrase>data</phrase> scientists at <phrase>Airbnb</phrase> <phrase>live</phrase> for <phrase>feedback</phrase> and discussion.  "Note: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone."
This is the second of a two-course <phrase>sequence</phrase> introducing the fundamentals of <phrase>Bayesian statistics</phrase>. It builds on the course <phrase>Bayesian Statistics</phrase>: From Concept to <phrase>Data</phrase> Analysis, which introduces <phrase>Bayesian</phrase> methods through use of simple conjugate models. <phrase>Real-world</phrase> <phrase>data</phrase> often require more sophisticated models to reach realistic conclusions. This course aims to expand our “<phrase>Bayesian</phrase> toolbox” with more <phrase>general</phrase> models, and <phrase>computational techniques</phrase> to fit them. In particular, we will <phrase>introduce</phrase> <phrase>Markov chain Monte Carlo</phrase> (MCMC) methods, which allow <phrase>sampling</phrase> from posterior <phrase>distributions</phrase> that have no analytical <phrase>solution</phrase>. We will use the <phrase>open-source</phrase>, <phrase>freely available</phrase> <phrase>software</phrase> R (some experience is assumed, e.g., completing the previous course in R) and JAGS (no experience required). We will learn how to construct, fit, assess, and compare <phrase>Bayesian</phrase> <phrase>statistical models</phrase> to answer scientific questions involving continuous, <phrase>binary</phrase>, and <phrase>count</phrase> <phrase>data</phrase>. This course combines lecture videos, <phrase>computer</phrase> demonstrations, readings, exercises, and discussion boards to create an <phrase>active learning</phrase> experience. The lectures provide some of the <phrase>basic</phrase> <phrase>mathematical</phrase> development, explanations of the <phrase>statistical modeling</phrase> process, and a few <phrase>basic</phrase> <phrase>modeling techniques</phrase> commonly used by statisticians. <phrase>Computer</phrase> demonstrations provide <phrase>concrete</phrase>, practical walkthroughs. Completion of this course will give you access to a wide <phrase>range</phrase> of <phrase>Bayesian</phrase> analytical tools, customizable to your <phrase>data</phrase>.
This course, which is designed to serve as the first course in the <phrase>Recommender Systems</phrase> specialization, introduces the concept of <phrase>recommender systems</phrase>, reviews several examples in detail, and leads you through non-personalized recommendation using <phrase>summary</phrase> <phrase>statistics</phrase> and product associations, <phrase>basic</phrase> <phrase>stereotype</phrase>-based or demographic recommendations, and <phrase>content-based</phrase> filtering recommendations.   After completing this course, you will be able to compute a <phrase>variety</phrase> of recommendations from datasets using <phrase>basic</phrase> <phrase>spreadsheet</phrase> tools, and if you complete the honors <phrase>track</phrase> you will also have programmed these recommendations using the <phrase>open source</phrase> LensKit recommender <phrase>toolkit</phrase>.    In addition to detailed lectures and interactive exercises, this course features interviews with several leaders in <phrase>research</phrase> and practice on <phrase>advanced topics</phrase> and current directions in <phrase>recommender systems</phrase>.
<phrase>Science</phrase> is undergoing a <phrase>data</phrase> explosion, and <phrase>astronomy</phrase> is leading the way. Modern <phrase>telescopes</phrase> produce terabytes of <phrase>data</phrase> per observation, and the simulations required to <phrase>model</phrase> our <phrase>observable Universe</phrase> push supercomputers to their limits. To analyse this <phrase>data</phrase> scientists need to be able to think computationally to <phrase>solve problems</phrase>. In this course you will investigate the challenges of working with <phrase>large datasets</phrase>: how to implement <phrase>algorithms</phrase> that work; how to use <phrase>databases</phrase> to manage your <phrase>data</phrase>; and how to learn from your <phrase>data</phrase> with <phrase>machine learning</phrase> tools. The focus is on practical skills - all the activities will be done in <phrase>Python</phrase> 3, a modern <phrase>programming language</phrase> used throughout <phrase>astronomy</phrase>.  Regardless of whether you’re already a <phrase>scientist</phrase>, studying to become one, or just interested in how modern <phrase>astronomy</phrase> works ‘under the bonnet’, this course will help you explore <phrase>astronomy</phrase>: from <phrase>planets</phrase>, to pulsars to <phrase>black</phrase> holes.  Course outline: Week 1: Thinking about <phrase>data</phrase> - Principles of computational thinking - Discovering pulsars in <phrase>radio</phrase> images  Week 2: <phrase>Big data</phrase> makes things slow - How to work out the time complexity of <phrase>algorithms</phrase> - Exploring the <phrase>black</phrase> holes at the centres of massive <phrase>galaxies</phrase>  Week 3: Querying <phrase>data</phrase> using <phrase>SQL</phrase> - How to use <phrase>databases</phrase> to analyse your <phrase>data</phrase> - Investigating <phrase>exoplanets</phrase> in other <phrase>solar</phrase> systems  Week 4: Managing your <phrase>data</phrase> - How to set up <phrase>databases</phrase> to manage your <phrase>data</phrase> - Exploring the lifecycle of <phrase>stars</phrase> in our <phrase>Galaxy</phrase>  Week 5: Learning from <phrase>data</phrase>: <phrase>regression</phrase> - Using <phrase>machine learning</phrase> tools to investigate your <phrase>data</phrase> - Calculating the redshifts of distant <phrase>galaxies</phrase>  Week 6: Learning from <phrase>data</phrase>: classification - Using <phrase>machine learning</phrase> tools to classify your <phrase>data</phrase> - Investigating different types of <phrase>galaxies</phrase>  Each week will also have an <phrase>interview</phrase> with a <phrase>data</phrase>-driven <phrase>astronomy</phrase> expert.  Note that some <phrase>knowledge</phrase> of <phrase>Python</phrase> is assumed, including variables, <phrase>control structures</phrase>, <phrase>data</phrase> structures, functions, and working with files.
In this second <phrase>MOOC</phrase> in the <phrase>Social Marketing</phrase> Specialization - "The Importance of Listening" - you will go deep into the <phrase>Big Data</phrase> of social and gain a more complete picture of what can be learned from interactions on social sites. You will be amazed at just how much <phrase>information</phrase> can be extracted from a <phrase>single</phrase> post, picture, or <phrase>video</phrase>. In this <phrase>MOOC</phrase>, guest speakers from Social Gist, BroadReader, Lexalytics, Semantria, Radian6, and IBM's Bluemix and <phrase>Social Media</phrase> Analytics Tools (<phrase>SMA</phrase>) will join <phrase>Professor</phrase> Hlavac to take you through the full <phrase>range</phrase> of analytics tools and options available to you and how to get the most from them. The best part, most of them will be available to you through the <phrase>MOOC</phrase> for <phrase>free</phrase>! Those purchasing the <phrase>MOOC</phrase> will receive special tools, templates, and videos to enhance your learning experience. In completing this course you will develop a fuller understanding of the <phrase>data</phrase> and will be able to increase the effectiveness of your content strategy by making better decisions and spotting crises before they happen! <phrase>MOOC</phrase> 2 bonus content in the paid <phrase>toolkit</phrase> includes access to Semantria's analytics <phrase>engine</phrase> to extract some <phrase>data</phrase> on the markets you are developing and have it analyzed.   As a <phrase>student</phrase> in this course, you are being provided the opportunity to access <phrase>IBM</phrase> Bluemix® <phrase>platform-as-a-service</phrase> trial for up to <phrase>six months</phrase> at no-charge with no <phrase>credit card</phrase> (up to a $1500 value).  NOTE: By enrolling in this course, given access to IBM's Bluemix <phrase>technology</phrase> for one month for <phrase>free</phrase> as well as Lexalytics' Semantria tool. For those earning a Course Certificate, you will be given an additional five months of Bluemix and three months of Semantria at no cost with a special key code. By enrolling for a Course Certificate for this <phrase>MOOC</phrase>, you are acknowledging that your <phrase>email</phrase> will be shared with Lexalytics for the sole and express purpose of generating your individual key code. After the key code has been generated, Lexalytics will delete your <phrase>email</phrase> from its records.   Additional <phrase>MOOC</phrase> 2 faculty include:  * Steve Dodd (<phrase>SVP</phrase> <phrase>Business</phrase> Development, Effyis - dba BoardReader and Socialgist - Global <phrase>Social Media</phrase> Content Access)  * Seth Redmore (CMO, Lexalytics, Inc.) * Chris Gruber (<phrase>Social Media</phrase> Analytics <phrase>Solution</phrase> <phrase>Architect</phrase>, <phrase>IBM</phrase>) * <phrase>Russell</phrase> Beardall (<phrase>Cloud</phrase> <phrase>Architect</phrase>, <phrase>IBM</phrase>)  * Tom Collinger (Executive <phrase>Director</phrase> <phrase>Spiegel</phrase> <phrase>Research</phrase> <phrase>Center</phrase> and Senior <phrase>Director</phrase> <phrase>Distance Learning</phrase>, Medill Integrated <phrase>Marketing</phrase> Communications, Northwestern) * Tressie <phrase>Lieberman</phrase> (VP <phrase>Digital</phrase> <phrase>Innovation</phrase>, <phrase>Taco</phrase> Bell)
Your <phrase>smartphone</phrase>, <phrase>smartwatch</phrase>, and <phrase>automobile</phrase> (if it is a newer <phrase>model</phrase>) have <phrase>AI</phrase> (<phrase>Artificial Intelligence</phrase>) inside serving you every day. In the near future, more advanced “self-learning” capable <phrase>DL</phrase> (<phrase>Deep Learning</phrase>) and <phrase>ML</phrase> (<phrase>Machine Learning</phrase>) <phrase>technology</phrase> will be used in almost every <phrase>aspect</phrase> of your <phrase>business</phrase> and <phrase>industry</phrase>. So now is the right time to learn what <phrase>DL</phrase> and <phrase>ML</phrase> is and how to use it in advantage of your <phrase>company</phrase>. This course has three parts, where the first part focuses on <phrase>DL</phrase> and <phrase>ML</phrase> <phrase>technology</phrase> based future <phrase>business</phrase> strategy including details on new <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>products</phrase>/services and <phrase>open source</phrase> <phrase>DL</phrase> <phrase>software</phrase>, which are the future enablers. The second part focuses on the core technologies of <phrase>DL</phrase> and <phrase>ML</phrase> systems, which include NN (<phrase>Neural Network</phrase>), <phrase>CNN</phrase> (Convolutional NN), and RNN (Recurrent NN) systems. The third part focuses on four TensorFlow <phrase>Playground</phrase> projects, where experience on designing <phrase>DL</phrase> NNs can be gained using an easy and fun yet very powerful <phrase>application</phrase> called the TensorFlow <phrase>Playground</phrase>. This course was designed to help you build <phrase>business</phrase> strategies and enable you to conduct technical planning on new <phrase>DL</phrase> and <phrase>ML</phrase> services and <phrase>products</phrase>.
In this course you will learn how to quickly and easily get started with <phrase>Artificial Intelligence</phrase> using <phrase>IBM</phrase> Watson. You will understand how Watson works, become familiar with its <phrase>use cases</phrase> and <phrase>real life</phrase> client examples, and be introduced to several of Watson <phrase>AI</phrase> services from <phrase>IBM</phrase> that enable anyone to easily apply <phrase>AI</phrase> and build smart apps. You will also work with several Watson services to demonstrate <phrase>AI</phrase> in <phrase>action</phrase>.   This course does not require any <phrase>programming</phrase> or <phrase>computer science</phrase> <phrase>expertise</phrase> and is designed for anyone whether you have a technical background or not.
<phrase>Case</phrase> Study - Predicting Housing Prices  In our first <phrase>case</phrase> study, predicting <phrase>house</phrase> prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...).  This is just one of the many places where <phrase>regression</phrase> can be applied.  Other applications <phrase>range</phrase> from predicting <phrase>health</phrase> outcomes in <phrase>medicine</phrase>, <phrase>stock</phrase> prices in <phrase>finance</phrase>, and <phrase>power usage</phrase> in <phrase>high-performance computing</phrase>, to analyzing which regulators are important for <phrase>gene expression</phrase>.  In this course, you will explore regularized <phrase>linear regression</phrase> models for the <phrase>task</phrase> of prediction and <phrase>feature selection</phrase>.  You will be able to <phrase>handle</phrase> very large sets of features and select between models of various complexity.  You will also analyze the impact of aspects of your <phrase>data</phrase> -- such as <phrase>outliers</phrase> -- on your selected models and predictions.  To fit these models, you will implement <phrase>optimization algorithms</phrase> that scale to <phrase>large datasets</phrase>.  <phrase>Learning Outcomes</phrase>:  By the end of this course, you will be able to:    -Describe the <phrase>input and output</phrase> of a <phrase>regression</phrase> <phrase>model</phrase>.    -Compare and contrast bias and <phrase>variance</phrase> when modeling <phrase>data</phrase>.    -Estimate <phrase>model</phrase> parameters using <phrase>optimization algorithms</phrase>.    -Tune parameters with <phrase>cross validation</phrase>.    -Analyze the performance of the <phrase>model</phrase>.    -Describe the notion of sparsity and how <phrase>LASSO</phrase> leads to sparse solutions.    -Deploy methods to select between models.    -Exploit the <phrase>model</phrase> to form predictions.     -Build a <phrase>regression</phrase> <phrase>model</phrase> to predict prices using a housing dataset.    -Implement these techniques in <phrase>Python</phrase>.
Have you ever heard about such technologies as HDFS, <phrase>MapReduce</phrase>, Spark? Always wanted to learn these new tools but missed concise starting material? Don’t miss this course either!   In this 6-week course you will: - learn some <phrase>basic</phrase> technologies of the modern <phrase>Big Data</phrase> <phrase>landscape</phrase>, namely: HDFS, <phrase>MapReduce</phrase> and Spark; - be guided both through systems internals and their applications; - learn about <phrase>distributed file systems</phrase>, why they exist and what <phrase>function</phrase> they serve; - grasp the <phrase>MapReduce</phrase> framework, a workhorse for many modern <phrase>Big Data</phrase> applications; - apply the framework to process texts and solve sample <phrase>business</phrase> cases; - learn about Spark, the <phrase>next-generation</phrase> computational framework; - build a strong understanding of Spark <phrase>basic</phrase> concepts; - develop skills to apply these tools to creating solutions in <phrase>finance</phrase>, <phrase>social networks</phrase>, <phrase>telecommunications</phrase> and many other fields.  Your learning experience will be as close to <phrase>real life</phrase> as possible with the chance to evaluate your practical assignments on a real cluster. No mocking, a <phrase>friendly</phrase> considerate <phrase>atmosphere</phrase> to make the process of your learning smooth and enjoyable.   Get ready to work with real datasets alongside with real <phrase>masters</phrase>!  Special thanks to: - Prof. Mikhail Roytberg, APT <phrase>dept</phrase>., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the <phrase>road</phrase>. - Oleg Sukhoroslov (<phrase>PhD</phrase>, Senior Researcher at IITP RAS), who has been teaching  <phrase>MapReduce</phrase>, <phrase>Hadoop</phrase>  and <phrase>friends</phrase> since 2008. Now he is leading the <phrase>infrastructure</phrase> team. - Oleg Ivchenko (<phrase>PhD</phrase> <phrase>student</phrase> APT <phrase>dept</phrase>., MIPT), Pavel Akhtyamov (<phrase>MSc</phrase>. <phrase>student</phrase> at APT <phrase>dept</phrase>., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl <phrase>State</phrase> <phrase>University</phrase>), superbrains who have developed and now maintain the <phrase>infrastructure</phrase> used for practical assignments in this course. - Asya Roitberg, <phrase>Eugene</phrase> Baulin, <phrase>Marina</phrase> Sudarikova. These people never <phrase>sleep</phrase> to babysit this course day and night, to make your learning experience productive, smooth and exciting.
This course explores <phrase>Excel</phrase> as a tool for solving <phrase>business</phrase> problems. In this course you will learn the <phrase>basic</phrase> functions of <phrase>excel</phrase> through guided demonstration. Each week you will build on your <phrase>excel</phrase> skills and be provided an opportunity to practice what you’ve learned. Finally, you will have a chance to put your <phrase>knowledge</phrase> to work in a final project.  Please note, the content in this course was developed using a <phrase>Windows</phrase> version of <phrase>Excel</phrase> 2013.    This course was created by <phrase>PricewaterhouseCoopers</phrase> LLP with an <phrase>address</phrase> at 300 <phrase>Madison</phrase> Avenue, <phrase>New York</phrase>, <phrase>New York</phrase>, 10017.
In this first course of the specialization, you will discover just what <phrase>data visualization</phrase> is, and how we can use it to better see and understand <phrase>data</phrase>. Using Tableau, we’ll examine the fundamental concepts of <phrase>data visualization</phrase> and explore the Tableau interface, identifying and applying the various tools Tableau has to offer. By the end of the course you will be able to prepare and <phrase>import</phrase> <phrase>data</phrase> into Tableau and explain the relationship between <phrase>data</phrase> analytics and <phrase>data</phrase> visualization. This course is designed for the learner who has never used Tableau before, or who may need a refresher or want to explore Tableau in more depth.  No prior technical or analytical background is required.  The course will guide you through the steps necessary to create your first visualization story from the beginning based on <phrase>data</phrase> <phrase>context</phrase>,  setting the stage for you to advance to the next course in the Specialization.
This <phrase>Model-Based</phrase> <phrase>Systems Engineering</phrase> (MBSE) course and the <phrase>Digital</phrase> Thread courses featured earlier in this specialization bring together the concepts from across <phrase>digital</phrase> <phrase>manufacturing</phrase> and <phrase>design</phrase>, forming a vision in which the <phrase>geometry</phrase> of a product is just one way of describing it. MBSE is where the <phrase>model</phrase> resulting from the <phrase>evolution</phrase> of system requirements, <phrase>design</phrase>, analysis, <phrase>verification and validation</phrase> activities is the focus of <phrase>design</phrase> and <phrase>manufacturing</phrase>. Students will gain an understanding of <phrase>systems engineering</phrase>, the <phrase>model-based</phrase> approach to <phrase>design</phrase> and <phrase>manufacturing</phrase>, the <phrase>Digital</phrase> Twin, and a roadmap toward a <phrase>model</phrase>-based enterprise.  Students will be able to explain the value and expectations of <phrase>systems engineering</phrase> and <phrase>model-based</phrase> <phrase>systems engineering</phrase>, and the underlying motivations and opportunities represented by a <phrase>model</phrase>-based enterprise. They will develop the <phrase>knowledge</phrase> necessary to perform a <phrase>baseline</phrase> assessment of an organization’s potential to leverage MBSE.   Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the eighth course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
<phrase>An introduction</phrase> to the <phrase>statistics</phrase> behind the most popular <phrase>genomic</phrase> <phrase>data science</phrase> projects. This is the sixth course in the <phrase>Genomic</phrase> <phrase>Big Data</phrase> <phrase>Science</phrase> Specialization from <phrase>Johns Hopkins University</phrase>.
In this course, we will expand our exploration of <phrase>statistical inference</phrase> techniques by focusing on the <phrase>science</phrase> and <phrase>art</phrase> of fitting <phrase>statistical models</phrase> to <phrase>data</phrase>. We will build on the concepts presented in the <phrase>Statistical Inference</phrase> course (Course 2) to emphasize the importance of connecting <phrase>research</phrase> questions to our <phrase>data analysis</phrase> methods. We will also focus on various modeling objectives, including making inference about relationships between variables and generating predictions for future observations.  This course will <phrase>introduce</phrase> and explore various <phrase>statistical modeling</phrase> techniques, including <phrase>linear regression</phrase>, <phrase>logistic regression</phrase>, <phrase>generalized linear</phrase> models, hierarchical and mixed effects (or multilevel) models, and <phrase>Bayesian inference</phrase> techniques. All techniques will be illustrated using a <phrase>variety</phrase> of real <phrase>data</phrase> sets, and the course will emphasize different modeling approaches for different types of <phrase>data</phrase> sets, depending on the <phrase>study design</phrase> underlying the <phrase>data</phrase> (referring back to Course 1, Understanding and Visualizing <phrase>Data</phrase> with <phrase>Python</phrase>).  During these lab-based sessions, learners will work through tutorials focusing on specific <phrase>case</phrase> studies to help solidify the week’s statistical concepts, which will include further deep dives into <phrase>Python</phrase> <phrase>libraries</phrase> including Statsmodels, <phrase>Pandas</phrase>, and Seaborn. This course utilizes the Jupyter Notebook environment within <phrase>Coursera</phrase>.
<phrase>Data analysis</phrase> has replaced <phrase>data</phrase> acquisition as the bottleneck to <phrase>evidence-based</phrase> <phrase>decision making</phrase> --- we are drowning in it.  Extracting <phrase>knowledge</phrase> from large, heterogeneous, and noisy datasets requires not only powerful <phrase>computing</phrase> resources, but the <phrase>programming</phrase> abstractions to use them effectively.  The abstractions that emerged in the last decade blend ideas from parallel <phrase>databases</phrase>, <phrase>distributed systems</phrase>, and <phrase>programming</phrase> languages to create a new class of scalable <phrase>data</phrase> analytics platforms that form the foundation for <phrase>data science</phrase> at realistic scales.  In this course, you will learn the <phrase>landscape</phrase> of relevant systems, the principles on which they rely, their tradeoffs, and how to evaluate their utility against your requirements. You will learn how practical systems were derived from the frontier of <phrase>research</phrase> in <phrase>computer science</phrase> and what systems are coming on the horizon.   <phrase>Cloud computing</phrase>, <phrase>SQL</phrase> and <phrase>NoSQL</phrase> <phrase>databases</phrase>, <phrase>MapReduce</phrase> and the <phrase>ecosystem</phrase> it spawned, Spark and its contemporaries, and specialized systems for <phrase>graphs</phrase> and arrays will be <phrase>covered</phrase>.  You will also learn the <phrase>history</phrase> and <phrase>context</phrase> of <phrase>data science</phrase>, the skills, challenges, and methodologies the term implies, and how to structure a <phrase>data science</phrase> project.  At the end of this course, you will be able to:  Learning Goals:  1. Describe common patterns, challenges, and approaches associated with <phrase>data science</phrase> projects, and what makes them different from projects in <phrase>related fields</phrase>. 2. Identify and use the <phrase>programming</phrase> models associated with scalable <phrase>data</phrase> manipulation, including <phrase>relational algebra</phrase>, <phrase>mapreduce</phrase>, and other <phrase>data</phrase> flow models. 3. Use <phrase>database</phrase> <phrase>technology</phrase> adapted for <phrase>large-scale</phrase> analytics, including the concepts driving parallel <phrase>databases</phrase>, parallel <phrase>query processing</phrase>, and in-<phrase>database</phrase> analytics 4. Evaluate key-value stores and <phrase>NoSQL</phrase> systems, describe their tradeoffs with comparable systems, the details of important examples in the space, and <phrase>future trends</phrase>. 5. “Think” in <phrase>MapReduce</phrase> to effectively write <phrase>algorithms</phrase> for systems including <phrase>Hadoop</phrase> and Spark.  You will understand their limitations, <phrase>design</phrase> details, their relationship to <phrase>databases</phrase>, and their associated <phrase>ecosystem</phrase> of <phrase>algorithms</phrase>, extensions, and languages. <phrase>write programs</phrase> in Spark 6. Describe the <phrase>landscape</phrase> of specialized <phrase>Big Data</phrase> systems for <phrase>graphs</phrase>, arrays, and streams
Whether being used to customize <phrase>advertising</phrase> to millions of <phrase>website</phrase> visitors or <phrase>streamline</phrase> inventory ordering at a small <phrase>restaurant</phrase>, <phrase>data</phrase> is becoming more <phrase>integral</phrase> to success. Too often, we’re not sure how use <phrase>data</phrase> to find answers to the questions that will make us more successful in what we do. In this course, you will discover what <phrase>data</phrase> is and think about what questions you have that can be answered by the <phrase>data</phrase> – even if you’ve never thought about <phrase>data</phrase> before. Based on existing <phrase>data</phrase>, you will learn to develop a <phrase>research</phrase> question, describe the variables and their relationships, calculate <phrase>basic</phrase> <phrase>statistics</phrase>, and present your <phrase>results</phrase> clearly. By the end of the course, you will be able to use powerful <phrase>data analysis</phrase> tools – either <phrase>SAS</phrase> or <phrase>Python</phrase> – to manage and visualize your <phrase>data</phrase>, including how to deal with <phrase>missing data</phrase>, <phrase>variable</phrase> groups, and <phrase>graphs</phrase>. Throughout the course, you will share your progress with others to gain valuable <phrase>feedback</phrase>, while also learning how your <phrase>peers</phrase> use <phrase>data</phrase> to answer their own questions.
How can you put <phrase>data</phrase> to work for you? Specifically, how can numbers in a <phrase>spreadsheet</phrase> tell us about present and past <phrase>business</phrase> activities, and how can we use them to forecast the future? The answer is in building quantitative models, and this course is designed to help you understand the fundamentals of this critical, foundational, <phrase>business</phrase> skill. Through a series of <phrase>short</phrase> lectures, demonstrations, and assignments, you’ll learn the key ideas and process of quantitative modeling so that you can begin to create your own models for your own <phrase>business</phrase> or enterprise. By the end of this course, you will have seen a <phrase>variety</phrase> of practical commonly used quantitative models as well as the <phrase>building blocks</phrase> that will allow you to start structuring your own models. These <phrase>building blocks</phrase> will be put to use in the other courses in this Specialization.
Анализ данных и машинное обучение существенно опираются на результаты из математического анализа, линейной алгебры, методов оптимизации, теории вероятностей. Без фундаментальных знаний по этим наукам невозможно понимать, как устроены методы анализа данных. Задача этого курса — сформировать такой фундамент. Мы обойдёмся без сложных формул и доказательств и сделаем упор на интерпретации и понимании смысла математических понятий и объектов.   Для успешного применения методов анализа данных нужно уметь программировать. Фактическим стандартом для этого в наши дни является язык <phrase>Python</phrase>. В данном курсе мы предлагаем познакомиться с его синтаксисом, а также научиться работать с его основными библиотеками, полезными для анализа данных, например, <phrase>NumPy</phrase>, <phrase>SciPy</phrase>, Matplotlib и <phrase>Pandas</phrase>.  Задания и видео разработаны на <phrase>Python</phrase> 2.
This course will <phrase>introduce</phrase> you to the wonderful world of <phrase>Python</phrase> <phrase>programming</phrase>!  We'll learn about the essential elements of <phrase>programming</phrase> and how to construct <phrase>basic</phrase> <phrase>Python</phrase> programs. We will <phrase>cover</phrase> expressions, variables, functions, <phrase>logic</phrase>, and conditionals, which are foundational concepts in <phrase>computer</phrase> <phrase>programming</phrase>. We will also teach you how to use <phrase>Python</phrase> modules, which enable you to benefit from the vast <phrase>array</phrase> of functionality that is already a part of the <phrase>Python</phrase> <phrase>language</phrase>. These concepts and skills will help you to begin to think like a <phrase>computer</phrase> <phrase>programmer</phrase> and to understand how to go about writing <phrase>Python</phrase> programs.  By the end of the course, you will be able to write <phrase>short</phrase> <phrase>Python</phrase> programs that are able to accomplish real, practical tasks. This course is the foundation for building <phrase>expertise</phrase> in <phrase>Python</phrase> <phrase>programming</phrase>. As the first course in a specialization, it provides the necessary <phrase>building blocks</phrase> for you to succeed at learning to write more complex <phrase>Python</phrase> programs.  This course uses <phrase>Python</phrase> 3.  While many <phrase>Python</phrase> programs continue to use <phrase>Python</phrase> 2, <phrase>Python</phrase> 3 is the future of the <phrase>Python programming language</phrase>. This first course will use a <phrase>Python</phrase> 3 version of the CodeSkulptor <phrase>development environment</phrase>, which is <phrase>specifically designed</phrase> to help beginning programmers learn quickly.  CodeSkulptor <phrase>runs</phrase> within any modern <phrase>web browser</phrase> and does not require you to <phrase>install</phrase> any <phrase>software</phrase>, allowing you to start writing and running <phrase>small programs</phrase> immediately.  In the later courses in this specialization,  we will help you to move to more sophisticated desktop <phrase>development environments</phrase>.
Starting from a <phrase>history</phrase> of <phrase>machine learning</phrase>, we discuss why <phrase>neural networks</phrase> today perform so well in a <phrase>variety</phrase> of <phrase>data science</phrase> problems. We then discuss how to set up a <phrase>supervised learning</phrase> problem and find a good <phrase>solution</phrase> using <phrase>gradient</phrase> descent. This involves creating datasets that <phrase>permit</phrase> generalization; we <phrase>talk</phrase> about methods of doing so in a repeatable way that supports experimentation.  Course Objectives: Identify why <phrase>deep learning</phrase> is currently popular Optimize and evaluate models using loss functions and <phrase>performance metrics</phrase> Mitigate <phrase>common problems</phrase> that arise in <phrase>machine learning</phrase> Create repeatable and scalable training, evaluation, and <phrase>test</phrase> datasets
This is the first course in the four-course specialization <phrase>Python</phrase> <phrase>Data</phrase> <phrase>Products</phrase> for <phrase>Predictive Analytics</phrase>, introducing the basics of <phrase>reading</phrase> and manipulating datasets in <phrase>Python</phrase>. In this course, you will learn what a <phrase>data</phrase> product is and go through several <phrase>Python</phrase> <phrase>libraries</phrase> to perform <phrase>data</phrase> retrieval, processing, and visualization.   This course will <phrase>introduce</phrase> you to the field of <phrase>data science</phrase> and prepare you for the next three courses in the Specialization: <phrase>Design</phrase> Thinking and <phrase>Predictive Analytics</phrase> for <phrase>Data</phrase> <phrase>Products</phrase>, Meaningful Predictive Modeling, and Deploying <phrase>Machine Learning</phrase> Models. At each <phrase>step</phrase> in the specialization, you will gain hands-on experience in <phrase>data</phrase> manipulation and building your skills, eventually culminating in a capstone project encompassing all the concepts taught in the specialization.
Improving <phrase>health</phrase> and <phrase>healthcare</phrase> institutions requires understanding of <phrase>data</phrase> and creation of interventions at the many levels at which <phrase>health</phrase> IT interact and affect the institution. These levels <phrase>range</phrase> from the external “world” in which the institution operates down to the specific technologies. <phrase>Data</phrase> scientists find that, when they aim at implementing their models in practice, it is the “socio” components that are both novel to them and <phrase>mission critical</phrase> to success. At the end of this course, students will be able to make a quick assessment of a <phrase>health informatics</phrase> problem—or a proposed <phrase>solution</phrase>—and to determine what is missing and what more needs to be learned.
Learn how to <phrase>model</phrase> social and economic networks and their impact on <phrase>human</phrase> behavior.  How do networks form, why do they exhibit certain patterns, and how does their structure impact <phrase>diffusion</phrase>, learning, and other behaviors?   We will bring together models and techniques from <phrase>economics</phrase>, <phrase>sociology</phrase>, <phrase>math</phrase>, <phrase>physics</phrase>, <phrase>statistics</phrase> and <phrase>computer science</phrase> to answer these questions.  The course begins with some empirical background on social and economic networks, and an overview of concepts used to describe and measure networks. Next, we will <phrase>cover</phrase> a set of models of how networks form, including random network models as well as strategic formation models, and some hybrids. We will then discuss a series of models of how networks impact behavior, including contagion, <phrase>diffusion</phrase>, learning, and peer influences.  You can find a more detailed syllabus here:  http://web.stanford.edu/~jacksonm/Networks-Online-Syllabus.pdf   You can find a <phrase>short</phrase> introductory videao here: http://web.stanford.edu/~jacksonm/Intro_Networks.mp4
В машинном обучении встречаются задачи, где нужно изучить структуру данных, найти в них скрытые взаимосвязи и закономерности. Например, нам может понадобиться описать каждого клиента банка с помощью меньшего количества переменных — для этого можно использовать методы понижения размерности, основанные на матричных разложениях. Такие методы пытаются сформировать новые признаки на основе старых, сохранив как можно больше информации в данных. Другим примером может служить задача тематического моделирования, в которой для набора текстов нужно построить модель, объясняющую процесс формирования этих текстов из небольшого количества тем.  Такие задачи назвают обучением без учителя. В отличие от обучения с учителем, в них не предполагают восстановление зависимости между объектами и целевой переменной.   Из этого курса вы узнаете об алгоритмах кластеризации данных, с помощью которых, например, можно искать группы схожих клиентов мобильного оператора. Вы научитесь строить матричные разложения и решать задачу тематического моделирования, понижать размерность данных, искать аномалии и визуализировать многомерные данные.  Задания и видео курса разработаны на <phrase>Python</phrase> 2.
The course is a compendium of the must-have <phrase>expertise</phrase> in <phrase>data science</phrase> for executive and middle-<phrase>management</phrase> to foster <phrase>data</phrase>-driven <phrase>innovation</phrase>. It consists of introductory lectures spanning <phrase>big data</phrase>, <phrase>machine learning</phrase>, <phrase>data</phrase> valorization and <phrase>communication</phrase>. Topics <phrase>cover</phrase> the essential concepts and intuitions on <phrase>data</phrase> needs, <phrase>data</phrase> analysis, <phrase>machine learning</phrase> methods, respective <phrase>pros and cons</phrase>, and practical applicability issues.	  The course covers terminology and concepts, tools and methods, <phrase>use cases</phrase> and success stories of <phrase>data science</phrase> applications.  The course explains what is <phrase>Data Science</phrase> and why it is so hyped. It discusses the value that <phrase>Data Science</phrase> can create, the main classes of problems that <phrase>Data Science</phrase> can solve, the difference is between descriptive, predictive and prescriptive analytics, and the roles of <phrase>machine learning</phrase> and <phrase>artificial intelligence</phrase>.  From a more technical perspective, the course covers supervised, unsupervised and <phrase>semi-supervised</phrase> methods, and explains what can be obtained with classification, clustering, and <phrase>regression</phrase> techniques. It discusses the role of <phrase>NoSQL</phrase> <phrase>data</phrase> models and technologies, and the role and impact of scalable <phrase>cloud</phrase>-based computation platforms. All topics are <phrase>covered</phrase> with example-based lectures, discussing <phrase>use cases</phrase>, success stories and realistic examples.
Writing good code for <phrase>data science</phrase> is only part of the job. In <phrase>order</phrase> to maximizing the usefulness and reusability of <phrase>data science</phrase> <phrase>software</phrase>, code must be organized and distributed in a manner that adheres to <phrase>community</phrase>-based standards and provides a good <phrase>user experience</phrase>. This course covers the primary means by which R <phrase>software</phrase> is organized and distributed to others. We <phrase>cover</phrase> R package development, writing good documentation and vignettes, writing robust <phrase>software</phrase>, <phrase>cross-platform</phrase> development, <phrase>continuous integration</phrase> tools, and distributing packages via CRAN and <phrase>GitHub</phrase>. Learners will produce R packages that satisfy the criteria for submission to CRAN.
This course covers the analysis of <phrase>Functional Magnetic Resonance Imaging</phrase> (<phrase>fMRI</phrase>) <phrase>data</phrase>. It is a <phrase>continuation</phrase> of the course “Principles of <phrase>fMRI</phrase>, Part 1”
<phrase>Epidemiological</phrase> studies can provide valuable insights about the <phrase>frequency</phrase> of a <phrase>disease</phrase>, its potential causes and the effectiveness of available treatments. Selecting an appropriate <phrase>study design</phrase> can take you a <phrase>long</phrase> way when trying to answer such a question. However, this is by no means enough. A study can yield biased <phrase>results</phrase> for many different reasons. This course offers <phrase>an introduction</phrase> to some of these factors and provides guidance on how to deal with bias in <phrase>epidemiological</phrase> <phrase>research</phrase>. In this course you will learn about the main types of bias and what effect they might have on your study findings. You will then focus on the concept of <phrase>confounding</phrase> and you will explore various methods to identify and control for <phrase>confounding</phrase> in different study designs. In the last module of this course we will discuss the phenomenon of effect modification, which is key to understanding and interpreting study <phrase>results</phrase>. We will finish the course with a broader discussion of <phrase>causality</phrase> in <phrase>epidemiology</phrase> and we will highlight how you can utilise all the tools that you have learnt to decide whether your findings indicate a true association and if this can be considered causal.
In this culminating project, you will deploy the tools and techniques that you've mastered over the course of the specialization. You'll work with a real <phrase>data</phrase> set to perform analyses and prepare a <phrase>report</phrase> of your findings.
课程介绍：  在社会学、心理学、教育学、经济学、管理学、市场学等研究领域的数据分析中，结构方程建模是当前最前沿的统计方法中应用最广、研究最多的一个。它包含了方差分析、回归分析、路径分析和因子分析，弥补了传统回归分析和因子分析的不足，可以分析多因多果的联系、潜变量的关系，还可以处理多水平数据和纵向数据，是非常重要的多元数据分析工具。本课程系统地介绍结构方程模型和LISREL软件的应用，内容包括：结构方程分析（包括验证性因子分析）的基本概念、统计原理、在社会科学研究中的应用、常用模型及其LISREL程序、结果的解释和模型评价。学员应具备基本的统计知识（如：标准差、t-检验、相关系数），理解回归分析和因子分析的概念。 注：本课程配套教材为《结构方程模型及其应用》（以LISREL软件为例）。   修课背景要求：  讲学语言：普通话及广东话 / 简体中文 这是一个艰深的高阶课程，学员应有下述的知识及训练：(i) 使用<phrase>SPSS</phrase>, <phrase>SAS</phrase>或其他类似软件包；(<phrase>ii</phrase>) 回归；和(<phrase>iii</phrase>) 因子分析(探索性因子分析)。   课程目标：  完成课程之后，学生的预期学习成果是：  1. 能够说出与传统的<phrase>ANOVA</phrase>和回归分析法相比，结构方程模型的优点； 2. 能够在仪器上进行验证性因子分析； 3. 能够用结构方程模型分析简单的全模型； 4. 通过计算出各种拟合指数和运用其他评估标准，能够比较并选出适合的模型； 5. 能够基于相应的参数统计修改模型；
This course will help you recognize how the "<phrase>digital</phrase> thread" is the backbone of the <phrase>digital</phrase> <phrase>manufacturing</phrase> and <phrase>design</phrase> (DM&D) transformation, turning <phrase>manufacturing</phrase> processes from <phrase>paper</phrase>-based to <phrase>digital</phrase>-based. You will have a working understanding of the <phrase>digital</phrase> thread – the <phrase>stream</phrase> that starts at product concept and continues to accumulate <phrase>information</phrase> and <phrase>data</phrase> throughout the product’s <phrase>life</phrase> cycle – and identify opportunities to leverage it.   Gain an understanding of how "the right <phrase>information</phrase>, in the right <phrase>place</phrase>, at the right time" should flow. This is one of the keys to unlocking the potential of a <phrase>digital</phrase> <phrase>design</phrase> process. Acknowledging this will enable you to be more involved in a product’s <phrase>development cycle</phrase>, and to help a <phrase>company</phrase> become more flexible.   Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the second course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
ستتلقى في هذه الدورة التدريبية مقدمة عن الأدوات الرئيسية والأفكار الخاصة بمجموعة أدوات عالم البيانات. تقدم الدورة التدريبية نظرة عامة عن البيانات والاستفسارات والأدوات التي يعمل عليها علماء البيانات ومحللو البيانات. هناك عنصران لهذه الدورة التدريبية. الأول هو مقدمة نظرية عن الأفكار الكامنة وراء تحويل البيانات إلى معلومات قابلة للتطبيق. والثاني هو مقدمة عملية عن الأدوات التي سيتم استخدامها في البرنامج مثل التحكم في النُسَخ ولغة <phrase>Markdown</phrase> وGit وGitHub وR وRStudio.
This <phrase>MOOC</phrase> – a joint <phrase>initiative</phrase> between EIT <phrase>Digital</phrase>, Université de <phrase>Nice</phrase> <phrase>Sophia-Antipolis</phrase> / Université Côte d'Azur and <phrase>INRIA</phrase> - introduces the <phrase>Linked Data</phrase> standards and principles that provide the foundation of <phrase>the Semantic web</phrase>. You will learn how to publish, obtain and use <phrase>structured data</phrase> directly from the Web. Learning the principles, languages and standards to exchange <phrase>Data</phrase> on the Web will enable you to <phrase>design</phrase> and produce new applications, <phrase>products</phrase> and services that leverage the volume and <phrase>variety</phrase> of <phrase>data</phrase> the Web holds.  We divided this course into four parts that <phrase>cover</phrase> the core technical skills and competencies you need to <phrase>master</phrase> to be able to use the Web as a space for giant structure <phrase>data</phrase> exchange: •    in the first part, “Principals of a Web of <phrase>Linked Data</phrase>”: you will learn and practice the principles to publish and obtain <phrase>data</phrase> directly on the Web instead of <phrase>Web pages</phrase>;  •    in the second part, “The <phrase>RDF</phrase> <phrase>Data Model</phrase>”: you will learn the standard <phrase>data model</phrase> for the Web and its syntaxes to publish and link <phrase>data</phrase> on the Web in your applications and services; •    in the third part, “<phrase>SPARQL</phrase> <phrase>Query Language</phrase>”: you will learn how to directly query and access <phrase>data</phrase> sources on the Web and obtain <phrase>structured data</phrase> relevant to your <phrase>activity</phrase> and <phrase>domain</phrase>; •    in the fourth and final part, “<phrase>Integration</phrase> of other <phrase>Data</phrase> Formats and Sources”: you will learn how the <phrase>Web standards</phrase> interact and interoperate with other <phrase>data</phrase> formats to allow the <phrase>integration</phrase> of a <phrase>variety</phrase> of <phrase>data</phrase> sources.  Each week alternates <phrase>short</phrase> videos and quizzes, as well as supplementary resources and forums to gradually progress through the different principles and standards.  After following this course successfully, you will have the skills to obtain focused and structured datasets from the Web that you can then use to augment you own datasets, enrich their dimensions, feed your applications, perform <phrase>data mining</phrase>, <phrase>machine learning</phrase> and training, <phrase>data</phrase> analysis, <phrase>AI</phrase> processing and reasoning and other <phrase>data management</phrase>.
<phrase>El</phrase> presente curso tiene como objetivo presentar los métodos y técnicas básicos para <phrase>el</phrase> procesamiento y análisis de datos <phrase>en</phrase> <phrase>el</phrase> contexto de <phrase>Big Data</phrase>. No prentende ser un curso exhaustivo sobre <phrase>Machine Learning</phrase> ni sobre métodos Estadísticos, simplemente <phrase>se</phrase> pretenden mostrar las características principales de estas técnicas para que <phrase>el</phrase> alumno pueda tener una visión <phrase>general</phrase> de las opciones que ofrece <phrase>el</phrase> análisis de datos para poder explorar, confirmar indicios y <phrase>en</phrase> definitiva, extraer conclusiones.  <phrase>El</phrase> curso está dirigido a estudiantes y profesionales que deseen aproximarse al procesamiento y análisis de datos <phrase>en</phrase> <phrase>Big Data</phrase>. Aunque no <phrase>es</phrase> un requisito indispensable tener experiencia <phrase>en</phrase> análisis de datos o <phrase>en</phrase> entornos <phrase>Big Data</phrase>, <phrase>el</phrase> curso puede resultar especialmente interesante  a estudiantes con ciertos conocimientos de análisis de datos que deseen introducirse <phrase>en</phrase> <phrase>el</phrase> entorno <phrase>Big Data</phrase>, por otro lado, también resultará interesante a aquellos estudiantes con cierta experiencia <phrase>en</phrase> entornos <phrase>Big Data</phrase> que deseen adquirir una <phrase>mayor</phrase> visión analítica.   <phrase>En</phrase> este sentido <phrase>el</phrase> curso pretende ofrecer recursos realistas <phrase>en</phrase> <phrase>el</phrase> contexto <phrase>Big Data</phrase> y por este motivo <phrase>se</phrase> trabajará <phrase>des</phrase> de una máquina virtual con <phrase>la</phrase> aplicación Jupyter como enlace para desarrollar los modelos y técnicas con PySpark.  <phrase>El</phrase> curso está dividido <phrase>en</phrase> 4 módulos más o menos independientes aunque <phrase>se</phrase> recomienda realizarlos de forma secuencial.   <phrase>En</phrase> <phrase>el</phrase> Módulo 1 <phrase>se</phrase> presentan los diferentes problemas y técnicas más habitules para analizar datos desde una perspectiva <phrase>general</phrase>. También <phrase>se</phrase> <phrase>introduce</phrase> <phrase>el</phrase> caso de estudio y las herramientas de trabajo que <phrase>se</phrase> emplearán. <phrase>El</phrase> resto de módulo está dedicado a <phrase>la</phrase> tarea de Exploración y Pre-Proceso de los datos, incluyendo consultas, tareas de gestión, resúmenes numéricos y gráficos. Los siguientes módulos <phrase>se</phrase> focalizan <phrase>en</phrase> las técnicas de análisis.  <phrase>El</phrase> Módulo 2 <phrase>se</phrase> <phrase>centra</phrase> <phrase>en</phrase> técnicas de modelización básicas, <phrase>en</phrase> particular regresión y regresión logística. Además de repasar las etapas de calibración del modelo, también <phrase>se</phrase> incluyen las etapas de validación y simplificación.  <phrase>El</phrase> módulo 3 está plenamente dedicado a <phrase>la</phrase> técnica de Árboles de Regresión y Clasificación. También <phrase>se</phrase> incluyen los bosques aleatorios.    <phrase>El</phrase> módulo final contiene <phrase>la</phrase> técnica de Redes Neuronales para clasificación y también una introducción a las técnicas No Supervisadas, <phrase>en</phrase> particular, reducción de dimensión a través del análisis de componentes principales y <phrase>la</phrase> clasificación automática a través del análisis de clústers.
Welcome to <phrase>Data</phrase> Analytics Foundations for <phrase>Accountancy</phrase> <phrase>II</phrase>!  I'm excited to have you in the class and look <phrase>forward</phrase> to your contributions to the learning <phrase>community</phrase>.  To begin, I recommend taking a few minutes to explore the course site. Review the material we’ll <phrase>cover</phrase> each week, and preview the assignments you’ll need to complete to <phrase>pass</phrase> the course. Click Discussions to see forums where you can discuss the course material with <phrase>fellow</phrase> students taking the class.  If you have questions about course content, please post them in the forums to get help from others in the course <phrase>community</phrase>. For technical problems with the <phrase>Coursera</phrase> platform, visit the Learner Help <phrase>Center</phrase>.  Good luck as you get started, and I hope you enjoy the course!
This course gives an overview of the changing regulatory environment since the 1997 <phrase>Asian</phrase> and 2008 global <phrase>financial crisis</phrase>. Following these two <phrase>major</phrase> crises, governments around the globe enacted a set of far-reaching new financial regulations that are aimed towards safeguarding financial stability. However, <phrase>banks</phrase> find it increasingly difficult to be profitable in this new regulatory environment. <phrase>Technology</phrase>, at the same time, has taken important leaps <phrase>forward</phrase> with the emergence of sophisticated models of <phrase>artificial intelligence</phrase> and the <phrase>invention</phrase> of the blockchain. These two developments <phrase>fuel</phrase> the emergence of fintech companies around the world.   This course discusses fintech regulation in <phrase>emerging markets</phrase> using <phrase>case</phrase> studies from <phrase>China</phrase> and <phrase>South Africa</phrase>. The course pays special attention to the socioeconomic environment in <phrase>emerging markets</phrase>, as well as to <phrase>political risk</phrase> as a <phrase>major</phrase> source of uncertainty for fintech entrepreneurs. <phrase>Peer-to-peer lending</phrase> and remittances are used as leading examples for fintech <phrase>innovation</phrase> in <phrase>emerging markets</phrase>.
Este curso rápido sob demanda apresentará as funcionalidades de <phrase>Big Data</phrase> <phrase>e</phrase> <phrase>Machine Learning</phrase> do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform (GCP). Ele fornecerá <phrase>uma</phrase> visão geral rápida do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>e</phrase> mostrará <phrase>em</phrase> detalhes as funcionalidades do processamento de dados.   <phrase>Ao</phrase> final deste curso, você terá aprendido a: • Identificar o objetivo <phrase>e</phrase> o valor <phrase>dos</phrase> principais produtos de <phrase>Big Data</phrase> <phrase>e</phrase> <phrase>Machine Learning</phrase> no <phrase>Google</phrase> <phrase>Cloud</phrase> Platform • Usar o <phrase>Cloud</phrase> <phrase>SQL</phrase> <phrase>e</phrase> o <phrase>Cloud</phrase> Dataproc para migrar as atuais cargas de trabalho <phrase>MySQL</phrase> <phrase>e</phrase> <phrase>Hadoop</phrase>/<phrase>Pig</phrase>/Spark/Hive para o <phrase>Google</phrase> <phrase>Cloud</phrase> Platform • Usar o BigQuery <phrase>e</phrase> o <phrase>Cloud</phrase> Datalab para fazer análises de dados interativas • Escolher entre o <phrase>Cloud</phrase> <phrase>SQL</phrase>, o <phrase>BigTable</phrase> <phrase>e</phrase> o Datastore • Treinar <phrase>e</phrase> usar <phrase>uma</phrase> rede neural com o TensorFlow • Escolher entre diferentes produtos de processamento de dados no <phrase>Google</phrase> <phrase>Cloud</phrase> Platform  Para <phrase>se</phrase> inscrever neste curso, você deve ter aproximadamente um (1) ano de experiência <phrase>em</phrase> um ou mais destes itens: • <phrase>Uma</phrase> linguagem de consulta (como <phrase>SQL</phrase>) • Atividades de extração, transformação <phrase>e</phrase> carga • Modelagem de dados • <phrase>Machine learning</phrase> <phrase>e</phrase>/ou estatísticas • Programação <phrase>em</phrase> <phrase>Python</phrase>  Observações sobre a Conta do <phrase>Google</phrase>: • Para <phrase>se</phrase> inscrever <phrase>na</phrase> avaliação gratuita do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform, você precisa de <phrase>uma</phrase> Conta do <phrase>Google</phrase>/<phrase>Gmail</phrase>, além de um cartão de crédito ou <phrase>uma</phrase> conta bancária. <phrase>Os</phrase> serviços do <phrase>Google</phrase> estão temporariamente indisponíveis <phrase>na</phrase> <phrase>China</phrase>. • <phrase>Se</phrase> você for um cliente do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform com endereço de faturamento <phrase>na</phrase> União Europeia (UE) <phrase>e</phrase> <phrase>na</phrase> Rússia, <phrase>leia</phrase> a documentação de visão geral sobre o IVA <phrase>em</phrase>: https://cloud.google.com/billing/docs/resources/vat-overview. • Veja mais perguntas frequentes sobre a avaliação gratuita do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform no site: https://cloud.google.com/<phrase>free</phrase>-trial/.  Buscando <phrase>la</phrase> versión <phrase>en</phrase> español de este curso? Visita https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>es</phrase>/ このコースの日本語版をお探しですか？https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>jp</phrase>/
Данный курс был создан сотрудниками "Mail.Ru Group". При разработке заданий упор делался на знания и опыт, которые используются сотрудниками на практике ежедневно при проектировании продуктов, которыми пользуются миллионы людей. В современном мире невозможно представить человека, который, заходя в интернет, не пользуется поисковыми системами. <phrase>Google</phrase>, <phrase>Yandex</phrase>, Mail.ru и другие интернет-гиганты решают задачи нахождения информации в интернете и удовлетворения информационных потребностей пользователя. В этом курсе мы расскажем вам, как устроена поисковая система изнутри, покажем, какие приемы обработки естественного языка и машинного обучения используются при построении поискового индекса и ответе на запросы. Также мы обсудим тему объективной оценки качества поисковой системы. В результате слушатели курса смогут опробовать все вышеперечисленные техники на практике и построить работающую модель поисковой системы.
A <phrase>data</phrase> product is the <phrase>production</phrase> output from a <phrase>statistical analysis</phrase>. <phrase>Data</phrase> <phrase>products</phrase> automate <phrase>complex analysis</phrase> tasks or use <phrase>technology</phrase> to expand the utility of a <phrase>data</phrase> informed <phrase>model</phrase>, <phrase>algorithm</phrase> or inference. This course covers the basics of creating <phrase>data</phrase> <phrase>products</phrase> using Shiny, R packages, and interactive <phrase>graphics</phrase>. The course will focus on the statistical fundamentals of creating a <phrase>data</phrase> product that can be used to tell a story about <phrase>data</phrase> to a <phrase>mass</phrase> audience.
This course provides a deep dive into how to create a chatbot using Dialogflow, augment it with <phrase>Cloud</phrase> <phrase>Natural Language</phrase> <phrase>API</phrase>, and operationalize it using <phrase>Google</phrase> <phrase>Cloud</phrase> tools.  >>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
In this course, you'll get an in-depth look at the <phrase>SQL</phrase> SELECT statement and its main clauses. The course focuses on <phrase>big data</phrase> <phrase>SQL</phrase> engines <phrase>Apache</phrase> Hive and <phrase>Apache</phrase> Impala, but most of the <phrase>information</phrase> is applicable to <phrase>SQL</phrase> with traditional <phrase>RDBMs</phrase> as well; the instructor explicitly addresses differences for <phrase>MySQL</phrase> and <phrase>PostgreSQL</phrase>.  By the end of the course, you will be able to • explore and navigate <phrase>databases</phrase> and tables using different tools; • understand the basics of SELECT statements; • understand how and why to <phrase>filter</phrase> <phrase>results</phrase>; • explore grouping and <phrase>aggregation</phrase> to answer analytic questions; • work with <phrase>sorting</phrase> and limiting <phrase>results</phrase>; and  • combine multiple tables in different ways.  To use the hands-on environment for this course, you need to download and <phrase>install</phrase> a <phrase>virtual machine</phrase> and the <phrase>software</phrase> on which to <phrase>run</phrase> it. Before continuing, be sure that you have access to a <phrase>computer</phrase> that meets the following <phrase>hardware</phrase> and <phrase>software</phrase> requirements: • <phrase>Windows</phrase>, macOS, or <phrase>Linux</phrase> <phrase>operating system</phrase> (<phrase>iPads</phrase> and <phrase>Android</phrase> tablets will not work) • <phrase>64-bit</phrase> <phrase>operating system</phrase> (<phrase>32-bit</phrase> <phrase>operating systems</phrase> will not work) • 8 <phrase>GB</phrase> <phrase>RAM</phrase> or more • 25GB <phrase>free</phrase> <phrase>disk space</phrase> or more • <phrase>Intel</phrase> <phrase>VT</phrase>-x or <phrase>AMD</phrase>-V <phrase>virtualization</phrase> support <phrase>enabled</phrase> (on <phrase>Mac</phrase> <phrase>computers</phrase> with <phrase>Intel</phrase> processors, this is always <phrase>enabled</phrase>; on <phrase>Windows</phrase> and <phrase>Linux</phrase> <phrase>computers</phrase>, you might need to enable it in the <phrase>BIOS</phrase>) • For <phrase>Windows XP</phrase> <phrase>computers</phrase> only: You must have an unzip utility such as 7-<phrase>Zip</phrase> or <phrase>WinZip</phrase> installed (<phrase>Windows</phrase> XP’s built-in unzip utility will not work)
Welcome to the <phrase>art</phrase> and <phrase>science</phrase> of <phrase>machine learning</phrase>. In this <phrase>data science</phrase> course you will learn the essential skills of <phrase>ML</phrase> intuition, good judgment and experimentation to finely tune and optimize your <phrase>ML</phrase> models for the best performance.    In this course you will learn the many knobs and levers involved in training a <phrase>model</phrase>. You will first manually adjust them to see their effects on <phrase>model</phrase> performance. Once familiar with the knobs and levers, otherwise known as hyperparameters, you will learn how to tune them in an automatic way using <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform.
Обучение на размеченных данных или обучение с учителем – это наиболее распространенный класс задач машинного обучения. К нему относятся те задачи, где нужно научиться предсказывать некоторую величину для любого объекта, имея конечное число примеров. Это может быть предсказание уровня пробок на участке дороги, определение возраста пользователя по его действиям в интернете, предсказание цены, по которой будет куплена подержанная машина.  В этом курсе вы научитесь формулировать и, конечно, решать такие задачи. В центре нашего внимания будут успешно применяемые на практике алгоритмы классификации и регрессии: линейные модели, нейронные сети, решающие деревья и так далее. Особый акцент мы сделаем на такой мощной технике как построение композиций, которая позволяет существенно повысить качество отдельных алгоритмов и широко используется при решении прикладных задач. В частности, мы узнаем про случайные леса и про метод градиентного бустинга.  Построение предсказывающих алгоритмов — это лишь часть работы при решении задачи анализа данных. Мы разберемся и с другими этапами: оценивание обобщающей способности алгоритмов, подбор параметров модели, выбор и подсчет метрик качества.  Задания и видео курса разработаны на <phrase>Python</phrase> 2.
<phrase>Linear models</phrase>, as their name implies, relates an outcome to a set of predictors of interest using linear assumptions.  <phrase>Regression</phrase> models, a <phrase>subset</phrase> of <phrase>linear models</phrase>, are the most important <phrase>statistical analysis</phrase> tool in a <phrase>data</phrase> scientist’s <phrase>toolkit</phrase>. This course covers <phrase>regression analysis</phrase>, <phrase>least squares</phrase> and inference using <phrase>regression</phrase> models. <phrase>Special cases</phrase> of the <phrase>regression</phrase> <phrase>model</phrase>, <phrase>ANOVA</phrase> and ANCOVA will be <phrase>covered</phrase> as well. Analysis of residuals and variability will be investigated. The course will <phrase>cover</phrase> modern thinking on <phrase>model selection</phrase> and novel uses of <phrase>regression</phrase> models including scatterplot smoothing.
This capstone project course will give you a <phrase>taste</phrase> of what <phrase>data</phrase> scientists go through in <phrase>real life</phrase> when working with <phrase>data</phrase>.   You will learn about location <phrase>data</phrase> and different location <phrase>data</phrase> providers, such as <phrase>Foursquare</phrase>. You will learn how to make RESTful <phrase>API</phrase> calls to the <phrase>Foursquare</phrase> <phrase>API</phrase> to <phrase>retrieve data</phrase> about venues in different neighborhoods around the world. You will also learn how to be creative in situations where <phrase>data</phrase> are not readily available by scraping web <phrase>data</phrase> and <phrase>parsing</phrase> <phrase>HTML</phrase> code. You will utilize <phrase>Python</phrase> and its <phrase>pandas</phrase> <phrase>library</phrase> to manipulate <phrase>data</phrase>, which will help you refine your skills for exploring and <phrase>analyzing data</phrase>.   Finally, you will be required to use the Folium <phrase>library</phrase> to great maps of geospatial <phrase>data</phrase> and to communicate your <phrase>results</phrase> and findings.  If you choose to take this course and earn the <phrase>Coursera</phrase> course certificate, you will also earn an <phrase>IBM</phrase> <phrase>digital</phrase> badge upon successful completion of the course.    LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
Aprenda a desenvolver a estratégia de <phrase>marketing</phrase> <phrase>digital</phrase> para a sua empresa ou startup, nesse curso você irá aprender sobre <phrase>os</phrase> principais pontos do <phrase>Marketing</phrase> como ROI, SEO, SEM, Testes <phrase>AB</phrase> <phrase>e</phrase> como gerenciar o funil de conversão <phrase>e</phrase> também como usar plataformas como <phrase>Google adwords</phrase> <phrase>e</phrase> Analytics, <phrase>Facebook</phrase> Ads <phrase>e</phrase> <phrase>Email Marketing</phrase>. Esse curso é ministrado por profissionais do mercado que vão apresentar como aplicar cada um <phrase>dos</phrase> conceitos com aulas teóricas <phrase>e</phrase> práticas.  Neste curso serão abordados <phrase>os</phrase> seguintes temas: Diferença entre o <phrase>marketing</phrase> tradicional Importância do ROI <phrase>e</phrase> como calcular Introdução a funil <phrase>Google</phrase> Analytics <phrase>Google Adwords</phrase>  Tipos de canais Como funciona SEM Como funciona SEO Como funciona <phrase>Facebook</phrase> Ads Como funciona <phrase>Email Marketing</phrase> Configurando <phrase>e</phrase> planejando campanhas Testes <phrase>AB</phrase> Como escolher sua estratégia Metrificação <phrase>e</phrase> iteração  <phrase>Ao</phrase> final desse curso, esperamos que você esteja familiarizado com <phrase>os</phrase> principais conceitos, ferramentas <phrase>e</phrase> metodologias do <phrase>marketing</phrase> <phrase>digital</phrase>.  Não deixe de ver as perguntas frequentes antes de <phrase>se</phrase> inscrever  Conheça <phrase>os</phrase> nossos outros cursos: - Criação de Startups: Como desenvolver negócios inovadores        https://www.coursera.org/learn/criacao-startups   - <phrase>UX</phrase> / UI: Fundamentos para o <phrase>design</phrase> de interface        https://www.coursera.org/learn/<phrase>ux</phrase>-ui-<phrase>design</phrase>-de-interface  - Consolidando empresas: Estrutura jurídica <phrase>e</phrase> financeira        https://www.coursera.org/learn/consolidando-empresas - Inove <phrase>na</phrase> gestão de equipes <phrase>e</phrase> negócios: o crescimento <phrase>da</phrase> empresa        https://www.coursera.org/learn/gestao-equipes-negocios  - <phrase>Marketing</phrase> <phrase>e</phrase> vendas <phrase>B2B</phrase>: fechando novos negócios        https://www.coursera.org/learn/<phrase>marketing</phrase>-vendas-<phrase>b2b</phrase>
After completing this course, you will be able to solve any <phrase>data</phrase> <phrase>engineering</phrase> and <phrase>data science</phrase> problem using <phrase>Apache</phrase> Spark. With 30.000+ commits, 1000+ contributors, nearly 1.000.000 <phrase>lines of code</phrase> and 200+ <phrase>man</phrase> years of effort, <phrase>Apache</phrase> Spark is the most active <phrase>Apache Software Foundation</phrase> project and one of the largest <phrase>open source</phrase> projects ever. In <phrase>IBM</phrase> alone, 3500 researchers and developers are working with <phrase>Apache</phrase> Spark and <phrase>IBM</phrase> calls it "potentially the most significant <phrase>open source</phrase> project of the next decade."  You’ll <phrase>master</phrase> <phrase>Apache</phrase> Spark, for BigData but also for SmallData problems.  You’ll be able to go beyond <phrase>CPU</phrase>, <phrase>main memory</phrase> and storage limitations by making use of <phrase>large scale</phrase> compute clusters (<phrase>IBM</phrase> provides a <phrase>free</phrase> <phrase>Apache</phrase> Spark cluster for you during the course which you can continue to use afterwards, all <phrase>free</phrase> of charge).  You’ll understand how parallel code is written, capable of running on thousands of <phrase>CPUs</phrase>.   You’ll be able to use simple <phrase>SQL</phrase> statements on Petabytes of <phrase>data</phrase> using <phrase>Apache</phrase> SparkSQL and the <phrase>Apache</phrase> Spark DataFrame <phrase>API</phrase>.  You’ll be able to explain how <phrase>Tungsten</phrase> and <phrase>Catalyst</phrase> are transforming <phrase>SQL</phrase> queries into cost based optimized dynamic execution <phrase>graphs</phrase>. A significant advantage Spark has over other <phrase>state</phrase>-of-the-<phrase>art</phrase> frameworks like TensorFlow.  You’ll be able to apply <phrase>machine learning</phrase> <phrase>algorithms</phrase> on Petabytes of <phrase>data</phrase> using <phrase>Apache</phrase> SparkML Pipelines.  Join us to learn one of the de-facto standards in <phrase>data science</phrase>, successfully applied by companies like <phrase>Alibaba</phrase>, <phrase>Apple</phrase>, <phrase>Amazon</phrase>, <phrase>Baidu</phrase>, <phrase>eBay</phrase>, <phrase>IBM</phrase>, <phrase>NASA</phrase>, <phrase>Samsung</phrase>, <phrase>SAP</phrase>, <phrase>TripAdvisor</phrase>, <phrase>Yahoo</phrase>! and Zalando.   Prerequisites: - <phrase>basic</phrase> <phrase>python</phrase> <phrase>programming</phrase> - <phrase>basic</phrase> <phrase>machine learning</phrase> (optional introduction videos are provided in this course as well) - <phrase>basic</phrase> <phrase>SQL</phrase> skills for optional content   The following courses are recommended taking before taking this class (unless think you have the skills already)  https://www.coursera.org/learn/<phrase>python</phrase>-for-applied-<phrase>data-science</phrase> or similar https://www.coursera.org/learn/<phrase>machine-learning</phrase>-with-<phrase>python</phrase> or similar https://www.coursera.org/learn/<phrase>sql</phrase>-<phrase>data-science</phrase> or similar for optional lectures
Organisations all around the world are using <phrase>data</phrase> to predict behaviours and extract valuable <phrase>real-world</phrase> insights to inform decisions. Managing and analysing <phrase>big data</phrase> has become an essential part of modern <phrase>finance</phrase>, <phrase>retail</phrase>, <phrase>marketing</phrase>, <phrase>social science</phrase>, development and <phrase>research</phrase>, <phrase>medicine</phrase> and <phrase>government</phrase>.  This <phrase>MOOC</phrase>, designed by an <phrase>academic</phrase> team from <phrase>Goldsmiths</phrase>, <phrase>University</phrase> of <phrase>London</phrase>, will quickly <phrase>introduce</phrase> you to the <phrase>core concepts</phrase> of <phrase>Data Science</phrase> to prepare you for intermediate and advanced <phrase>Data Science</phrase> courses. It focuses on the <phrase>basic</phrase> <phrase>mathematics</phrase>, <phrase>statistics</phrase> and <phrase>programming</phrase> skills that are necessary for typical <phrase>data</phrase> analysis tasks.   You will consider these fundamental concepts on an example <phrase>data</phrase> clustering <phrase>task</phrase>, and you will use this example to learn <phrase>basic</phrase> <phrase>programming</phrase> skills that are necessary for <phrase>mastering</phrase> <phrase>Data Science</phrase> techniques. During the course, you will be asked to do a series of <phrase>mathematical</phrase> and <phrase>programming</phrase> exercises and a small <phrase>data</phrase> clustering project for a given dataset.
Learn to use the tools that are available from the <phrase>Galaxy</phrase> Project. This is the second course in the <phrase>Genomic</phrase> <phrase>Big Data</phrase> <phrase>Science</phrase> Specialization.
As <phrase>data</phrase> collection has increased exponentially, so has the need for people skilled at using and interacting with <phrase>data</phrase>; to be able to think critically, and provide insights to make better decisions and optimize their businesses. This is a <phrase>data</phrase> <phrase>scientist</phrase>, “part <phrase>mathematician</phrase>, part <phrase>computer</phrase> <phrase>scientist</phrase>, and part trend spotter” (<phrase>SAS</phrase> Institute, Inc.). According to Glassdoor, being a <phrase>data</phrase> <phrase>scientist</phrase> is the best job in <phrase>America</phrase>; with a median base salary of $110,000 and thousands of job openings at a time. The skills necessary to be a good <phrase>data</phrase> <phrase>scientist</phrase> include being able to retrieve and work with <phrase>data</phrase>, and to do that you need to be well versed in <phrase>SQL</phrase>, the <phrase>standard language</phrase> for communicating with <phrase>database</phrase> systems.  This course is designed to give you a primer in the fundamentals of <phrase>SQL</phrase> and working with <phrase>data</phrase> so that you can begin analyzing it for <phrase>data science</phrase> purposes. You will begin to ask the right questions and come up with good answers to deliver valuable insights for your <phrase>organization</phrase>. This course starts with the basics and assumes you do not have any <phrase>knowledge</phrase> or skills in <phrase>SQL</phrase>. It will build on that foundation and gradually have you write both simple and complex queries to help you select <phrase>data</phrase> from tables.  You'll start to work with different types of <phrase>data</phrase> like <phrase>strings</phrase> and numbers and discuss methods to <phrase>filter</phrase> and pare down your <phrase>results</phrase>.   You will create new tables and be able to move <phrase>data</phrase> into them. You will learn common operators and how to combine the <phrase>data</phrase>. You will <phrase>use case</phrase> statements and concepts like <phrase>data governance</phrase> and profiling. You will discuss topics on <phrase>data</phrase>, and practice using <phrase>real-world</phrase> <phrase>programming</phrase> assignments. You will interpret the structure, meaning, and relationships in source <phrase>data</phrase> and use <phrase>SQL</phrase> as a <phrase>professional</phrase> to shape your <phrase>data</phrase> for targeted analysis purposes.   Although we do not have any specific prerequisites or <phrase>software</phrase> requirements to take this course, a simple <phrase>text editor</phrase> is recommended for the final project. So what are you waiting for? This is your first <phrase>step</phrase> in landing a job in the best occupation in the US and soon the world!
Introduces to the commands that you need to manage and analyze directories, files, and large sets of <phrase>genomic</phrase> <phrase>data</phrase>. This is the fourth course in the <phrase>Genomic</phrase> <phrase>Big Data</phrase> <phrase>Science</phrase> Specialization from <phrase>Johns Hopkins University</phrase>.
Have you ever had the perfect <phrase>data science</phrase> experience? The <phrase>data</phrase> pull went perfectly. There were no merging errors or <phrase>missing data</phrase>. Hypotheses were clearly defined prior to analyses. <phrase>Randomization</phrase> was performed for the treatment of interest. The analytic plan was outlined prior to analysis and followed exactly. The conclusions were clear and actionable decisions were obvious. Has that every happened to you? Of course not. <phrase>Data analysis</phrase> in <phrase>real life</phrase> is messy. How does one manage a team facing real <phrase>data</phrase> analyses? In this one-week course, we contrast the ideal with what happens in <phrase>real life</phrase>. By contrasting the ideal, you will learn <phrase>key concepts</phrase> that will help you manage <phrase>real life</phrase> analyses.   This is a focused course designed to rapidly get you up to speed on doing <phrase>data science</phrase> in <phrase>real life</phrase>. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the <phrase>technical information</phrase> aside so that you can focus on managing your team and moving it <phrase>forward</phrase>.  After completing this course you will know how to:  1, Describe the “perfect” <phrase>data science</phrase> experience 2. Identify <phrase>strengths and weaknesses</phrase> in <phrase>experimental</phrase> designs 3. Describe possible pitfalls when pulling / assembling <phrase>data</phrase> and learn solutions for managing <phrase>data</phrase> pulls. 4. Challenge <phrase>statistical modeling</phrase> assumptions and drive <phrase>feedback</phrase> to <phrase>data</phrase> analysts 5. Describe common pitfalls in communicating <phrase>data</phrase> analyses 6. Get a glimpse into a day in the <phrase>life</phrase> of a <phrase>data</phrase> analysis <phrase>manager</phrase>.  The course will be taught at a conceptual level for active managers of <phrase>data</phrase> scientists and statisticians.  Some <phrase>key concepts</phrase> being discussed include: 1. <phrase>Experimental</phrase> <phrase>design</phrase>, <phrase>randomization</phrase>, <phrase>A/B testing</phrase> 2. Causal inference, counterfactuals,  3. Strategies for managing <phrase>data</phrase> quality. 4. Bias and <phrase>confounding</phrase> 5. Contrasting <phrase>machine learning</phrase> versus <phrase>classical</phrase> <phrase>statistical inference</phrase>  Course promo: https://<phrase>www.youtube.com/watch?v=</phrase>9BIYmw5wnBI  Course <phrase>cover</phrase> image by Jonathan Gross. <phrase>Creative Commons</phrase> BY-ND https://flic.kr/p/q1vudb
In this course you will learn how to create models for <phrase>decision making</phrase>. We will start with <phrase>cluster analysis</phrase>, a technique for <phrase>data</phrase> reduction that is very useful in <phrase>market segmentation</phrase>. You will then learn the basics of <phrase>Monte Carlo simulation</phrase> that will help you <phrase>model</phrase> the uncertainty that is prevalent in many <phrase>business</phrase> decisions. A key element of <phrase>decision making</phrase> is to identify the best course of <phrase>action</phrase>. Since businesses problems often have too many <phrase>alternative</phrase> solutions, you will learn how optimization can help you identify the best <phrase>option</phrase>. What is really exciting about this course is that you won’t need to know a <phrase>computer</phrase> <phrase>language</phrase> or advanced <phrase>statistics</phrase> to learn about these predictive and prescriptive analytic models. The Analytic Solver Platform and <phrase>basic</phrase> <phrase>knowledge</phrase> of <phrase>Excel</phrase> is all you’ll need. Learners participating in assignments will be able to get <phrase>free</phrase> access to the Analytic Solver Platform.
Не так давно получил распространение термин «большие данные», обозначивший новую прикладную область — поиск способов автоматического быстрого анализа огромных объёмов разнородной информации. Наука о больших данных ещё только оформляется, но уже сейчас она очень востребована — и в будущем будет востребована только больше. С её помощью можно решать невероятные задачи: оценивать состояние печени по кардиограмме, предсказывать зарплату по описанию вакансии, предлагать пользователю музыку на основании его анкеты в интернете.  Большими данными может оказаться что угодно: результаты научных экспериментов, логи банковских транзакций, метеорологические наблюдения, профили в социальных сетях — словом, всё, что может быть полезно проанализировать. Самым перспективным подходом к анализу больших данных считается применение машинного обучения — набора методов, благодаря которым компьютер может находить в массивах изначально неизвестные взаимосвязи и закономерности.  На факультете компьютерных наук ВШЭ и в Школе анализа данных есть люди, активно использующие машинное обучение и разрабатывающие новые подходы к нему. Именно они — преподаватели этого курса.  Вы изучите основные типы задач, решаемых с помощью машинного обучения — в основном речь пойдёт о классификации, регрессии и кластеризации. Узнаете об основных методах машинного обучения и их особенностях, научитесь оценивать качество моделей — и решать, подходит ли модель для решения конкретной задачи. Наконец, познакомитесь с современными библиотеками, в которых реализованы обсуждаемые модели и методы оценки их качества. Для работы мы будем использовать реальные данные из реальных задач.  Краткая программа курса: Неделя 1. Введение. Примеры задач. Логические методы: решающие деревья и решающие леса. Неделя 2. Метрические методы классификации. Линейные методы, стохастический градиент. Неделя 3. Метод опорных векторов (<phrase>SVM</phrase>). Логистическая регрессия. Метрики качества классификации. Неделя 4. Линейная регрессия. Понижение размерности, метод главных компонент. Неделя 5. Композиции алгоритмов, градиентный бустинг. Нейронные сети. Неделя 6. Кластеризация и визуализация. Частичное обучение. Неделя 7. Прикладные задачи анализа данных: постановки и методы решения.  Слушателю нужно знать об основных понятиях математики: функциях, производных, векторах, матрицах. Для выполнения практических заданий потребуются базовые навыки программирования. Очень желательно знать <phrase>Python</phrase>. Задания рассчитаны на использование этого языка и его библиотек <phrase>numpy</phrase>, <phrase>pandas</phrase> и <phrase>scikit-learn</phrase>.  Чтобы успешно завершить курс, нужно набрать проходную сумму баллов за тесты и практические задания, а также выполнить финальный проект, посвящённый решению прикладной задачи анализа данных.  Мы уверены, что этот курс будет полезен каждому, кто хочет постичь искусство предсказательного моделирования и освоить интеллектуальный анализ данных.  Появились технические трудности? Обращайтесь на адрес: <phrase>coursera</phrase>@hse.ru
The goal of this course is to give learners <phrase>basic</phrase> understanding of modern <phrase>neural networks</phrase> and their applications in <phrase>computer vision</phrase> and <phrase>natural language understanding</phrase>. The course starts with a recap of <phrase>linear models</phrase> and discussion of <phrase>stochastic optimization</phrase> methods that are crucial for training <phrase>deep neural networks</phrase>. Learners will study all popular <phrase>building blocks</phrase> of <phrase>neural networks</phrase> including fully connected layers, convolutional and recurrent layers.  Learners will use these <phrase>building blocks</phrase> to define complex modern architectures in TensorFlow and Keras frameworks. In the course project learner will implement <phrase>deep neural network</phrase> for the <phrase>task</phrase> of image captioning which solves the problem of giving a text description for an input image.  The prerequisites for this course are:  1) <phrase>Basic</phrase> <phrase>knowledge</phrase> of <phrase>Python</phrase>. 2) <phrase>Basic</phrase> <phrase>linear algebra</phrase> and <phrase>probability</phrase>.  Please note that this is an advanced course and we assume <phrase>basic</phrase> <phrase>knowledge</phrase> of <phrase>machine learning</phrase>. You should understand: 1) <phrase>Linear regression</phrase>: mean <phrase>squared error</phrase>, analytical <phrase>solution</phrase>. 2) <phrase>Logistic regression</phrase>: <phrase>model</phrase>, <phrase>cross-entropy</phrase> loss, class <phrase>probability</phrase> estimation. 3) <phrase>Gradient</phrase> descent for <phrase>linear models</phrase>. Derivatives of MSE and <phrase>cross-entropy</phrase> loss functions. 4) The problem of <phrase>overfitting</phrase>. 5) Regularization for <phrase>linear models</phrase>.  Do you have technical problems? Write to us: <phrase>coursera</phrase>@hse.ru
One of the most common tasks performed by <phrase>data</phrase> scientists and <phrase>data</phrase> analysts are prediction and <phrase>machine learning</phrase>. This course will <phrase>cover</phrase> the <phrase>basic</phrase> components of building and applying prediction functions with an emphasis on <phrase>practical applications</phrase>. The course will provide <phrase>basic</phrase> grounding in concepts such as training and <phrase>tests</phrase> sets, <phrase>overfitting</phrase>, and <phrase>error rates</phrase>. The course will also <phrase>introduce</phrase> a <phrase>range</phrase> of <phrase>model</phrase> based and algorithmic <phrase>machine learning</phrase> methods including <phrase>regression</phrase>, classification <phrase>trees</phrase>, <phrase>Naive Bayes</phrase>, and random <phrase>forests</phrase>. The course will <phrase>cover</phrase> the complete process of building prediction functions including <phrase>data</phrase> collection, feature creation, <phrase>algorithms</phrase>, and evaluation.
In this course, you will get hands-on <phrase>instruction</phrase> of advanced <phrase>Excel</phrase> 2013 functions.  You’ll learn to use PowerPivot to build <phrase>databases</phrase> and <phrase>data</phrase> models.  We’ll show you how to perform different types of scenario and <phrase>simulation</phrase> analysis and you’ll have an opportunity to practice these skills by leveraging some of Excel's built in tools including, solver, <phrase>data</phrase> tables, scenario <phrase>manager</phrase> and goal seek.  In the second half of the course, will <phrase>cover</phrase> how to visualize <phrase>data</phrase>, tell a story and explore <phrase>data</phrase> by reviewing core principles of <phrase>data visualization</phrase> and dashboarding.  You’ll use <phrase>Excel</phrase> to build complex <phrase>graphs</phrase> and Power View reports and then start to combine them into dynamic dashboards.  Note: Learners will need PowerPivot to complete some of the exercises. Please use MS <phrase>Excel</phrase> 2013 version. If you have other MS <phrase>Excel</phrase> versions or a <phrase>MAC</phrase> you might not be able to complete all assignments.  This course was created by <phrase>PricewaterhouseCoopers</phrase> LLP with an <phrase>address</phrase> at 300 <phrase>Madison</phrase> Avenue, <phrase>New York</phrase>, <phrase>New York</phrase>, 10017.
This course is designed to provide you with an understanding of the role of <phrase>data</phrase> and <phrase>technology</phrase> in <phrase>human capital</phrase> <phrase>management</phrase>. Every topic in the course will be <phrase>covered</phrase> in the most practical way so that learners get hands-on experience. In the course we use the 4Ts principle: <phrase>Task</phrase>, Theory, Technique and <phrase>Technology</phrase> so that there is always a connection to <phrase>organizational performance</phrase> objectives, an overview of underlying theories and principles, and specific tools which help achieve <phrase>business</phrase> objectives.  You will learn  ●	what combination of <phrase>data</phrase>, technologies, and tools can be used in people <phrase>management</phrase> processes to improve organization’s performance ●	how to use some of these tools and how to select the ones that suit your objectives and budget ●	to <phrase>design</phrase> individual and team development plans and measure its ROI for the <phrase>organization</phrase> ●	how to figure out the qualities that <phrase>lead</phrase> employees to their best performance so you know what to encourage in current and look for in new employees ●	how to identify the right channels to recruit your employees or <phrase>team members</phrase> ●	what combination of monetary and non-monetary <phrase>motivation</phrase> tools work best for your <phrase>organization</phrase>  ●	how to predict what people will leave in the near future and how to make sure some of them stay ●	how to measure engagement and make a strong <phrase>organizational culture</phrase> <phrase>improve performance</phrase>
<phrase>Data science</phrase> is a <phrase>team sport</phrase>. As a <phrase>data science</phrase> executive it is your job to recruit, organize, and manage the team to success. In this one-week course, we will <phrase>cover</phrase> how you can find the right people to fill out your <phrase>data science</phrase> team, how to organize them to give them the best chance to feel empowered and successful, and how to manage your team as it grows.   This is a focused course designed to rapidly get you up to speed on the process of building and managing a <phrase>data science</phrase> team. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the <phrase>technical information</phrase> aside so that you can focus on managing your team and moving it <phrase>forward</phrase>.  After completing this course you will know.  1. The different roles in the <phrase>data science</phrase> team including <phrase>data</phrase> <phrase>scientist</phrase> and <phrase>data</phrase> <phrase>engineer</phrase> 2. How the <phrase>data science</phrase> team relates to other teams in an <phrase>organization</phrase> 3. What are the expected qualifications of different <phrase>data science</phrase> <phrase>team members</phrase> 4. Relevant questions for interviewing <phrase>data</phrase> scientists 5. How to manage the <phrase>onboarding</phrase> process for the team 6. How to guide <phrase>data science</phrase> teams to success 7. How to encourage and empower <phrase>data science</phrase> teams  Commitment: 1 week of study, 4-6 hours  Course <phrase>cover</phrase> image by JaredZammit. <phrase>Creative Commons</phrase> BY-SA. https://flic.kr/p/5vuWZz
The ability to understand and apply <phrase>Business</phrase> <phrase>Statistics</phrase> is becoming <phrase>increasingly important</phrase> in the <phrase>industry</phrase>. A good understanding of <phrase>Business</phrase> <phrase>Statistics</phrase> is a requirement to make correct and relevant interpretations of <phrase>data</phrase>. Lack of <phrase>knowledge</phrase> could <phrase>lead</phrase> to erroneous decisions which could potentially have negative consequences for a <phrase>firm</phrase>. This course is designed to <phrase>introduce</phrase> you to <phrase>Business</phrase> <phrase>Statistics</phrase>. We begin with the notion of <phrase>descriptive statistics</phrase>, which is summarizing <phrase>data</phrase> using a few numbers. Different categories of descriptive measures are introduced and discussed along with the <phrase>Excel</phrase> functions to calculate them. The notion of <phrase>probability</phrase> or uncertainty is introduced along with the concept of a sample and <phrase>population</phrase> <phrase>data</phrase> using relevant <phrase>business</phrase> examples. This leads us to various statistical <phrase>distributions</phrase> along with their <phrase>Excel</phrase> functions which are then used to <phrase>model</phrase> or approximate <phrase>business</phrase> processes. You get to apply these descriptive measures of <phrase>data</phrase> and various statistical <phrase>distributions</phrase> using easy-to-follow <phrase>Excel</phrase> based examples which are demonstrated throughout the course.  To successfully complete course assignments, students must have access to <phrase>Microsoft Excel</phrase>.  <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 1 Module 1: <phrase>Basic</phrase> <phrase>Data</phrase> Descriptors In this module you will get to understand, calculate and interpret various descriptive or <phrase>summary</phrase> measures of <phrase>data</phrase>. These descriptive measures summarize and present <phrase>data</phrase> using a few numbers. Appropriate <phrase>Excel</phrase> functions to do these calculations are introduced and demonstrated.  Topics <phrase>covered</phrase> include: •	Categories of descriptive <phrase>data</phrase> •	Measures of <phrase>central tendency</phrase>, the mean, median, mode, and their interpretations and calculations •	Measures of spread-in-<phrase>data</phrase>, the <phrase>range</phrase>, <phrase>interquartile-range</phrase>, <phrase>standard deviation</phrase> and <phrase>variance</phrase> •	Box plots •	Interpreting the <phrase>standard deviation</phrase> measure using the <phrase>rule-of-thumb</phrase> and Chebyshev’s theorem <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 2 Module 2: Descriptive Measures of Association, <phrase>Probability</phrase>, and Statistical <phrase>Distributions</phrase> This module presents the <phrase>covariance</phrase> and correlation measures and their respective <phrase>Excel</phrase> functions. You get to understand the notion of causation versus correlation. The module then introduces the notion of <phrase>probability</phrase> and <phrase>random variables</phrase> and starts introducing statistical <phrase>distributions</phrase>.  Topics <phrase>covered</phrase> include: •	Measures of association, the <phrase>covariance</phrase> and correlation measures; causation versus correlation •	<phrase>Probability</phrase> and <phrase>random variables</phrase>; discrete versus continuous <phrase>data</phrase> •	Introduction to statistical <phrase>distributions</phrase> <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 3 Module 3: The <phrase>Normal Distribution</phrase> This module introduces the <phrase>Normal distribution</phrase> and the <phrase>Excel</phrase> <phrase>function</phrase> to calculate <phrase>probabilities</phrase> and various outcomes from the distribution.   Topics <phrase>covered</phrase> include: •	<phrase>Probability density function</phrase> and <phrase>area</phrase> under the curve as a measure of <phrase>probability</phrase> •	The <phrase>Normal distribution</phrase> (<phrase>bell curve</phrase>), NORM.DIST, NORM.INV functions in <phrase>Excel</phrase> <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 4 Module 4: Working with <phrase>Distributions</phrase>, Normal, Binomial, <phrase>Poisson</phrase> In this module, you'll see various applications of the <phrase>Normal distribution</phrase>. You will also get introduced to the Binomial and <phrase>Poisson</phrase> <phrase>distributions</phrase>. The <phrase>Central Limit Theorem</phrase> is introduced and explained in the <phrase>context</phrase> of understanding sample <phrase>data</phrase> versus <phrase>population</phrase> <phrase>data</phrase> and the link between the two.  Topics <phrase>covered</phrase> include: •	Various applications of the <phrase>Normal distribution</phrase> •	The Binomial and <phrase>Poisson</phrase> <phrase>distributions</phrase> •	Sample versus <phrase>population</phrase> <phrase>data</phrase>; the <phrase>Central Limit Theorem</phrase>
<phrase>Computer</phrase> Vision is one of the most exciting fields in <phrase>Machine Learning</phrase> and <phrase>AI</phrase>. It has applications in many industries such as <phrase>self-driving cars</phrase>, <phrase>robotics</phrase>, <phrase>augmented reality</phrase>, <phrase>face detection</phrase> in <phrase>law</phrase> enforcement agencies. In this beginner-<phrase>friendly</phrase> course you will understand about <phrase>computer vision</phrase>, and will learn about its various applications across many industries.  As part of this course you will utilize <phrase>Python</phrase>, Watson <phrase>AI</phrase>, and <phrase>OpenCV</phrase> to process images and interact with <phrase>image classification</phrase> models. You will also build, <phrase>train</phrase>, and <phrase>test</phrase> your own custom image classifiers.   This is a hands-on course and involves several labs and exercises. All the labs will be performed on the <phrase>Cloud</phrase> and you will be provided access to a <phrase>Cloud</phrase> environment completely <phrase>free</phrase> of charge. At the end of the course, you will create your own <phrase>computer vision</phrase> web app and deploy it to the <phrase>Cloud</phrase>.  This course does not require any prior <phrase>Machine Learning</phrase> or <phrase>Computer</phrase> Vision experience, however some <phrase>knowledge</phrase> of <phrase>Python programming language</phrase> is necessary.
Covering the tools and techniques of both multivariate and geographical analysis, this course provides hands-on experience visualizing <phrase>data</phrase> that represents <phrase>multiple variables</phrase>. This course will use <phrase>statistical techniques</phrase> and <phrase>software</phrase> to develop and analyze geographical <phrase>knowledge</phrase>.
Important note: The second assignment in this course covers the topic of <phrase>Graph</phrase> Analysis in the <phrase>Cloud</phrase>, in which you will use <phrase>Elastic</phrase> <phrase>MapReduce</phrase> and the <phrase>Pig</phrase> <phrase>language</phrase> to perform <phrase>graph</phrase> analysis over a moderately large dataset, about 600GB. In <phrase>order</phrase> to complete this assignment, you will need to make use of <phrase>Amazon Web Services</phrase> (<phrase>AWS</phrase>). <phrase>Amazon</phrase> has generously offered to provide up to $50 in <phrase>free</phrase> <phrase>AWS</phrase> credit to each learner in this course to allow you to complete the assignment. Further details regarding the process of receiving this credit are available in the welcome message for the course, as well as in the assignment itself. Please note that <phrase>Amazon</phrase>, <phrase>University</phrase> of <phrase>Washington</phrase>, and <phrase>Coursera</phrase> cannot reimburse you for any charges if you exhaust your credit.  While we believe that this assignment contributes an excellent learning experience in this course, we understand that some learners may be unable or unwilling to use <phrase>AWS</phrase>. We are unable to issue Course Certificates for learners who do not complete the assignment that requires use of <phrase>AWS</phrase>. As such, you should not pay for a Course Certificate in Communicating <phrase>Data</phrase> <phrase>Results</phrase> if you are unable or unwilling to use <phrase>AWS</phrase>, as you will not be able to successfully complete the course without doing so.  Making predictions is not enough!  Effective <phrase>data</phrase> scientists know how to explain and interpret their <phrase>results</phrase>, and communicate findings accurately to stakeholders to inform <phrase>business</phrase> decisions.  Visualization is the field of <phrase>research</phrase> in <phrase>computer science</phrase> that studies effective <phrase>communication</phrase> of quantitative <phrase>results</phrase> by linking <phrase>perception</phrase>, <phrase>cognition</phrase>, and <phrase>algorithms</phrase> to exploit the enormous bandwidth of the <phrase>human</phrase> <phrase>visual cortex</phrase>.  In this course you will learn to recognize, <phrase>design</phrase>, and use effective visualizations.  Just because you can make a prediction and convince others to <phrase>act</phrase> on it doesn’t mean you should.  In this course you will explore the <phrase>ethical</phrase> considerations around <phrase>big data</phrase> and how these considerations are beginning to influence policy and practice.   You will learn the foundational limitations of using <phrase>technology</phrase> to protect <phrase>privacy</phrase> and the codes of conduct emerging to guide the behavior of <phrase>data</phrase> scientists.  You will also learn the importance of <phrase>reproducibility</phrase> in <phrase>data science</phrase> and how the commercial <phrase>cloud</phrase> can help support reproducible <phrase>research</phrase> even for experiments involving massive datasets, complex computational infrastructures, or both.  Learning Goals: After completing this course, you will be able to: 1. <phrase>Design</phrase> and critique visualizations 2. Explain the <phrase>state</phrase>-of-the-<phrase>art</phrase> in <phrase>privacy</phrase>, <phrase>ethics</phrase>, governance around <phrase>big data</phrase> and <phrase>data science</phrase> 3. Use <phrase>cloud computing</phrase> to analyze <phrase>large datasets</phrase> in a reproducible way.
Learners will create a roadmap to achieve their own personal goals related to the <phrase>digital</phrase> <phrase>manufacturing</phrase> and <phrase>design</phrase> (DM&D) profession, which will help them leverage relevant opportunities. The culminating project provides a tangible element to include in their <phrase>professional</phrase> portfolios that showcases their <phrase>knowledge</phrase> of <phrase>Industry</phrase> 4.0.  This project is part of the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0. To learn more about the specialization and its courses, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
Welcome to the Capstone Project for <phrase>Big Data</phrase>! In this culminating project, you will build a <phrase>big data</phrase> <phrase>ecosystem</phrase> using tools and methods form the earlier courses in this specialization. You will analyze a <phrase>data</phrase> set simulating <phrase>big data</phrase> generated from a large <phrase>number of users</phrase> who are playing our imaginary <phrase>game</phrase> "Catch the <phrase>Pink</phrase> <phrase>Flamingo</phrase>". During the five week Capstone Project, you will walk through the typical <phrase>big data</phrase> <phrase>science</phrase> steps for acquiring, exploring, preparing, analyzing, and reporting. In the first two weeks, we will <phrase>introduce</phrase> you to the <phrase>data set</phrase> and guide you through some <phrase>exploratory analysis</phrase> using tools such as <phrase>Splunk</phrase> and Open Office. Then we will move into more challenging <phrase>big data</phrase> problems requiring the more advanced tools you have learned including KNIME, Spark's MLLib and Gephi. Finally, during the fifth and final week, we will show you how to bring it all together to create engaging and compelling reports and <phrase>slide</phrase> presentations. As a result of our collaboration with <phrase>Splunk</phrase>, a <phrase>software</phrase> <phrase>company</phrase> focus on analyzing machine-generated <phrase>big data</phrase>, learners with the top projects will be eligible to present to <phrase>Splunk</phrase> and meet <phrase>Splunk</phrase> recruiters and <phrase>engineering</phrase> <phrase>leadership</phrase>.
Курс “Методология научных исследований и котики” знакомит с основами исследовательской деятельности. В нем описан полный цикл проведения научных исследований - от постановки научной проблемы до публикации статьи.   Вы узнаете, чем отличаются научные исследования от ненаучных и познакомитесь с базовыми типами исследований и основами статистической обработки данных.   Курс позволит вам научиться: a. Ставить научную проблему и гипотезу,  б. Искать и анализировать статьи в научных базах данных,  в. Проектировать дизайн исследования,  г. Обрабатывать и грамотно интерпретировать полученные данные,  д. Описывать результаты исследования в статьях и научных докладах.   Помимо этого, в курсе показаны некоторые тонкости и практические приемы, значительно облегчающие работу исследователю. Каждая глава курса подкрепляется забавными и полезными практическими заданиями, а итоговым заданием курса будет написание почти настоящей научной статьи.   Главная фишка курса - все основные идеи объясняются на котиках. Поэтому курс получился доступным для понимания максимально широкой аудитории.  Курс будет полезен студентам естественно-научных и социо-гуманитарных специальностей (биологам, социологам, психологам, медикам), начинающим исследователям и аналитикам .
Note: You should complete all the other courses in this Specialization before beginning this course.  This six-week <phrase>long</phrase> Project course of the <phrase>Data Mining</phrase> Specialization will allow you to apply the learned <phrase>algorithms</phrase> and techniques for <phrase>data mining</phrase> from the previous courses in the Specialization, including Pattern Discovery, Clustering, <phrase>Text Retrieval</phrase>, <phrase>Text Mining</phrase>, and Visualization, to solve interesting <phrase>real-world</phrase> <phrase>data mining</phrase> challenges. Specifically, you will work on a <phrase>restaurant</phrase> review <phrase>data</phrase> set from <phrase>Yelp</phrase> and use all the <phrase>knowledge</phrase> and skills you’ve learned from the previous courses to mine this <phrase>data</phrase> set to discover interesting and useful <phrase>knowledge</phrase>. The <phrase>design</phrase> of the Project emphasizes: 1) simulating the <phrase>workflow</phrase> of a <phrase>data</phrase> miner in a real job setting; 2) integrating different <phrase>mining</phrase> techniques <phrase>covered</phrase> in multiple individual courses; 3) experimenting with different ways to solve a problem to deepen your understanding of techniques; and 4) allowing you to propose and explore your own ideas creatively.   The goal of the Project is to analyze and mine a large <phrase>Yelp</phrase> review <phrase>data</phrase> set to discover useful <phrase>knowledge</phrase> to help people make decisions in dining. The project will include the following outputs:   1. Opinion visualization: explore and visualize the review content to understand what people have said in those reviews. 2. Cuisine map <phrase>construction</phrase>: mine the <phrase>data</phrase> set to understand the <phrase>landscape</phrase> of different types of cuisines and their similarities. 3. Discovery of popular dishes for a cuisine: mine the <phrase>data</phrase> set to discover the common/popular dishes of a particular cuisine. 4. Recommendation of <phrase>restaurants</phrase> to help people decide where to dine: mine the <phrase>data</phrase> set to rank <phrase>restaurants</phrase> for a specific dish and predict the <phrase>hygiene</phrase> <phrase>condition</phrase> of a <phrase>restaurant</phrase>.  From the perspective of users, a cuisine map can help them understand what cuisines are there and see the big picture of all kinds of cuisines and their relations. Once they decide what cuisine to try, they would be interested in knowing what the popular dishes of that cuisine are and decide what dishes to have. Finally, they will need to choose a <phrase>restaurant</phrase>. Thus, recommending <phrase>restaurants</phrase> based on a particular dish would be useful. Moreover, predicting the <phrase>hygiene</phrase> <phrase>condition</phrase> of a <phrase>restaurant</phrase> would also be helpful.   By working on these tasks, you will gain experience with a typical <phrase>workflow</phrase> in <phrase>data mining</phrase> that includes <phrase>data</phrase> preprocessing, <phrase>data</phrase> exploration, <phrase>data</phrase> analysis, improvement of analysis methods, and presentation of <phrase>results</phrase>. You will have an opportunity to combine multiple <phrase>algorithms</phrase> from different courses to complete a relatively complicated <phrase>mining</phrase> <phrase>task</phrase> and experiment with different ways to solve a problem to understand the best way to solve it. We will suggest specific approaches, but you are highly encouraged to explore your own ideas since open exploration is, by <phrase>design</phrase>, a goal of the Project.   You are required to submit a brief <phrase>report</phrase> for each of the tasks for peer grading. A final consolidated <phrase>report</phrase> is also required, which will be peer-graded.
How can innovators understand if their idea is worth developing and pursuing? In this course, we lay out a systematic process to make strategic decisions about innovative product or services that will help entrepreneurs, managers and innovators to avoid common pitfalls. We teach students to assess the feasibility of an innovative idea through problem-framing techniques and rigorous <phrase>data</phrase> analysis labelled ‘a scientific approach’. The course is highly interactive and includes exercises and <phrase>real-world</phrase> applications. We will also show the implications of a scientific approach to <phrase>innovation management</phrase> through a wide <phrase>range</phrase> of examples and <phrase>case</phrase> studies.
¿Qué <phrase>es</phrase> <phrase>el</phrase> aprendizaje automático? ¿Qué tipos de problemas puede solucionar? <phrase>En</phrase> <phrase>Google</phrase>, tenemos una perspectiva ligeramente distinta sobre <phrase>el</phrase> aprendizaje automático: no <phrase>se</phrase> trata solo de los datos, <phrase>sino</phrase> también <phrase>de la</phrase> lógica. Hablamos de por qué un marco de este tipo <phrase>es</phrase> útil cuando pensamos <phrase>en</phrase> <phrase>la</phrase> creación de una canalización de modelos de aprendizaje automático. Luego, analizamos cinco fases para convertir un posible caso práctico <phrase>en</phrase> un recurso que pueda aprovechar <phrase>la</phrase> tecnología de aprendizaje automático y por qué <phrase>es</phrase> importante no saltarse fases. Finalizamos con un reconocimiento de los sesgos que puede amplificar <phrase>el</phrase> aprendizaje automático y cómo reconocerlos.
The analytical process does not end with models than can predict with accuracy or prescribe the best <phrase>solution</phrase> to <phrase>business</phrase> problems. Developing these models and gaining insights from <phrase>data</phrase> do not necessarily <phrase>lead</phrase> to successful implementations. This depends on the ability to communicate <phrase>results</phrase> to those who make decisions. Presenting findings to <phrase>decision makers</phrase> who are not familiar with the <phrase>language</phrase> of analytics presents a challenge. In this course you will learn how to communicate analytics <phrase>results</phrase> to  stakeholders who do not understand the details of analytics but want evidence of analysis and <phrase>data</phrase>. You will be able to choose the right vehicles to present quantitative <phrase>information</phrase>, including those based on principles of <phrase>data visualization</phrase>. You will also learn how to develop and deliver <phrase>data</phrase>-analytics stories that provide <phrase>context</phrase>, insight, and interpretation.
本コースは 1 週間の集中セミナーであり、<phrase>Data</phrase> <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform 専門分野認定の各コースをベースにしています。動画講義、デモ、ハンズオンラボを通して、コンピューティング クラスタを作成、管理して <phrase>Google</phrase> <phrase>Cloud</phrase> Platform で <phrase>Hadoop</phrase>、Spark、<phrase>Pig</phrase>、Hive のジョブを実行する方法を学びます。また、コンピューティング クラスタからクラウド ストレージのオプションにアクセスする方法や、<phrase>Google</phrase> の機械学習機能を分析プログラムに統合する方法も学びます。    ハンズオンラボでは、ウェブ コンソールと <phrase>CLI</phrase> を使って Dataproc クラスタを作成して管理し、クラスタを使って Spark と <phrase>Pig</phrase> のジョブを実行します。さらに、BigQuery およびストレージと統合し、Spark を利用する Jupyter Notebook も作成します。最後に、機械学習 <phrase>API</phrase> をデータ分析に統合します。   受講条件 • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> & <phrase>Machine Learning</phrase> Fundamentals を受講済み（または同等の経験） • <phrase>Python</phrase> に関する一定程度の知識
This course will expose learners to additional tools that can be used to perform <phrase>Data</phrase> Visualization. In particular, the courses focuses on Tableau, a <phrase>state</phrase>-of-the-<phrase>art</phrase> visualization package. In this course, the visualization concepts from previous courses are <phrase>reinforced</phrase> and the Tableau <phrase>software</phrase> is introduced through replication of the visualizations built in previous courses.
Welcome to the specialization course of <phrase>NoSQL</phrase> Systems.  This course will be completed on six weeks, it will be supported with videos and exercises that will allow you to identify the <phrase>differences between</phrase> the <phrase>relational</phrase> and <phrase>NoSQL</phrase> <phrase>databases</phrase>.  As part of these <phrase>alternative</phrase> technologies the <phrase>student</phrase> will learn the main characteristics and how to implement the typical <phrase>NoSQL</phrase> <phrase>databases</phrase>, such as Key-value, columnar, document and <phrase>graph</phrase>.  Let's start!  After completing this course, a learner will be able to ●	Identify what type of <phrase>NoSQL</phrase> <phrase>database</phrase> to implement based on <phrase>business</phrase> requirements (key-value, document, full text, <phrase>graph</phrase>, etc.) ●	Apply <phrase>NoSQL</phrase> <phrase>data</phrase> modeling from <phrase>application</phrase> specific queries ●	Use Atomic Aggregates and denormalization as <phrase>data</phrase> modelling techniques to optimize <phrase>query processing</phrase>  <phrase>Software</phrase> to download: <phrase>MongoDB</phrase> Neo4j SAPIQ <phrase>Cassandra</phrase>  In <phrase>case</phrase> you have a <phrase>Mac</phrase> / <phrase>IOS</phrase> <phrase>operating system</phrase> you will need to use a <phrase>virtual Machine</phrase> (<phrase>VirtualBox</phrase>, <phrase>Vmware</phrase>).
This course will <phrase>cover</phrase> the steps used in weighting sample surveys, including methods for adjusting for nonresponse and using <phrase>data</phrase> external to the survey for calibration.  Among the techniques discussed are adjustments using estimated response propensities, poststratification, raking, and <phrase>general</phrase> <phrase>regression</phrase> estimation.  <phrase>Alternative</phrase> techniques for imputing values for missing items will be discussed.  For both weighting and imputation, the capabilities of different statistical <phrase>software</phrase> packages will be <phrase>covered</phrase>, including R®, <phrase>Stata</phrase>®, and <phrase>SAS</phrase>®.
The capstone project will be an analysis using R that answers a specific scientific/<phrase>business</phrase> question provided by the course team. A large and complex dataset will be provided to learners and the analysis will require the <phrase>application</phrase> of a <phrase>variety</phrase> of methods and techniques introduced in the previous courses, including <phrase>exploratory data analysis</phrase> through <phrase>data visualization</phrase> and numerical summaries, <phrase>statistical inference</phrase>, and modeling as well as interpretations of these <phrase>results</phrase> in the <phrase>context</phrase> of the <phrase>data</phrase> and the <phrase>research</phrase> question. The analysis will implement both frequentist and <phrase>Bayesian</phrase> techniques and discuss in <phrase>context</phrase> of the <phrase>data</phrase> how these two approaches are similar and different, and what these differences mean for conclusions that can be drawn from the <phrase>data</phrase>.   A <phrase>sampling</phrase> of the final projects will be featured on the <phrase>Duke</phrase> Statistical <phrase>Science</phrase> <phrase>department</phrase> <phrase>website</phrase>.  Note: Only learners who have passed the four previous courses in the specialization are eligible to take the Capstone.
<phrase>Recent years</phrase> have seen a dramatic growth of <phrase>natural language</phrase> text <phrase>data</phrase>, including <phrase>web pages</phrase>, <phrase>news</phrase> articles, <phrase>scientific literature</phrase>, emails, enterprise documents, and <phrase>social media</phrase> such as <phrase>blog</phrase> articles, forum posts, product reviews, and <phrase>tweets</phrase>. Text <phrase>data</phrase> are unique in that they are usually generated directly by humans rather than a <phrase>computer</phrase> system or sensors, and are thus especially valuable for discovering <phrase>knowledge</phrase> about people’s opinions and preferences, in addition to many other kinds of <phrase>knowledge</phrase> that we encode in text.   This course will <phrase>cover</phrase> <phrase>search engine</phrase> technologies, which <phrase>play</phrase> an <phrase>important role</phrase> in any <phrase>data mining</phrase> applications involving text <phrase>data</phrase> for two reasons. First, while the <phrase>raw data</phrase> may be large for any particular problem, it is often a relatively small <phrase>subset</phrase> of the <phrase>data</phrase> that are relevant, and a <phrase>search engine</phrase> is an essential tool for quickly discovering a small <phrase>subset</phrase> of relevant text <phrase>data</phrase> in a large text collection. Second, <phrase>search engines</phrase> are needed to help analysts interpret any patterns discovered in the <phrase>data</phrase> by allowing them to examine the relevant original text <phrase>data</phrase> to make sense of any discovered pattern. You will learn the <phrase>basic</phrase> concepts, principles, and the <phrase>major</phrase> techniques in <phrase>text retrieval</phrase>, which is the underlying <phrase>science</phrase> of <phrase>search engines</phrase>.
Влияет ли знание методов анализа данных на уровень заработной платы? Работает ли система оценки кредитоспособности клиентов банка? Действительно ли новый баннер лучше старого? Чтобы ответить на такие вопросы, нужно собрать данные. Данные почти всегда содержат шум, поэтому утверждения, которые можно сделать на их основе, верны не всегда, а только с определённой вероятностью. Строить наиболее корректные выводы и численно оценивать степень уверенности в них помогают методы статистики.   Как можно оценивать неизвестные параметры системы по небольшому количеству наблюдений? Как измерить точность таких оценок? Какие данные нужны, чтобы ответить на ваш вопрос, и на какие вопросы можно ответить с помощью уже имеющихся данных? Вы узнаете все, что нужно для успешного превращения данных в выводы — организация экспериментов, A/B-тестирование, универсальные методы оценки параметров и проверки гипотез, корреляции и причинно-следственные связи.  Задания и видео курса разработаны на <phrase>Python</phrase> 2.
The course aims at helping students to be able to solve practical <phrase>ML</phrase>-amenable problems that they may encounter in <phrase>real life</phrase> that include: (1) understanding where the problem one faces lands on a <phrase>general</phrase> <phrase>landscape</phrase> of available <phrase>ML</phrase> methods, (2) understanding which particular <phrase>ML</phrase> approach(<phrase>es</phrase>) would be most appropriate for resolving the problem, and (3) ability to successfully implement a <phrase>solution</phrase>, and assess its performance.   A learner with some or no previous <phrase>knowledge</phrase> of <phrase>Machine Learning</phrase> (<phrase>ML</phrase>)  will get to know main <phrase>algorithms</phrase> of Supervised and <phrase>Unsupervised Learning</phrase>, and <phrase>Reinforcement Learning</phrase>, and will be able to use <phrase>ML</phrase> <phrase>open source</phrase> <phrase>Python</phrase> packages to <phrase>design</phrase>, <phrase>test</phrase>, and implement <phrase>ML</phrase> <phrase>algorithms</phrase> in <phrase>Finance</phrase>. Fundamentals of <phrase>Machine Learning</phrase> in <phrase>Finance</phrase> will provide more at-depth view of supervised, unsupervised, and <phrase>reinforcement learning</phrase>, and end up in a project on using <phrase>unsupervised learning</phrase> for implementing a simple portfolio trading strategy.  The course is designed for three categories of students: Practitioners working at <phrase>financial institutions</phrase> such as <phrase>banks</phrase>, <phrase>asset management</phrase> firms or <phrase>hedge funds</phrase> Individuals interested in applications of <phrase>ML</phrase> for personal day trading Current full-time students pursuing a <phrase>degree</phrase> in <phrase>Finance</phrase>, <phrase>Statistics</phrase>, <phrase>Computer Science</phrase>, <phrase>Mathematics</phrase>, <phrase>Physics</phrase>, <phrase>Engineering</phrase> or other <phrase>related disciplines</phrase> who want to learn about <phrase>practical applications</phrase> of <phrase>ML</phrase> in <phrase>Finance</phrase>    Experience with <phrase>Python</phrase> (including <phrase>numpy</phrase>, <phrase>pandas</phrase>, and IPython/Jupyter notebooks), <phrase>linear algebra</phrase>, <phrase>basic</phrase> <phrase>probability theory</phrase> and <phrase>basic</phrase> <phrase>calculus</phrase> is necessary to complete assignments in this course.
Inferential <phrase>statistics</phrase> are concerned with making inferences based on relations found in the sample, to relations in the <phrase>population</phrase>. Inferential <phrase>statistics</phrase> help us decide, for example, whether the <phrase>differences between</phrase> groups that we see in our <phrase>data</phrase> are strong enough to provide support for our <phrase>hypothesis</phrase> that group differences exist in <phrase>general</phrase>, in the entire <phrase>population</phrase>.  We will start by considering the <phrase>basic</phrase> principles of significance testing: the <phrase>sampling</phrase> and <phrase>test statistic</phrase> distribution, <phrase>p-value</phrase>, significance level, power and type I and <phrase>type II</phrase> errors. Then we will consider a large number of statistical <phrase>tests</phrase> and techniques that help us make inferences for different types of <phrase>data</phrase> and different types of <phrase>research</phrase> designs. For each individual statistical <phrase>test</phrase> we will consider how it works, for what <phrase>data</phrase> and <phrase>design</phrase> it is appropriate and how <phrase>results</phrase> should be interpreted. You will also learn how to perform these <phrase>tests</phrase> using <phrase>freely available</phrase> <phrase>software</phrase>.   For those who are already familiar with statistical testing: We will look at z-<phrase>tests</phrase> for 1 and 2 proportions,  McNemar's <phrase>test</phrase> for dependent proportions, t-<phrase>tests</phrase> for 1 mean (paired differences) and 2 means, the <phrase>Chi</phrase>-square <phrase>test</phrase> for <phrase>independence</phrase>, Fisher’s exact <phrase>test</phrase>, simple <phrase>regression</phrase> (linear and exponential) and multiple <phrase>regression</phrase> (linear and logistic), one way and <phrase>factorial</phrase> <phrase>analysis of variance</phrase>, and non-parametric <phrase>tests</phrase> (Wilcoxon, <phrase>Kruskal</phrase>-Wallis, sign <phrase>test</phrase>,  signed-rank <phrase>test</phrase>, <phrase>runs</phrase> <phrase>test</phrase>).
In the first course of this specialization, we will recap what was <phrase>covered</phrase> in the  <phrase>Machine Learning</phrase> with TensorFlow on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform Specialization (https://www.coursera.org/specializations/<phrase>machine-learning</phrase>-tensorflow-gcp).  One of the best ways to review something is to work with the concepts and technologies that you have learned.  So, this course is set up as a workshop and in this workshop, you will do <phrase>End-to-End</phrase> <phrase>Machine Learning</phrase> with TensorFlow on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform  Prerequisites: <phrase>Basic</phrase> <phrase>SQL</phrase>, familiarity with <phrase>Python</phrase> and TensorFlow  >>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
This intermediate-level course introduces the <phrase>mathematical</phrase> foundations to derive <phrase>Principal Component Analysis</phrase> (<phrase>PCA</phrase>), a fundamental <phrase>dimensionality reduction</phrase> technique. We'll <phrase>cover</phrase> some <phrase>basic</phrase> <phrase>statistics</phrase> of <phrase>data</phrase> sets, such as mean values and variances, we'll compute distances and angles between vectors using inner <phrase>products</phrase> and derive orthogonal projections of <phrase>data</phrase> onto <phrase>lower</phrase>-dimensional subspaces. Using all these tools, we'll then derive <phrase>PCA</phrase> as a <phrase>method</phrase> that minimizes the <phrase>average</phrase> squared <phrase>reconstruction</phrase> error between <phrase>data</phrase> points and their <phrase>reconstruction</phrase>.  At the end of this course, you'll be familiar with important <phrase>mathematical</phrase> concepts and you can implement <phrase>PCA</phrase> all by yourself. If you’re struggling, you'll find a set of jupyter notebooks that will allow you to explore properties of the techniques and walk you through what you need to do to get on <phrase>track</phrase>. If you are already an expert, this course may refresh some of your <phrase>knowledge</phrase>.  The lectures, examples and exercises require: 1. Some ability of abstract thinking 2. Good background in <phrase>linear algebra</phrase> (e.g., matrix and <phrase>vector</phrase> <phrase>algebra</phrase>, <phrase>linear independence</phrase>, basis) 3. <phrase>Basic</phrase> background in multivariate <phrase>calculus</phrase> (e.g., <phrase>partial derivatives</phrase>, <phrase>basic</phrase> optimization) 4. <phrase>Basic</phrase> <phrase>knowledge</phrase> in <phrase>python</phrase> <phrase>programming</phrase> and <phrase>numpy</phrase>  <phrase>Disclaimer</phrase>: This course is substantially more abstract and requires more <phrase>programming</phrase> than the other two courses of the specialization. However, this type of abstract thinking, <phrase>algebraic</phrase> manipulation and <phrase>programming</phrase> is necessary if you want to understand and develop <phrase>machine learning</phrase> <phrase>algorithms</phrase>.
The use of <phrase>Excel</phrase> is widespread in the <phrase>industry</phrase>. It is a very powerful <phrase>data analysis</phrase> tool and almost all big and <phrase>small businesses</phrase> use <phrase>Excel</phrase> in their day to day functioning. This is an introductory course in the use of <phrase>Excel</phrase> and is designed to give you a working <phrase>knowledge</phrase> of <phrase>Excel</phrase> with the aim of getting to use it for more advance topics in <phrase>Business</phrase> <phrase>Statistics</phrase> later. The course is designed keeping in mind two kinds of learners -  those who have very little functional <phrase>knowledge</phrase> of <phrase>Excel</phrase> and those who use <phrase>Excel</phrase> regularly but at a peripheral level and wish to enhance their skills. The course takes you from <phrase>basic</phrase> operations such as <phrase>reading</phrase> <phrase>data</phrase> into <phrase>excel</phrase> using various <phrase>data</phrase> formats, organizing and manipulating <phrase>data</phrase>, to some of the more advanced functionality of <phrase>Excel</phrase>. All along, <phrase>Excel</phrase> functionality is introduced using <phrase>easy to understand</phrase> examples which are demonstrated in a way that learners can become comfortable in understanding and applying them.  To successfully complete course assignments, students must have access to a <phrase>Windows</phrase> version of <phrase>Microsoft Excel</phrase> 2010 or later.  <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 1 Module 1: Introduction to <phrase>Spreadsheets</phrase> In this module, you will be introduced to the use of <phrase>Excel</phrase> <phrase>spreadsheets</phrase> and various <phrase>basic</phrase> <phrase>data</phrase> functions of <phrase>Excel</phrase>.  Topics <phrase>covered</phrase> include: •	<phrase>Reading</phrase> <phrase>data</phrase> into <phrase>Excel</phrase> using various formats •	<phrase>Basic</phrase> functions in <phrase>Excel</phrase>, <phrase>arithmetic</phrase> as well as various logical functions •	Formatting <phrase>rows and columns</phrase> •	Using formulas in <phrase>Excel</phrase> and their copy and paste using absolute and relative referencing <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 2 Module 2: <phrase>Spreadsheet</phrase> Functions to Organize <phrase>Data</phrase> This module introduces various <phrase>Excel</phrase> functions to organize and query <phrase>data</phrase>. Learners are introduced to the IF, nested IF, VLOOKUP and the HLOOKUP functions of <phrase>Excel</phrase>.   Topics <phrase>covered</phrase> include: •	IF and the nested IF functions •	VLOOKUP and HLOOKUP •	The RANDBETWEEN <phrase>function</phrase> <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 3 Module 3: Introduction to Filtering, Pivot Tables, and Charts This module introduces various <phrase>data</phrase> filtering capabilities of <phrase>Excel</phrase>. You’ll learn how to set filters in <phrase>data</phrase> to selectively access <phrase>data</phrase>. A very powerful <phrase>data</phrase> summarizing tool, the Pivot Table, is also explained and we begin to <phrase>introduce</phrase> the charting feature of <phrase>Excel</phrase>.  Topics <phrase>covered</phrase> include: •	VLOOKUP across worksheets •	<phrase>Data</phrase> filtering in <phrase>Excel</phrase> •	Use of Pivot tables with categorical as well as numerical <phrase>data</phrase> •	Introduction to the charting capability of <phrase>Excel</phrase> <phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase><phrase>___</phrase>_ WEEK 4 Module 4: Advanced Graphing and Charting This module explores various advanced graphing and charting techniques available in <phrase>Excel</phrase>. Starting with various line, bar and pie charts we <phrase>introduce</phrase> pivot charts, scatter plots and histograms. You will get to understand these various charts and get to build them on your own.  Topics <phrase>covered</phrase> include •	Line, Bar and Pie charts •	Pivot charts •	Scatter plots •	Histograms
In this course, you will learn <phrase>best practices</phrase> for how to use <phrase>data</phrase> analytics to make any <phrase>company</phrase> more competitive and more profitable. You will be able to recognize the most critical <phrase>business</phrase> metrics and distinguish them from mere <phrase>data</phrase>.   You’ll get a clear picture of the vital but different roles <phrase>business</phrase> analysts, <phrase>business</phrase> <phrase>data</phrase> analysts, and <phrase>data</phrase> scientists each <phrase>play</phrase> in various types of companies. And you’ll know exactly what skills are required to be hired for, and succeed at, these <phrase>high</phrase>-demand jobs.   Finally, you will be able to use a checklist provided in the course to score any <phrase>company</phrase> on how effectively it is embracing <phrase>big data</phrase> <phrase>culture</phrase>. <phrase>Digital</phrase> companies like <phrase>Amazon</phrase>, Uber and <phrase>Airbnb</phrase> are transforming entire industries through their creative use of <phrase>big data</phrase>. You’ll understand why these companies are so disruptive and how they use <phrase>data</phrase>-analytics techniques to out-compete traditional companies.
機械学習の歴史を皮切りに、ニューラル ネットワークがさまざまな問題でうまく機能している理由をご紹介します。次に、教師あり学習の問題を設定し、勾配降下法を使用して適切な解決策を見つける方法について説明します。これには、一般化が可能になるデータセットの作成も含まれます。実験に対応するため、データセットを繰り返し作成できる方法について解説します。  コースの目標: ディープ ラーニングが注目を集めている理由を知る 損失関数とパフォーマンス指標を使用して、モデルを最適化および評価する 機械学習で発生しがちな一般的な問題を軽減する 再現可能なスケーラブル トレーニング用、評価用、テスト用データセットを作成する
This course will continue the introduction to <phrase>Python</phrase> <phrase>programming</phrase> that started with <phrase>Python</phrase> <phrase>Programming</phrase> Essentials.  We'll learn about different <phrase>data</phrase> representations, including <phrase>strings</phrase>, lists, and tuples, that form the core of all <phrase>Python</phrase> programs.  We will also teach you how to access files, which will allow you to store and <phrase>retrieve data</phrase> within your programs. These concepts and skills will help you to manipulate <phrase>data</phrase> and write more complex <phrase>Python</phrase> programs.  By the end of the course, you will be able to write <phrase>Python</phrase> programs that can manipulate <phrase>data</phrase> stored in files.  This will extend your <phrase>Python</phrase> <phrase>programming</phrase> <phrase>expertise</phrase>, enabling you to write a wide <phrase>range</phrase> of scripts using <phrase>Python</phrase>  This course uses <phrase>Python</phrase> 3.  While most <phrase>Python</phrase> programs continue to use <phrase>Python</phrase> 2, <phrase>Python</phrase> 3 is the future of the <phrase>Python programming language</phrase>. This course introduces <phrase>basic</phrase> desktop <phrase>Python</phrase> <phrase>development environments</phrase>, allowing you to <phrase>run</phrase> <phrase>Python</phrase> programs directly on your <phrase>computer</phrase>. This choice enables a smooth <phrase>transition</phrase> from online <phrase>development environments</phrase>.
This course introduces students to the <phrase>science</phrase> of <phrase>business</phrase> analytics while casting a keen eye toward the artful use of numbers found in the <phrase>digital</phrase> space. The goal is to provide businesses and managers with the foundation needed to apply <phrase>data</phrase> analytics to <phrase>real-world</phrase> challenges they confront <phrase>daily</phrase> in their <phrase>professional</phrase> lives. Students will learn to identify the ideal analytic tool for their specific needs; understand valid and reliable ways to collect, analyze, and visualize <phrase>data</phrase>; and utilize <phrase>data</phrase> in <phrase>decision making</phrase> for their agencies, organizations or clients.
This course will teach you how to build models for <phrase>natural language</phrase>, audio, and other <phrase>sequence</phrase> <phrase>data</phrase>. Thanks to <phrase>deep learning</phrase>, <phrase>sequence</phrase> <phrase>algorithms</phrase> are working far better than just two <phrase>years ago</phrase>, and this is enabling numerous exciting applications in <phrase>speech recognition</phrase>, <phrase>music</phrase> <phrase>synthesis</phrase>, chatbots, <phrase>machine translation</phrase>, <phrase>natural language understanding</phrase>, and many others.   You will: - Understand how to build and <phrase>train</phrase> <phrase>Recurrent Neural</phrase> Networks (RNNs), and commonly-used variants such as GRUs and LSTMs. - Be able to apply <phrase>sequence</phrase> models to <phrase>natural language</phrase> problems, including text <phrase>synthesis</phrase>.  - Be able to apply <phrase>sequence</phrase> models to audio applications, including <phrase>speech recognition</phrase> and <phrase>music</phrase> <phrase>synthesis</phrase>.  This is the fifth and final course of the <phrase>Deep Learning</phrase> Specialization.  deeplearning.ai is also partnering with the <phrase>NVIDIA</phrase> <phrase>Deep Learning</phrase> Institute (DLI) in Course 5, <phrase>Sequence</phrase> Models, to provide a <phrase>programming</phrase> assignment on <phrase>Machine Translation</phrase> with <phrase>deep learning</phrase>. You will have the opportunity to build a <phrase>deep learning</phrase> project with <phrase>cutting-edge</phrase>, <phrase>industry</phrase>-relevant content.
<phrase>Regression Analysis</phrase> is perhaps the <phrase>single</phrase> most important <phrase>Business</phrase> <phrase>Statistics</phrase> tool used in the <phrase>industry</phrase>. <phrase>Regression</phrase> is the <phrase>engine</phrase> behind a multitude of <phrase>data</phrase> analytics applications used for many forms of forecasting and prediction.   This is the fourth course in the specialization, "<phrase>Business</phrase> <phrase>Statistics</phrase> and Analysis". The course  introduces you to the very important tool known as <phrase>Linear Regression</phrase>. You will learn to apply various procedures such as dummy <phrase>variable</phrase> regressions, transforming variables, and interaction effects. All these are introduced and explained using <phrase>easy to understand</phrase> examples in <phrase>Microsoft Excel</phrase>. The focus of the course is on understanding and <phrase>application</phrase>, rather than detailed <phrase>mathematical</phrase> derivations. Note: This course uses the ‘<phrase>Data</phrase> Analysis’ tool box which is standard with the <phrase>Windows</phrase> version of <phrase>Microsoft Excel</phrase>. It is also standard with the 2016 or later <phrase>Mac</phrase> version of <phrase>Excel</phrase>. However, it is not standard with <phrase>earlier versions</phrase> of <phrase>Excel</phrase> for <phrase>Mac</phrase>.    WEEK 1 Module 1: <phrase>Regression Analysis</phrase>: <phrase>An Introduction</phrase> In this module you will get introduced to the <phrase>Linear Regression</phrase> <phrase>Model</phrase>. We will build a <phrase>regression</phrase> <phrase>model</phrase> and estimate it using <phrase>Excel</phrase>. We will use the estimated <phrase>model</phrase> to infer relationships between various variables and use the <phrase>model</phrase> to make predictions. The module also introduces the notion of errors, residuals and R-square in a <phrase>regression</phrase> <phrase>model</phrase>.  Topics <phrase>covered</phrase> include: •	Introducing the <phrase>Linear Regression</phrase> •	Building a <phrase>Regression</phrase> <phrase>Model</phrase> and estimating it using <phrase>Excel</phrase> •	Making inferences using the estimated <phrase>model</phrase> •	Using the <phrase>Regression</phrase> <phrase>model</phrase> to make predictions •	Errors, Residuals and R-square    WEEK 2 Module 2: <phrase>Regression Analysis</phrase>: <phrase>Hypothesis</phrase> Testing and <phrase>Goodness of Fit</phrase> This module presents different <phrase>hypothesis</phrase> <phrase>tests</phrase> you could do using the <phrase>Regression</phrase> output. These <phrase>tests</phrase> are an important part of inference and the module introduces them using <phrase>Excel</phrase> based examples. The p-values are introduced along with <phrase>goodness of fit</phrase> measures R-square and the adjusted R-square. Towards the end of module we <phrase>introduce</phrase> the ‘Dummy <phrase>variable</phrase> <phrase>regression</phrase>’ which is used to incorporate categorical variables in a <phrase>regression</phrase>.   Topics <phrase>covered</phrase> include: •	<phrase>Hypothesis testing</phrase> in a <phrase>Linear Regression</phrase> •	‘<phrase>Goodness of Fit</phrase>’ measures (R-square, adjusted R-square) •	Dummy <phrase>variable</phrase> <phrase>Regression</phrase> (using Categorical variables in a <phrase>Regression</phrase>)    WEEK 3 Module 3: <phrase>Regression Analysis</phrase>: Dummy Variables, Multicollinearity This module continues with the <phrase>application</phrase> of Dummy <phrase>variable</phrase> <phrase>Regression</phrase>. You get to understand the interpretation of <phrase>Regression</phrase> output in the presence of categorical variables. Examples are worked out to re-inforce various concepts introduced. The module also explains what is Multicollinearity and how to deal with it.   Topics <phrase>covered</phrase> include: •	Dummy <phrase>variable</phrase> <phrase>Regression</phrase> (using Categorical variables in a <phrase>Regression</phrase>) •	Interpretation of coefficients and p-values in the presence of Dummy variables •	Multicollinearity in <phrase>Regression</phrase> Models    WEEK 4 Module 4: <phrase>Regression Analysis</phrase>: Various Extensions The module extends your understanding of the <phrase>Linear Regression</phrase>, introducing techniques such as mean-centering of variables and building confidence bounds for predictions using the <phrase>Regression</phrase> <phrase>model</phrase>. A powerful <phrase>regression</phrase> extension known as ‘Interaction variables’ is introduced and explained using examples. We also study the transformation of variables in a <phrase>regression</phrase> and in that <phrase>context</phrase> <phrase>introduce</phrase> the <phrase>log-log</phrase> and the semi-log <phrase>regression</phrase> models.   Topics <phrase>covered</phrase> include: •	Mean centering of variables in a <phrase>Regression</phrase> <phrase>model</phrase> •	Building confidence bounds for predictions using a <phrase>Regression</phrase> <phrase>model</phrase> •	Interaction effects in a <phrase>Regression</phrase> •	Transformation of variables •	The <phrase>log-log</phrase> and semi-log <phrase>regression</phrase> models
There is a significant number of tasks when we need not just to process an enormous volume of <phrase>data</phrase> but to process it as quickly as possible. Delays in <phrase>tsunami</phrase> prediction can cost people’s lives. Delays in traffic jam prediction cost extra time. Advertisements based on the recent users’ <phrase>activity</phrase> are ten times more popular.  However, <phrase>stream processing</phrase> techniques alone are not enough to create a complete <phrase>real-time</phrase> system. For example to create a recommendation system we need to have a storage that allows to store and fetch <phrase>data</phrase> for a user with minimal latency. These <phrase>databases</phrase> should be able to store hundreds of terabytes of <phrase>data</phrase>, <phrase>handle</phrase> billions of requests per day and have a 100% <phrase>uptime</phrase>. <phrase>NoSQL</phrase> <phrase>databases</phrase> are commonly used to solve this challenging problem.  After you finish this course, you will <phrase>master</phrase> <phrase>stream processing</phrase> systems and <phrase>NoSQL</phrase> <phrase>databases</phrase>. You will also learn how to use such popular and powerful systems as  <phrase>Kafka</phrase>, <phrase>Cassandra</phrase> and <phrase>Redis</phrase>.  To get the most out of this course, you need to know <phrase>Hadoop</phrase> and <phrase>SQL</phrase>. You should also have a working <phrase>knowledge</phrase> of bash, <phrase>Python</phrase> and Spark.  Do you want to learn how to build <phrase>Big Data</phrase> applications that can withstand modern challenges? Jump right in!
By now you have definitely heard about <phrase>data science</phrase> and <phrase>big data</phrase>. In this one-week class, we will provide a crash course in what these terms mean and how they <phrase>play</phrase> a role in successful organizations. This class is for anyone who wants to learn what all the <phrase>data science</phrase> <phrase>action</phrase> is about, including those who will eventually need to manage <phrase>data</phrase> scientists. The goal is to get you up to speed as quickly as possible on <phrase>data science</phrase> without all the fluff. We've designed this course to be as convenient as possible without sacrificing any of the essentials.  This is a focused course designed to rapidly get you up to speed on the field of <phrase>data science</phrase>. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the <phrase>technical information</phrase> aside so that you can focus on managing your team and moving it <phrase>forward</phrase>.  After completing this course you will know.   1. How to describe the role <phrase>data science</phrase> plays in various contexts 2. How <phrase>statistics</phrase>, <phrase>machine learning</phrase>, and <phrase>software engineering</phrase> <phrase>play</phrase> a role in <phrase>data science</phrase> 3. How to describe the structure of a <phrase>data science</phrase> project 4. Know the key terms and tools used by <phrase>data</phrase> scientists 5. How to identify a successful and an unsuccessful <phrase>data science</phrase> project 3. The role of a <phrase>data science</phrase> <phrase>manager</phrase>   Course <phrase>cover</phrase> image by r2hox. <phrase>Creative Commons</phrase> BY-SA: https://flic.kr/p/gdMuhT
This course covers a wide <phrase>range</phrase> of tasks in <phrase>Natural Language</phrase> Processing from <phrase>basic</phrase> to advanced: <phrase>sentiment analysis</phrase>, summarization, dialogue <phrase>state</phrase> <phrase>tracking</phrase>, to name a few. Upon completing, you will be able to recognize <phrase>NLP</phrase> tasks in your day-to-day work, propose approaches, and <phrase>judge</phrase> what techniques are likely to work well.  The final project is devoted to one of the most hot topics in today’s <phrase>NLP</phrase>. You will build your own conversational chat-bot that will assist with search on StackOverflow <phrase>website</phrase>. The project will be based on practical assignments of the course, that will give you hands-on experience with such tasks as text classification, <phrase>named entities</phrase> recognition, and duplicates detection.   Throughout the lectures, we will aim at finding a balance between traditional and <phrase>deep learning</phrase> techniques in <phrase>NLP</phrase> and <phrase>cover</phrase> them in parallel. For example, we will discuss <phrase>word alignment</phrase> models in <phrase>machine translation</phrase> and see how similar it is to attention mechanism in encoder-<phrase>decoder</phrase> <phrase>neural networks</phrase>. Core techniques are not treated as <phrase>black</phrase> boxes. On the contrary, you will get in-depth understanding of what’s happening inside. To succeed in that, we expect your familiarity with the basics of <phrase>linear algebra</phrase> and <phrase>probability theory</phrase>, <phrase>machine learning</phrase> setup, and <phrase>deep neural networks</phrase>. Some materials are based on one-month-old papers and <phrase>introduce</phrase> you to the very <phrase>state</phrase>-of-the-<phrase>art</phrase> in <phrase>NLP</phrase> <phrase>research</phrase>.  Do you have technical problems? Write to us: <phrase>coursera</phrase>@hse.ru
<phrase>Data science</phrase> courses contain <phrase>math</phrase>—no avoiding that! This course is designed to teach learners the <phrase>basic</phrase> <phrase>math</phrase> you will need in <phrase>order</phrase> to be successful in almost any <phrase>data science</phrase> <phrase>math</phrase> course and was created for learners who have <phrase>basic</phrase> <phrase>math</phrase> skills but may not have taken <phrase>algebra</phrase> or pre-<phrase>calculus</phrase>. <phrase>Data Science</phrase> <phrase>Math</phrase> Skills introduces the core <phrase>math</phrase> that <phrase>data science</phrase> is built upon, with no extra complexity, introducing unfamiliar ideas and <phrase>math</phrase> symbols one-at-a-time.   Learners who complete this course will <phrase>master</phrase> the <phrase>vocabulary</phrase>, <phrase>notation</phrase>, concepts, and <phrase>algebra</phrase> rules that all <phrase>data</phrase> scientists must know before moving on to more advanced material.  Topics include: ~<phrase>Set theory</phrase>, including Venn diagrams ~Properties of the <phrase>real number</phrase> line ~Interval <phrase>notation</phrase> and <phrase>algebra</phrase> with inequalities ~Uses for summation and Sigma <phrase>notation</phrase> ~<phrase>Math</phrase> on the <phrase>Cartesian</phrase> (x,y) plane, slope and distance formulas ~Graphing and describing functions and their inverses on the x-y plane, ~The concept of instantaneous <phrase>rate of change</phrase> and <phrase>tangent</phrase> lines to a curve ~Exponents, <phrase>logarithms</phrase>, and the natural log <phrase>function</phrase>. ~<phrase>Probability theory</phrase>, including <phrase>Bayes</phrase>’ theorem.  While this course is intended as a <phrase>general</phrase> introduction to the <phrase>math</phrase> skills needed for <phrase>data science</phrase>, it can be considered a prerequisite for learners interested in the course, "<phrase>Mastering</phrase> <phrase>Data</phrase> Analysis in <phrase>Excel</phrase>," which is part of the <phrase>Excel</phrase> to <phrase>MySQL</phrase> <phrase>Data Science</phrase> Specialization.  Learners who <phrase>master</phrase> <phrase>Data Science</phrase> <phrase>Math</phrase> Skills will be fully prepared for success with the more advanced <phrase>math</phrase> concepts introduced in "<phrase>Mastering</phrase> <phrase>Data</phrase> Analysis in <phrase>Excel</phrase>."   Good luck and we hope you enjoy the course!
<phrase>Spreadsheet</phrase> <phrase>software</phrase> remains one of the most ubiquitous pieces of <phrase>software</phrase> used in workplaces around the world. Learning to confidently operate this <phrase>software</phrase> means adding a highly valuable asset to your employability portfolio. Across the globe, millions of job advertisements requiring <phrase>Excel</phrase> skills are posted every day. At a time when <phrase>digital</phrase> skills jobs are growing much faster than non-<phrase>digital</phrase> jobs, completing this course will position you ahead of others, so keep <phrase>reading</phrase>.  In this last course of our Specialization <phrase>Excel</phrase> Skills for <phrase>Business</phrase> you will build on the strong foundations of the first three courses: Essentials, Intermediate I + <phrase>II</phrase>.  In the Advanced course, we will prepare you to become a power user of <phrase>Excel</phrase> - this is your last <phrase>step</phrase> before specializing at a <phrase>professional</phrase> level. The topics we have prepared will challenge you as you learn how to use advanced formula techniques and sophisticated lookups. You will clean and prepare <phrase>data</phrase> for analysis, and learn how to work with dates and financial functions. An in-depth look at <phrase>spreadsheet</phrase> <phrase>design</phrase> and documentation will prepare you for our big finale, where you will learn how to build <phrase>professional</phrase> dashboards in <phrase>Excel</phrase>.
This course is designed to impact the way you think about transforming <phrase>data</phrase> into better decisions. Recent extraordinary improvements in <phrase>data</phrase>-collecting technologies have changed the way firms make informed and effective <phrase>business</phrase> decisions. The course on operations analytics, taught by three of Wharton’s leading experts, focuses on how the <phrase>data</phrase> can be used to profitably match supply with demand in various <phrase>business</phrase> settings. In this course, you will learn how to <phrase>model</phrase> future demand uncertainties, how to predict the outcomes of competing policy choices and how to choose the best course of <phrase>action</phrase> in the face of <phrase>risk</phrase>. The course will <phrase>introduce</phrase> frameworks and ideas that provide insights into a <phrase>spectrum</phrase> of <phrase>real-world</phrase> <phrase>business</phrase> challenges, will teach you methods and <phrase>software</phrase> available for tackling these challenges quantitatively as well as the <phrase>issues involved</phrase> in gathering the relevant <phrase>data</phrase>.  This course is appropriate for beginners and <phrase>business</phrase> professionals with no <phrase>prior analytics</phrase> experience.
Mediante este curso serás capaz de identificar, evaluar y aprovechar las distintas oportunidades que <phrase>te</phrase> presenta <phrase>el</phrase> análisis de datos para generar valor <phrase>en</phrase> <phrase>tu</phrase> organización. Al finalizarlo, habrás desarrollado <phrase>la</phrase> capacidad de utilizar métodos de análisis de datos, así como herramientas y modelos de apoyo que soportarán <phrase>tu</phrase> toma de <phrase>decisiones</phrase> a nivel personal y <phrase>en</phrase> <phrase>tu</phrase> organización.  <phrase>En</phrase> <phrase>la</phrase> parte inicial de este curso conocerás <phrase>la</phrase> importancia y rol que juega <phrase>el</phrase> análisis de datos (Analytics) <phrase>en</phrase> <phrase>el mundo</phrase> actual, para apoyar <phrase>la</phrase> toma de <phrase>decisiones</phrase> <phrase>en</phrase> nuestra vida diaria, nuestro trabajo y nuestra organización. Conocerás no solo <phrase>el</phrase> enorme crecimiento que <phrase>ha</phrase> tenido esta área <phrase>en</phrase> los últimos años, <phrase>sino</phrase> también <phrase>el</phrase> gran potencial y oportunidades que <phrase>te</phrase> presenta <phrase>en</phrase> <phrase>el</phrase> futuro inmediato. Conocerás y analizarás diversos casos de éxito tanto a nivel mundial, como <phrase>en</phrase> países de habla hispana.  De igual forma, habrás desarrollado <phrase>la</phrase> capacidad de utilizar distintos métodos que <phrase>te</phrase> apoyarán <phrase>en</phrase> <phrase>tu</phrase> análisis de datos y proceso de toma de <phrase>decisiones</phrase>, utilizando además herramientas computacionales para explorar bases de datos, realizar predicciones y crear infografías asociadas al problema o decisión de <phrase>tu</phrase> interés. Además, identificarás elementos clave para <phrase>la</phrase> modelación matemática, que serán de gran utilidad para identificar <phrase>el</phrase> mejor curso de acción que deberás seguir.  Finalmente, este curso <phrase>te</phrase> permitirá conocer y utilizar distintos criterios bajo los cuales podrás tomar <phrase>decisiones</phrase>, a partir del análisis de datos <phrase>e</phrase> información clave asociada a <phrase>la</phrase> misma. Estos criterios incluyen aspectos fundamentales como costos de oportunidad, escenarios de incertidumbre, así como distintos objetivos o metas a seleccionar, para que puedas elegir <phrase>en</phrase> forma efectiva <phrase>la</phrase> mejor decisión a tomar.  Agradecemos a Fundación Televisa por <phrase>su</phrase> participación <phrase>en</phrase> <phrase>la</phrase> producción de este curso; con lo cual colabora a inspirar y desarrollar <phrase>el</phrase> potencial de las personas, a través de <phrase>su</phrase> compromiso con <phrase>la</phrase> educación y <phrase>la</phrase> cultura.
Welcome to <phrase>Supply Chain</phrase> Analytics - an exciting and emerging <phrase>area</phrase> that is in <phrase>high</phrase> demand!   In this introductory course to <phrase>Supply Chain</phrase> Analytics, I will take you on a journey to this fascinating <phrase>area</phrase> of <phrase>Supply chain</phrase> analytics where <phrase>supply chain management</phrase> meets <phrase>data</phrase> analytics. You will learn <phrase>real life</phrase> stories and examples on how analytics can apply to various domains of a <phrase>supply chain</phrase>, from sell, to move, make and source, to generate a significant social and / or economic impact. You will also learn about job <phrase>market trend</phrase>, job requirement and preparation. Lastly, you will <phrase>master</phrase> a <phrase>data</phrase> analysis technique to assess the efficiency of a <phrase>supply chain</phrase>. <phrase>Supply Chain</phrase> Analytics is a relatively new and fast developing <phrase>area</phrase>. Thus this course is by no mean exhaustive nor should you expect to be an expert upon completion. My goal of this course is to open the eyes of the learners to the impact of <phrase>Supply Chain</phrase> Analytics, and hopefully this will encourage you to learn more.  Upon completing this course, you will  1. Understand why analytics is critical to today’s <phrase>supply chains</phrase> 2. See the <phrase>pain</phrase> points of a <phrase>supply chain</phrase> and how analytics can be applied to <phrase>address</phrase> them 3. Understand the requirement of <phrase>supply chain</phrase> analytics jobs, and how to prepare for them 4. Assess a company’s overall <phrase>supply chain</phrase> efficiency quantitatively  I hope you enjoy the course!
In this course, you will learn about several <phrase>algorithms</phrase> that can learn near optimal policies based on trial and error interaction with the environment---learning from the agent’s own experience. Learning from actual experience is striking because it requires no <phrase>prior knowledge</phrase> of the environment’s dynamics, yet can still attain optimal behavior. We will <phrase>cover</phrase> intuitively simple but powerful <phrase>Monte Carlo methods</phrase>, and <phrase>temporal difference</phrase> learning methods including <phrase>Q-learning</phrase>. We will wrap up this course investigating how we can get the best of both worlds: <phrase>algorithms</phrase> that can combine <phrase>model</phrase>-based planning (similar to <phrase>dynamic programming</phrase>) and <phrase>temporal difference</phrase> updates to radically accelerate learning.  By the end of this course you will be able to:   - Understand <phrase>Temporal-Difference</phrase> learning and <phrase>Monte Carlo</phrase> as two strategies for estimating value functions from sampled experience - Understand the importance of exploration, when using sampled experience rather than <phrase>dynamic programming</phrase> sweeps within a <phrase>model</phrase> - Understand the connections between <phrase>Monte Carlo</phrase> and <phrase>Dynamic Programming</phrase> and <phrase>TD</phrase>.  - Implement and apply the <phrase>TD</phrase> <phrase>algorithm</phrase>, for estimating value functions - Implement and apply Expected Sarsa and <phrase>Q-learning</phrase> (two <phrase>TD</phrase> methods for control)  - Understand the difference between on-policy and off-policy control - Understand planning with simulated experience (as opposed to classic planning strategies) - Implement a <phrase>model-based</phrase> approach to <phrase>RL</phrase>, called Dyna, which uses simulated experience  - Conduct <phrase>an empirical study</phrase> to see the improvements in sample efficiency when using Dyna
<phrase>Case</phrase> Studies: Finding Similar Documents  A reader is interested in a specific <phrase>news</phrase> article and you want to find similar articles to recommend.  What is the right notion of similarity?  Moreover, what if there are millions of other documents?  Each time you want to a retrieve a new document, do you need to search through all other documents?  How do you group similar documents together?  How do you discover new, emerging topics that the documents <phrase>cover</phrase>?     In this third <phrase>case</phrase> study, finding similar documents, you will examine similarity-<phrase>based algorithms</phrase> for retrieval.  In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as <phrase>latent Dirichlet allocation</phrase> (<phrase>LDA</phrase>).  You will implement <phrase>expectation maximization</phrase> (<phrase>EM</phrase>) to learn the document clusterings, and see how to scale the methods using <phrase>MapReduce</phrase>.  <phrase>Learning Outcomes</phrase>:  By the end of this course, you will be able to:    -Create a document retrieval system using <phrase>k-nearest neighbors</phrase>.    -Identify various similarity metrics for text <phrase>data</phrase>.    -Reduce computations in k-<phrase>nearest neighbor search</phrase> by using KD-<phrase>trees</phrase>.    -Produce approximate <phrase>nearest neighbors</phrase> using <phrase>locality sensitive hashing</phrase>.    -Compare and contrast supervised and <phrase>unsupervised learning</phrase> tasks.    -Cluster documents by topic using k-means.    -Describe how to parallelize k-means using <phrase>MapReduce</phrase>.    -Examine probabilistic clustering approaches using mixtures models.    -Fit a mixture of Gaussian <phrase>model</phrase> using <phrase>expectation maximization</phrase> (<phrase>EM</phrase>).    -Perform mixed membership modeling using <phrase>latent Dirichlet allocation</phrase> (<phrase>LDA</phrase>).    -Describe the steps of a Gibbs sampler and how to use its output to draw inferences.    -Compare and contrast initialization techniques for non-<phrase>convex optimization</phrase> objectives.    -Implement these techniques in <phrase>Python</phrase>.
Organisé <phrase>en</phrase> deux <phrase>parties</phrase>, <phrase>ce</phrase> cours présente <phrase>les</phrase> bases théoriques et pratiques <phrase>des</phrase> systèmes d’information géographique. - Il propose une introduction aux systèmes d’information géographique qui ne requiert <phrase>pas</phrase> de connaissances préalables <phrase>en</phrase> informatique - Il donne <phrase>la</phrase> possibilité d’acquérir rapidement <phrase>les</phrase> notions de base qui vous permettent de créer <phrase>des</phrase> bases de données spatiales et de fabriquer <phrase>des</phrase> cartes géographiques - Il s’agit d’un cours pratique qui repose sur l’utilisation de logiciels libres, notamment QGIS <phrase>En</phrase> somme, <phrase>si</phrase> vos études ou votre profession comprennent <phrase>des</phrase> activités liées à <phrase>la</phrase> gestion de territoires, à l’analyse d’objets distribués dans l’espace géographique (aménagement du territoire, biologie, santé publique, écologie, énergie, etc.), <phrase>ce</phrase> cours est fait pour vous!  <phrase>En</phrase> suivant cette première partie du cours, vous explorerez <phrase>les</phrase> <phrase>principes</phrase> de base <phrase>de la</phrase> numérisation du territoire et du stockage <phrase>des</phrase> géodonnées. Vous apprendrez notamment à : - Caractériser <phrase>des</phrase> objets et/ou phénomènes spatiaux (modélisation du territoire) du point de vue de leur positionnement dans l’espace (systèmes de coordonnées et projections, relations spatiales) et <phrase>en</phrase> fonction de leur <phrase>nature</phrase> intrinsèque (mode objet ou vecteur vs. mode image ou <phrase>raster</phrase>),  - Utiliser <phrase>les</phrase> diverses méthodes d’acquisition de données (mesure directe, géoréférencement d’images, digitalisation, source de données existantes, etc.) - Utiliser <phrase>les</phrase> divers modes de stockage <phrase>des</phrase> géodonnées – Fichiers simples et/ou bases de données relationnelles - Utiliser <phrase>des</phrase> outils de modélisation <phrase>des</phrase> données pour décrire et implémenter une base de données  - Créer <phrase>des</phrase> requêtes dans le langage d’interrogation et de manipulation <phrase>des</phrase> données  <phrase>La</phrase> seconde partie du cours portera sur <phrase>les</phrase> méthodes d'analyse spatiale et <phrase>les</phrase> techniques de représentation <phrase>des</phrase> géo-données. Vous apprendrez notamment à: - Analyser <phrase>les</phrase> propriétés spatiales de variables discrètes, par exemple <phrase>en</phrase> quantifiant l’autocorrélation spatiale - Travailler avec <phrase>les</phrase> variables continues (échantillonnage, <phrase>construction</phrase> de courbes d’isovaleurs, méthodes d’interpolation) - Utiliser <phrase>les</phrase> modèles numériques d'altitude et leurs dérivées (pente, orientation, etc.) - Utiliser <phrase>les</phrase> techniques de <phrase>superposition</phrase> de géodonnées et d'interaction entre elles - Produire <phrase>des</phrase> documents cartographiques selon <phrase>les</phrase> règles <phrase>de la</phrase> sémiologie graphique - Explorer d’autres formes de représentation spatiale (cartographie interactive sur <phrase>internet</phrase>, représentations 3D, etc.)  <phrase>La</phrase> page https://www.facebook.com/moocsig fournit un forum interactif pour <phrase>les</phrase> participants à <phrase>ce</phrase> cours.
There are opportunities throughout the <phrase>design</phrase> process of any product to make significant changes, and ultimately impact the future of <phrase>manufacturing</phrase>, by embracing the <phrase>digital</phrase> thread. In this course, you will dig into the transformation <phrase>taking place</phrase> in how <phrase>products</phrase> are designed and manufactured throughout the world. It is the second of two courses that focuses on the "<phrase>digital</phrase> thread" – the <phrase>stream</phrase> that starts at the creation of a product concept and continues to accumulate <phrase>information</phrase> and <phrase>data</phrase> throughout the <phrase>product life cycle</phrase>.    Hear about the realities of implementing the <phrase>digital</phrase> thread, directly from someone responsible for making it happen at a <phrase>company</phrase>. Learn how the <phrase>digital</phrase> thread can fit into <phrase>product development</phrase> processes in an office, on a shop floor, and even across an enterprise. Be prepared to <phrase>talk</phrase> about the benefits, and limitations, of enacting it.  Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the third course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
What happens when <phrase>creativity</phrase> and <phrase>science</phrase> come together?  The power to <phrase>design</phrase> our world is unleashed, providing tools to inform choices about how we <phrase>live</phrase>!  Geodesign is the glue—it’s a process that deploys <phrase>creativity</phrase> to connect <phrase>information</phrase> to people, using collaboration to better inform how we <phrase>design</phrase> our world.  This course includes well-illustrated lectures by the instructor, but also guest lectures each week to ensure you are hearing a <phrase>variety</phrase> of viewpoints.  Each week you will also be able to examine what geodesign is through interactive mapping that showcases real-word <phrase>Case</phrase> Study examples of geodesign from around the globe.  As you move along in the course, you will discover the interrelationships of both the physical and <phrase>human</phrase> aspects that contribute to how geodesign strategies are composed.  The course concludes with you outlining your own Geodesign Challenge, and receiving <phrase>feedback</phrase> about that from your <phrase>peers</phrase>.
The practice of <phrase>investment management</phrase> has been transformed in <phrase>recent years</phrase> by <phrase>computational methods</phrase>. This course provides <phrase>an introduction</phrase> to the underlying <phrase>science</phrase>, with the aim of giving you a thorough understanding of that scientific basis. However, instead of merely explaining the <phrase>science</phrase>, we help you build on that foundation in a practical manner, with an emphasis on the hands-on implementation of those ideas in the <phrase>Python programming language</phrase>.   This course is the first in a four course specialization in <phrase>Data Science</phrase> and <phrase>Machine Learning</phrase> in <phrase>Asset Management</phrase> but can be taken independently. In this course, we <phrase>cover</phrase> the basics of <phrase>Investment</phrase> <phrase>Science</phrase>, and we'll build practical implementations of each of the concepts along the way. We'll start with the very basics of <phrase>risk</phrase> and return and quickly progress to <phrase>cover</phrase> a <phrase>range</phrase> of topics including several <phrase>Nobel</phrase> Prize winning concepts. We'll <phrase>cover</phrase> some of the most popular practical techniques in modern, <phrase>state</phrase> of the <phrase>art</phrase> <phrase>investment management</phrase> and portfolio <phrase>construction</phrase>.   As we <phrase>cover</phrase> the theory and <phrase>math</phrase> in lecture videos, we'll also implement the concepts in <phrase>Python</phrase>, and you'll be able to code along with us so that you have a deep and practical understanding of how those methods work. By the time you are done, not only will you have a foundational understanding of modern <phrase>computational methods</phrase> in <phrase>investment management</phrase>, you'll have practical mastery in the implementation of those methods.
在社会学、心理学、教育学、经济学、管理学、市场学等研究领域的数据分析中，结构方程建模是当前最前沿的统计方法中应用最广、研究最多的一个。它包含了方差分析、回归分析、路径分析和因子分析，弥补了传统回归分析和因子分析的不足，可以分析多因多果的联系、潜变量的关系，还可以处理多水平数据和纵向数据，是非常重要的多元数据分析工具。本课程系统地介绍结构方程模型和LISREL软件的应用，内容包括：结构方程分析（包括验证性因子分析）的基本概念、统计原理、在社会科学研究中的应用、常用模型及其LISREL程序、结果的解释和模型评价。学员应具备基本的统计知识（如：标准差、t-检验、相关系数），理解回归分析和因子分析的概念。 注：本课程配套教材为《结构方程模型及其应用》（以LISREL软件为例）。
<phrase>Big Data</phrase> analytics tools are increasingly critical for providing meaningful <phrase>information</phrase> for making better <phrase>business</phrase> decisions. <phrase>Big data</phrase> technologies bring significant cost advantages when it comes to storing and managing <phrase>large amounts of data</phrase>. Understanding how to query a <phrase>database</phrase> to extract <phrase>data</phrase> will empower better analysis of large, complex datasets. <phrase>Knowledge</phrase> of Indexing mechanisms makes possible <phrase>high</phrase>-speed, selective retrieval of large amounts of <phrase>information</phrase>.
<phrase>En</phrase> este último curso <phrase>de la</phrase> Especialización <phrase>Big Data</phrase> <phrase>el</phrase> estudiante tendrá <phrase>la</phrase> oportunidad de aplicar algunas de las herramientas y métodos aprendidos <phrase>en</phrase> los cursos anteriores <phrase>en</phrase> un caso práctico.  <phrase>El</phrase> objetivo de este Capstone Project <phrase>es</phrase> mostrar un ejemplo del trabajo que <phrase>se</phrase> realiza diariamente <phrase>en</phrase> <phrase>el</phrase> departamento de Cosmología del <phrase>Port</phrase> d’Informació Científica, <phrase>en</phrase> <phrase>Barcelona</phrase>. <phrase>Se</phrase> trata de crear un clasificador para imágenes de galaxias, a partir de datos del proyecto GalaxyZoo <phrase>e</phrase> imágenes y datos del telescopio <phrase>Sloan Digital Sky Survey</phrase>. Los trabajos y ejercicios guiados llevarán al estudiante a <phrase>la</phrase> exploración y analisis de estos datos, hasta realizar una herramienta automática de <phrase>Machine Learning</phrase>.  <phrase>El</phrase> proceso seguido por los estudiantes <phrase>en</phrase> este curso <phrase>se</phrase> podría aplicar <phrase>en</phrase> cualquier otra disciplina, por ejemplo <phrase>en</phrase> las ciencias sociales, <phrase>en</phrase> un estudio de mercado o <phrase>en</phrase> cualquier ámbito que comporte toma de <phrase>decisiones</phrase> a partir de un gran volumen de datos.
This course seeks to turn learners into informed consumers of <phrase>social science</phrase> <phrase>research</phrase>. It introduces concepts, standards, and principles of <phrase>social science</phrase> <phrase>research</phrase> to the interested non-expert. Learners who complete the course will be able to assess evidence and critically evaluate claims about important social phenomena. It reviews the origins and development of <phrase>social science</phrase>, describes the process of discovery in contemporary <phrase>social science</phrase> <phrase>research</phrase>, and explains how contemporary <phrase>social science</phrase> differs from apparently <phrase>related fields</phrase>. It describes the goals, <phrase>basic</phrase> paradigms, and methodologies of the <phrase>major</phrase> <phrase>social science</phrase> disciplines. It offers an overview of the <phrase>major</phrase> questions that are the focus of much contemporary <phrase>social science</phrase> <phrase>research</phrase>, overall and for <phrase>China</phrase>. Special emphasis is given to explaining the challenges that social scientists face in <phrase>drawing</phrase> conclusions about cause and effect from their studies, and offers an overview of the approaches that are used to overcome these challenges. Explanation is non-technical and does not involve <phrase>mathematics</phrase>. <phrase>Statistics</phrase> and quantitative methods are not <phrase>covered</phrase>.   Explore the big questions in <phrase>social science</phrase> and learn how you can be a critical, informed <phrase>consumer</phrase> of <phrase>social science</phrase> <phrase>research</phrase>.   Course Overview <phrase>video</phrase>: https://youtu.be/QuMOAlwhpvU  After you complete Part 1, enroll in Part 2 to learn how to be a <phrase>PRODUCER</phrase> of <phrase>Social science</phrase> <phrase>research</phrase>.  Part 2:  https://www.coursera.org/learn/<phrase>social-science</phrase>-<phrase>research</phrase>-<phrase>chinese</phrase>-<phrase>society</phrase>
The number of composite indices that are constructed and used internationally is growing very fast; but whilst the complexity of quantitative techniques has increased dramatically, the <phrase>education</phrase> and training in this <phrase>area</phrase> has been dragging and lagging behind. As a consequence, these simple numbers, expected to synthesize quite complex issues, are often presented to the <phrase>public</phrase> and used in the <phrase>political</phrase> debate without proper emphasis on their intrinsic limitations and correct interpretations.   In this course on global <phrase>statistics</phrase>, offered by the <phrase>University</phrase> of <phrase>Geneva</phrase> jointly with the <phrase>ETH Zürich</phrase> KOF, you will learn the <phrase>general</phrase> approach of constructing composite indices and some of resulting problems. We will discuss the technical properties, the <phrase>internal structure</phrase> (like <phrase>aggregation</phrase>, weighting, stability of <phrase>time series</phrase>), the primary <phrase>data</phrase> used and the <phrase>variable</phrase> selection methods.  These concepts will be illustrated using a sample of the most popular composite indices. We will try to <phrase>address</phrase> not only statistical questions but also focus on the distinction between policy-, <phrase>media</phrase>- and <phrase>paradigm</phrase>-driven indicators.
In this capstone course, you will apply everything you have learned by designing and then completing your own <phrase>GIS</phrase> project. You will plan out your project by writing a brief proposal that explains what you plan to do and why. You will then find <phrase>data</phrase> for a topic and location of your choice, and perform analysis and create maps that allow you to try out different tools and <phrase>data</phrase> sets. The <phrase>results</phrase> of your work will be assembled into an <phrase>Esri</phrase> story map, which is a <phrase>web site</phrase> with maps, images, text, and <phrase>video</phrase>. The goal is for you to have a <phrase>finished product</phrase> that you can share, and that demonstrates what you have learned.
In this fourth of our five courses, I will go deeper into the training and <phrase>education</phrase> <phrase>leadership</phrase> skills that are helpful for <phrase>nursing</phrase> informatics leaders. I will also guide you through the process of preparing a course document or syllabus for the <phrase>nursing</phrase> informatics specialty both in <phrase>academic</phrase> settings and in practice or <phrase>industry</phrase>. Following are the course objectives: 1. Describe relevant <phrase>nursing</phrase> informatics course development in clinical and <phrase>academic</phrase> settings to understand similarities and differences in informatics teaching and <phrase>education</phrase> across settings. 2. Describe informatics <phrase>education</phrase> and training needs for diverse participants with various experience levels to enable development of appropriate training and <phrase>education</phrase> materials. 3. Develop a <phrase>prototype</phrase> course syllabus and introductory recorded message to apply learning in a simulated setting. 4. Describe the benefits of formal and informal mentoring for <phrase>nursing</phrase> informaticians to advance career opportunities and support the <phrase>nursing</phrase> informatics specialty.
この 1 週間の集中オンデマンド コースでは、<phrase>Google</phrase> <phrase>Cloud</phrase> Platform での機械学習モデルの設計と構築を実践しながら学びます。講義、デモ、ハンズオンラボを通じて、機械学習（<phrase>ML</phrase>）と TensorFlow の概念を学習し、<phrase>ML</phrase> モデルの開発、評価、製品化の実践的なスキルを習得します。  目標  このコースでは、次のスキルについて学習します。     ● 機械学習のユースケースを理解する    ● TensorFlow を使用して <phrase>ML</phrase> モデルを構築する    ● <phrase>Cloud</phrase> <phrase>ML</phrase> を使用してスケーラブルで導入可能な <phrase>ML</phrase> モデルを構築する    ● 前処理と結合機能の重要性を理解する    ● 高度な <phrase>ML</phrase> の概念をモデルに組み込む    ● トレーニング済みの <phrase>ML</phrase> モデルを製品化する  前提条件  このコースを最大限に活用するには、次の前提条件を満たしている必要があります。     ● <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals コースを修了しているか、同等の経験がある    ● <phrase>SQL</phrase> などの一般的なクエリ言語に関する基本的な知識がある    ● データのモデリング、抽出、変換、読み込みのアクティビティの経験がある    ● <phrase>Python</phrase> などの一般的なプログラミング言語を使用したアプリケーションの開発経験がある    ● 機械学習と統計に精通している  <phrase>Google</phrase> アカウントについての注意: • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform の無料トライアルに登録するには、<phrase>Google</phrase> / <phrase>Gmail</phrase> アカウントとクレジットカードまたは銀行口座が必要です（<phrase>Google</phrase> のサービスは、現在中国ではご利用いただけません）。 • 請求先住所が欧州連合（<phrase>EU</phrase>）またはロシアの <phrase>Google</phrase> <phrase>Cloud</phrase> Platform のユーザーは、VAT の概要文書（https://cloud.google.com/billing/docs/resources/vat-overview）をお読みください。 • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform の無料トライアルに関するよくある質問については、https://cloud.google.com/<phrase>free</phrase>/docs/<phrase>frequently-asked-questions</phrase> をご覧ください。
在信息化社会，充分有效地管理和利用各类信息资源，是进行科学研究和决策管理的前提。数据库技术是有效地管理和利用各类信息资源的重要技术手段。通过本课程，你将获得数据库技术的基本知识，学会通过<phrase>SQL Server</phrase>数据库管理系统管理数据，包括安装数据库管理系统，创建数据库和数据表，插入数据和维护数据等，特别是可以使用<phrase>SQL Server</phrase>按各种条件查询需要的信息。
This course offers a <phrase>rigorous mathematical</phrase> survey of <phrase>advanced topics</phrase> in causal inference at the Master’s level.  Inferences about causation are of great importance in <phrase>science</phrase>, <phrase>medicine</phrase>, policy, and <phrase>business</phrase>.  This course provides <phrase>an introduction</phrase> to the statistical <phrase>literature</phrase> on causal inference that has emerged in the last 35-40 years and that has revolutionized the way in which statisticians and applied researchers in many disciplines use <phrase>data</phrase> to make inferences about <phrase>causal relationships</phrase>.    We will study <phrase>advanced topics</phrase> in causal inference, including mediation, principal <phrase>stratification</phrase>, longitudinal causal inference, <phrase>regression</phrase> discontinuity, interference, and fixed effects models.
This course aims to <phrase>introduce</phrase> learners to advanced visualization techniques beyond the <phrase>basic</phrase> charts <phrase>covered</phrase> in <phrase>Information</phrase> Visualization: Fundamentals. These techniques are organized around <phrase>data</phrase> types to <phrase>cover</phrase> advance methods for: <phrase>temporal and spatial</phrase> <phrase>data</phrase>, networks and <phrase>trees</phrase> and <phrase>textual data</phrase>. In this module we also teach learners how to develop innovative techniques in D3.js.  Learning Goals Goal: Analyze the <phrase>design</phrase> space of visualization solutions for various kinds of <phrase>data visualization</phrase> problems. Learn what designs are available for a given problem and what are their respective <phrase>advantages and disadvantages</phrase>. - Temporal - Spatial - <phrase>Spatio-Temporal</phrase> - Networks - <phrase>Trees</phrase> - Text  This is the fourth course in the <phrase>Information</phrase> Visualization Specialization. The course expects you to have some <phrase>basic</phrase> <phrase>knowledge</phrase> of <phrase>programming</phrase> as well as some <phrase>basic</phrase> visualization skills (as those introduced in the first course of the specialization).
By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<  Welcome to the <phrase>Coursera</phrase> course, <phrase>Industrial</phrase> <phrase>Internet</phrase> of Things (<phrase>IoT</phrase>) on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform (GCP) brought to you by the <phrase>Google</phrase> <phrase>Cloud</phrase> team. I’m Catherine Gamboa and I’m going to be your guide.  This course covers the entire <phrase>Industrial</phrase> <phrase>IoT</phrase> <phrase>network architecture</phrase> from sensors and devices to analysis. The course discusses sensors and devices but the focus is on the <phrase>cloud</phrase> side.  You'll learn about the importance of scaling, device <phrase>communication</phrase>, and processing <phrase>streaming</phrase> <phrase>data</phrase>. The course uses simulated devices in the labs to allow you to concentrate on learning the <phrase>cloud</phrase> side of IIoT.  The course is a little different than most <phrase>Coursera</phrase> courses because there is very little <phrase>video</phrase>. Most of the learning is done with <phrase>short</phrase> readings, quizzes, and labs.    This course takes about two weeks to complete, 11-12 hours of work with 6 of those hours spent in labs.  By the end of this course, you’ll be able to: create a <phrase>streaming</phrase> <phrase>data</phrase> <phrase>pipeline</phrase>, to create registries with <phrase>Cloud</phrase> <phrase>IoT</phrase> Core, topics and subscriptions with <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub, <phrase>store data</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Storage, query the <phrase>data</phrase> in BigQuery, and gain <phrase>data</phrase> insights with Dataprep.  You'll learn and practice these skills in 7 labs.  Then you'll have an opportunity to <phrase>test</phrase> yourself in an optional capstone lab using simulated devices or <phrase>Cloud</phrase> <phrase>IoT</phrase> Core Inspector.
We <phrase>live</phrase> in an uncertain and complex world, yet we continually have to make decisions in the present with uncertain future outcomes.  Indeed, we should be on the look-out for "<phrase>black</phrase> swans" - low-<phrase>probability</phrase> <phrase>high</phrase>-impact events.  To study, or not to study?  To invest, or not to invest?  To marry, or not to marry?  While uncertainty makes <phrase>decision-making</phrase> difficult, it does at least make <phrase>life</phrase> exciting!  If the entire future was known in advance, there would never be an element of surprise.  Whether a good future or a bad future, it would be a known future.  In this course we consider many useful tools to deal with uncertainty and help us to make informed (and hence better) decisions - essential skills for a <phrase>lifetime</phrase> of good <phrase>decision-making</phrase>.  Key topics include quantifying uncertainty with <phrase>probability</phrase>, <phrase>descriptive statistics</phrase>, point and interval estimation of means and proportions, the basics of <phrase>hypothesis testing</phrase>, and a selection of multivariate applications of key <phrase>terms and concepts</phrase> seen throughout the course.
Want to know how you can improve the accuracy of your <phrase>machine learning</phrase> models? What about how to find which <phrase>data</phrase> columns make the most useful features? Welcome to Feature <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform where we will discuss the elements of good vs bad features and how you can preprocess and transform them for optimal use in your <phrase>machine learning</phrase> models.  In this course you will get hands-on practice choosing features and preprocessing them inside of <phrase>Google</phrase> <phrase>Cloud</phrase> Platform with interactive labs. Our instructors will walk you through the code solutions which will also be made <phrase>public</phrase> for your reference as you work on your own future <phrase>data science</phrase> projects.  >>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
Learner Outcomes: After taking this course, you will be able to: - Utilize various <phrase>Application Programming Interface</phrase> (<phrase>API</phrase>) services to <phrase>collect data</phrase> from different <phrase>social media</phrase> sources such as <phrase>YouTube</phrase>, <phrase>Twitter</phrase>, and <phrase>Flickr</phrase>. - Process the <phrase>collected data</phrase> - primarily structured - using methods involving correlation, <phrase>regression</phrase>, and classification to derive insights about the sources and people who generated that <phrase>data</phrase>. - Analyze <phrase>unstructured data</phrase> - primarily textual comments - for sentiments expressed in them. - Use different tools for collecting, analyzing, and exploring <phrase>social media</phrase> <phrase>data</phrase> for <phrase>research</phrase> and development purposes.  Sample Learner Story: <phrase>Data</phrase> analyst wanting to leverage <phrase>social media</phrase> <phrase>data</phrase>. Isabella is a <phrase>Data</phrase> Analyst working as a consultant for a <phrase>multinational corporation</phrase>. She has experience working with Web <phrase>analysis tools</phrase> as well as <phrase>marketing</phrase> <phrase>data</phrase>. She wants to now expand into <phrase>social media</phrase> <phrase>arena</phrase>, trying to leverage the vast amounts of <phrase>data</phrase> available through various <phrase>social media</phrase> channels. Specifically, she wants to see how their clients, partners, and competitors view their <phrase>products</phrase>/services and <phrase>talk</phrase> about them. She hopes to build a new <phrase>workflow</phrase> of <phrase>data</phrase> analytics that incorporates traditional <phrase>data</phrase> processing using Web and <phrase>marketing</phrase> tools, as well as newer methods of using <phrase>social media</phrase> <phrase>data</phrase>.  Sample Job Roles requiring these skills:  - <phrase>Social Media</phrase> Analyst - Web Analyst - <phrase>Data</phrase> Analyst - <phrase>Marketing</phrase> and <phrase>Public Relations</phrase>   Final Project Deliverable/ <phrase>Artifact</phrase>: The course will have a series of small assignments or <phrase>mini</phrase>-projects that involve <phrase>data</phrase> collection, analysis, and presentation involving various <phrase>social media</phrase> sources using the techniques learned in the class.
This course will prepare you to complete all parts of the <phrase>Clinical Data</phrase> <phrase>Science</phrase> Specialization. In this course you will learn how <phrase>clinical data</phrase> are generated, the format of these <phrase>data</phrase>, and the <phrase>ethical</phrase> and legal restrictions on these <phrase>data</phrase>. You will also learn enough <phrase>SQL</phrase> and R <phrase>programming</phrase> skills to be able to complete the entire Specialization - even if you are a beginner <phrase>programmer</phrase>. While you are taking this course you will have access to an actual <phrase>clinical data</phrase> set and a <phrase>free</phrase>, online computational environment for <phrase>data science</phrase> hosted by our <phrase>Industry</phrase> Partner <phrase>Google</phrase> <phrase>Cloud</phrase>.   At the end of this course you will be prepared to embark on your <phrase>clinical data</phrase> <phrase>science</phrase> <phrase>education</phrase> journey, learning how to take <phrase>data</phrase> created by the <phrase>healthcare</phrase> system and improve the <phrase>health</phrase> of tomorrow's patients.
This one-week accelerated <phrase>on-demand</phrase> course provides participants a a hands-on introduction to <phrase>designing and building</phrase> <phrase>machine learning</phrase> models on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Through a combination of presentations, demos, and hand-on labs, participants will learn <phrase>machine learning</phrase> (<phrase>ML</phrase>) and TensorFlow concepts, and develop hands-on skills in developing, evaluating, and productionizing <phrase>ML</phrase> models.  OBJECTIVES  This course teaches participants the following skills:    ● Identify <phrase>use cases</phrase> for <phrase>machine learning</phrase>    ● Build an <phrase>ML</phrase> <phrase>model</phrase> using TensorFlow    ● Build scalable, deployable <phrase>ML</phrase> models using <phrase>Cloud</phrase> <phrase>ML</phrase>    ● Know the importance of preprocessing and combining features    ● Incorporate advanced <phrase>ML</phrase> concepts into their models    ● Productionize trained <phrase>ML</phrase> models   PREREQUISITES  To get the most of out of this course, participants should have:    ● Completed <phrase>Google</phrase> <phrase>Cloud</phrase> Fundamentals- <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> course OR have equivalent experience    ● <phrase>Basic</phrase> proficiency with common <phrase>query language</phrase> such as <phrase>SQL</phrase>    ● Experience with <phrase>data</phrase> modeling, extract, transform, load activities    ● Developing applications using a common <phrase>programming language</phrase> such <phrase>Python</phrase>    ● Familiarity with <phrase>Machine Learning</phrase> and/or <phrase>statistics</phrase>  <phrase>Google</phrase> Account Notes: • <phrase>Google</phrase> services are currently unavailable in <phrase>China</phrase>.
This course provides <phrase>an introduction</phrase> to <phrase>basic</phrase> <phrase>computational methods</phrase> for understanding what <phrase>nervous</phrase> systems do and for determining how they <phrase>function</phrase>. We will explore the computational principles governing various aspects of vision, sensory-<phrase>motor control</phrase>, learning, and <phrase>memory</phrase>. Specific topics that will be <phrase>covered</phrase> include representation of <phrase>information</phrase> by spiking <phrase>neurons</phrase>, processing of <phrase>information</phrase> in <phrase>neural networks</phrase>, and <phrase>algorithms</phrase> for adaptation and learning. We will make use of <phrase>Matlab</phrase>/<phrase>Octave</phrase>/<phrase>Python</phrase> demonstrations and exercises to gain a deeper understanding of concepts and methods introduced in the course. The course is primarily aimed at third- or fourth-year undergraduates and beginning graduate students, as well as professionals and distance learners interested in learning how the <phrase>brain</phrase> processes <phrase>information</phrase>.
If you’ve ever skipped over`the <phrase>results</phrase> section of a <phrase>medical</phrase> <phrase>paper</phrase> because terms like “<phrase>confidence interval</phrase>” or “<phrase>p-value</phrase>” go over your head, then you’re in the right <phrase>place</phrase>. You may be a clinical practitioner <phrase>reading</phrase> <phrase>research</phrase> articles to keep up-to-date with developments in your field or a <phrase>medical</phrase> <phrase>student</phrase> wondering how to approach your own <phrase>research</phrase>. Greater confidence in understanding <phrase>statistical analysis</phrase> and the <phrase>results</phrase> can benefit both working professionals and those undertaking <phrase>research</phrase> themselves.   If you are simply interested in properly understanding the published <phrase>literature</phrase> or if you are embarking on <phrase>conducting</phrase> your own <phrase>research</phrase>, this course is your first <phrase>step</phrase>. It offers an easy entry into interpreting common statistical concepts without getting into nitty-gritty <phrase>mathematical</phrase> formulae. To be able to interpret and understand these concepts is the best way to start your journey into the world of clinical <phrase>literature</phrase>. That’s where this course comes in - so let’s get started!  The course is <phrase>free</phrase> to enroll and take. You will be offered the <phrase>option</phrase> of purchasing a certificate of completion which you become eligible for, if you successfully complete the course requirements. This can be an excellent way of staying motivated!  Financial Aid is also available.
Interested in increasing your <phrase>knowledge</phrase> of the <phrase>Big Data</phrase> <phrase>landscape</phrase>?  This course is for those new to <phrase>data science</phrase> and interested in understanding why the <phrase>Big Data</phrase> <phrase>Era</phrase> has come to be.  It is for those who want to become conversant with the terminology and the <phrase>core concepts</phrase> behind <phrase>big data</phrase> problems, applications, and systems.  It is for those who want to start thinking about how <phrase>Big Data</phrase> might be useful in their <phrase>business</phrase> or career.  It provides <phrase>an introduction</phrase> to one of the most common frameworks, <phrase>Hadoop</phrase>, that has made <phrase>big data</phrase> analysis easier and more accessible -- increasing the potential for <phrase>data</phrase> to transform our world!  At the end of this course, you will be able to:  * Describe the <phrase>Big Data</phrase> <phrase>landscape</phrase> including examples of <phrase>real world</phrase> <phrase>big data</phrase> problems including the three key sources of <phrase>Big Data</phrase>: people, organizations, and sensors.   * Explain the V’s of <phrase>Big Data</phrase> (volume, <phrase>velocity</phrase>, <phrase>variety</phrase>, veracity, valence, and value) and why each impacts <phrase>data</phrase> collection, monitoring, storage, analysis and reporting.  * Get value out of <phrase>Big Data</phrase> by using a 5-<phrase>step</phrase> process to structure your analysis.   * Identify what are and what are not <phrase>big data</phrase> problems and be able to recast <phrase>big data</phrase> problems as <phrase>data science</phrase> questions.  * Provide an explanation of the <phrase>architectural</phrase> components and <phrase>programming</phrase> models used for scalable <phrase>big data</phrase> analysis.  * Summarize the features and value of core <phrase>Hadoop</phrase> <phrase>stack</phrase> components including the <phrase>YARN</phrase> resource and job <phrase>management</phrase> system, the HDFS <phrase>file system</phrase> and the <phrase>MapReduce</phrase> <phrase>programming</phrase> <phrase>model</phrase>.  * <phrase>Install</phrase> and <phrase>run</phrase> a program using <phrase>Hadoop</phrase>!  This course is for those new to <phrase>data science</phrase>.  No prior <phrase>programming</phrase> experience is needed, although the ability to <phrase>install</phrase> applications and utilize a <phrase>virtual machine</phrase> is necessary to complete the hands-on assignments.    <phrase>Hardware</phrase> Requirements: (A) <phrase>Quad Core</phrase> Processor (<phrase>VT</phrase>-x or <phrase>AMD</phrase>-V support recommended), <phrase>64-bit</phrase>; (B) 8 <phrase>GB</phrase> <phrase>RAM</phrase>; (C) 20 <phrase>GB</phrase> disk <phrase>free</phrase>. How to find your <phrase>hardware</phrase> <phrase>information</phrase>: (<phrase>Windows</phrase>): <phrase>Open System</phrase> by clicking the Start button, right-clicking <phrase>Computer</phrase>, and then clicking Properties; (<phrase>Mac</phrase>): Open Overview by clicking on the <phrase>Apple</phrase> menu and clicking “About This <phrase>Mac</phrase>.” Most <phrase>computers</phrase> with 8 <phrase>GB</phrase> <phrase>RAM</phrase> purchased in the last 3 years will meet the minimum requirements.You will need a <phrase>high</phrase> speed <phrase>internet</phrase> connection because you will be downloading files up to 4 <phrase>Gb</phrase> in size.    <phrase>Software</phrase> Requirements: This course relies on several <phrase>open-source software</phrase> tools, including <phrase>Apache Hadoop</phrase>. All required <phrase>software</phrase> can be downloaded and installed <phrase>free</phrase> of charge. <phrase>Software</phrase> requirements include: <phrase>Windows</phrase> 7+, <phrase>Mac OS</phrase> X 10.10+, <phrase>Ubuntu</phrase> 14.04+ or <phrase>CentOS</phrase> 6+ <phrase>VirtualBox</phrase> 5+.
This course covers the essential exploratory techniques for summarizing <phrase>data</phrase>. These techniques are typically applied before formal modeling commences and can help inform the development of more complex <phrase>statistical models</phrase>. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the <phrase>data</phrase>. We will <phrase>cover</phrase> in detail the plotting systems in R as well as some of the <phrase>basic</phrase> principles of constructing <phrase>data</phrase> <phrase>graphics</phrase>. We will also <phrase>cover</phrase> some of the common multivariate <phrase>statistical techniques</phrase> used to visualize <phrase>high</phrase>-dimensional <phrase>data</phrase>.
This course covers <phrase>advanced topics</phrase> in R <phrase>programming</phrase> that are necessary for developing powerful, robust, and reusable <phrase>data science</phrase> tools. Topics <phrase>covered</phrase> include <phrase>functional programming</phrase> in R, robust <phrase>error handling</phrase>, <phrase>object oriented programming</phrase>, profiling and <phrase>benchmarking</phrase>, <phrase>debugging</phrase>, and proper <phrase>design</phrase> of functions. Upon completing this course you will be able to identify and abstract common <phrase>data analysis</phrase> tasks and to encapsulate them in user-facing functions. Because every <phrase>data science</phrase> environment encounters unique <phrase>data</phrase> challenges, there is always a need to develop custom <phrase>software</phrase> specific to your organization’s mission. You will also be able to define new <phrase>data</phrase> types in R and to develop a <phrase>universe</phrase> of functionality specific to those <phrase>data</phrase> types to enable cleaner execution of <phrase>data science</phrase> tasks and stronger reusability within a team.
<phrase>Epidemiological</phrase> <phrase>research</phrase> is ubiquitous. Even if you don’t realise it, you come across <phrase>epidemiological</phrase> studies and the impact of their findings every <phrase>single</phrase> day. You have probably heard that <phrase>obesity</phrase> is increasing in <phrase>high</phrase> income countries or that <phrase>malaria</phrase> is killing millions of people in low income countries. It is <phrase>common knowledge</phrase> that smoking causes <phrase>cancer</phrase> and that physical <phrase>activity</phrase> is protective against <phrase>heart disease</phrase>. These facts may seem obvious today, but it took decades of <phrase>epidemiological</phrase> <phrase>research</phrase> to produce the necessary evidence. In this course, you will learn the fundamental tools of <phrase>epidemiology</phrase> which are essential to conduct such studies, starting with the measures used to describe the <phrase>frequency</phrase> of a <phrase>disease</phrase> or <phrase>health</phrase>-related <phrase>condition</phrase>. You will also learn how to quantify the strength of an association and discuss the distinction between association and causation. In the second half of the course, you will use this <phrase>knowledge</phrase> to describe different strategies for prevention, identify <phrase>strengths and weaknesses</phrase> of diagnostic <phrase>tests</phrase> and consider when a screening programme is appropriate.
This introduction to <phrase>Python</phrase> will kickstart your learning of <phrase>Python</phrase> for <phrase>data science</phrase>, as well as <phrase>programming</phrase> in <phrase>general</phrase>. This beginner-<phrase>friendly</phrase> <phrase>Python</phrase> course will take you from <phrase>zero</phrase> to <phrase>programming</phrase> in <phrase>Python</phrase> in a <phrase>matter</phrase> of hours.  Module 1 - <phrase>Python</phrase> Basics o	Your first program o	Types o	Expressions and Variables o	<phrase>String</phrase> Operations  Module 2 - <phrase>Python</phrase> <phrase>Data</phrase> Structures o	Lists and Tuples o	Sets o	Dictionaries  Module 3 - <phrase>Python</phrase> <phrase>Programming</phrase> Fundamentals o	Conditions and Branching o	Loops o	Functions o	Objects and Classes  Module 4 - Working with <phrase>Data</phrase> in <phrase>Python</phrase> o	<phrase>Reading</phrase> files with open o	Writing files with open o	Loading <phrase>data</phrase> with <phrase>Pandas</phrase> o	<phrase>Numpy</phrase>   Finally, you will create a project to <phrase>test</phrase> your skills.  LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
We are always using experiments to improve our lives, our <phrase>community</phrase>, and our work. Are you doing it efficiently? Or are you (incorrectly) changing one thing at a time and hoping for the best?   In this course, you will learn how to plan efficient experiments - testing with many variables. Our goal is to find the best <phrase>results</phrase> using only a few experiments. A key part of the course is how to optimize a system.  We use simple tools: starting with fast calculations by hand, then we show how to use <phrase>FREE software</phrase>.   The course comes with slides, transcripts of all lectures, subtitles (<phrase>English</phrase>, <phrase>Spanish</phrase> and <phrase>Portuguese</phrase>; some <phrase>Chinese</phrase> and <phrase>French</phrase>), videos, audio files, <phrase>source code</phrase>, and a <phrase>free</phrase> <phrase>textbook</phrase>. You get to keep all of it, all freely <phrase>downloadable</phrase>.  This course is for anyone working in a <phrase>company</phrase>, or wanting to make changes to their <phrase>life</phrase>, their <phrase>community</phrase>, their <phrase>neighbourhood</phrase>. You don't need to be a <phrase>statistician</phrase> or <phrase>scientist</phrase>! There's something for everyone in here.  ⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯ Over 1000 people have completed this <phrase>online course</phrase>. What have prior students said about this course?  "This definitely is one of the most fruitful courses I have participated at <phrase>Coursera</phrase>, considering the takeaways and implementations! And so far I finished 12 [courses]."  "Excelente curso, flexible y con suficiente material didáctico fácilmente digerible y cómodo. No importa <phrase>si</phrase> <phrase>se</phrase> tiene pocas bases matemáticas o estadísticas, <phrase>el</phrase> curso proporciona casi toda explicación necesaria para un entendimiento <phrase>alto</phrase>."  "I wish I had enrolled in your course <phrase>years ago</phrase> -- it would have saved us a lot of time in optimizing <phrase>experimental</phrase> conditions." <phrase>Jason</phrase> Eriksen, 3 Jan 2017  "Interesting and developing both analytical and creative thinking. The <phrase>lecturer</phrase> took care to bring lots of real <phrase>live</phrase> examples which are fun to analyze." 20 <phrase>February 2016</phrase>.  "... <phrase>love</phrase> your style of presentation, and the examples you took from <phrase>everyday life</phrase> to explain things. It is very difficult to make such a <phrase>mathematical</phrase> course accessible and comprehensible to this wide a <phrase>variety</phrase> of people!" ⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
Presentaremos TensorFlow de bajo nivel y abordaremos las <phrase>API</phrase> y los conceptos necesarios para poder escribir modelos de aprendizaje automático distribuido. Con un modelo de TensorFlow, explicaremos cómo escalar de manera horizontal <phrase>el</phrase> entrenamiento de ese modelo y ofreceremos predicciones de <phrase>alto</phrase> rendimiento mediante <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase>.  Objetivos del curso: Crear modelos de aprendizaje automático <phrase>en</phrase> TensorFlow Usar las bibliotecas de TensorFlow para resolver problemas numéricos Solucionar problemas y depurar errores de código comunes de TensorFlow Usar tf_estimator para crear, entrenar y evaluar un modelo de <phrase>AA</phrase> Entrenar, implementar y llevar a producción modelos de <phrase>AA</phrase> a gran escala con <phrase>Cloud</phrase> <phrase>ML</phrase> <phrase>Engine</phrase>
Ever wonder how <phrase>major</phrase> <phrase>search engines</phrase> such as <phrase>Google</phrase>, Bing and <phrase>Yahoo</phrase> rank your <phrase>website</phrase> within their searches? Or how content such as videos or local listings are shown and ranked based on what the <phrase>search engine</phrase> considers most relevant to users? Welcome to the world of <phrase>Search Engine Optimization</phrase> (SEO). This course is the first within the SEO Specialization and it is intended to give you a <phrase>taste</phrase> of SEO.   You will be introduced to the foundational elements of how <phrase>search engines</phrase> work, how the SEO <phrase>landscape</phrase> has changed and what you can expect in the future. You discuss core SEO strategies and tactics used to drive more <phrase>organic</phrase> <phrase>search results</phrase> to a specific <phrase>website</phrase> or set of <phrase>websites</phrase>, as well as tactics to avoid to prevent penalization from <phrase>search engines</phrase>. You will also discover how to position yourself for a successful career in SEO should this subject prove interesting to you. We hope this <phrase>taste</phrase> of SEO, will entice you to continue through the Specialization.
Probabilistic <phrase>graphical</phrase> models (PGMs) are a rich framework for encoding <phrase>probability distributions</phrase> over complex domains: joint (multivariate) <phrase>distributions</phrase> over <phrase>large numbers</phrase> of <phrase>random variables</phrase> that interact with each other. These representations sit at the intersection of <phrase>statistics</phrase> and <phrase>computer science</phrase>, relying on concepts from <phrase>probability theory</phrase>, <phrase>graph</phrase> <phrase>algorithms</phrase>, <phrase>machine learning</phrase>, and more. They are the basis for the <phrase>state</phrase>-of-the-<phrase>art</phrase> methods in a wide <phrase>variety</phrase> of applications, such as <phrase>medical</phrase> diagnosis, <phrase>image understanding</phrase>, <phrase>speech recognition</phrase>, <phrase>natural language processing</phrase>, and many, many more. They are also a foundational tool in formulating many <phrase>machine learning</phrase> problems.   This course is the first in a <phrase>sequence</phrase> of three. It describes the two <phrase>basic</phrase> PGM representations: <phrase>Bayesian</phrase> Networks, which rely on a <phrase>directed graph</phrase>; and <phrase>Markov</phrase> networks, which use an <phrase>undirected graph</phrase>. The course discusses both the theoretical properties of these representations as well as their use in practice. The (highly recommended) honors <phrase>track</phrase> contains several hands-on assignments on how to represent some <phrase>real-world</phrase> problems. The course also presents some important extensions beyond the <phrase>basic</phrase> PGM representation, which allow more complex models to be encoded compactly.
Organizations large and small are inundated with <phrase>data</phrase> about <phrase>consumer</phrase> choices. But that wealth of <phrase>information</phrase> does not always translate into better decisions. Knowing how to interpret <phrase>data</phrase> is the challenge -- and <phrase>marketers</phrase> in particular are increasingly expected to use analytics to inform and justify their decisions.   <phrase>Marketing</phrase> analytics enables <phrase>marketers</phrase> to measure, manage and analyze <phrase>marketing</phrase> performance to maximize its effectiveness and optimize <phrase>return on investment</phrase> (ROI). Beyond the obvious sales and <phrase>lead</phrase> generation applications, <phrase>marketing</phrase> analytics can offer profound insights into customer preferences and trends, which can be further utilized for future <phrase>marketing</phrase> and <phrase>business</phrase> decisions.   This course gives you the tools to measure <phrase>brand</phrase> and customer assets, understand <phrase>regression analysis</phrase>, and <phrase>design</phrase> experiments as a way to evaluate and optimize <phrase>marketing</phrase> campaigns. You'll leave the course with a solid understanding of how to use <phrase>marketing</phrase> analytics to predict outcomes and systematically allocate resources.  For more <phrase>information</phrase> on <phrase>marketing</phrase> analytics, you may visit; http://dmanalytics.org. You can also follow my posts in <phrase>Twitter</phrase>, @rajkumarvenk, and on <phrase>linkedin</phrase>; www.linkedin.com/in/rajkumar-venkatesan-14970a3.
Leveraging the visualizations you created in the previous course, <phrase>Visual Analytics</phrase> with Tableau, you will create dashboards that help you identify the story within your <phrase>data</phrase>, and you will discover how to use Storypoints to create a powerful story to leave a lasting impression with your audience.  You will balance the goals of your stakeholders with the needs of your <phrase>end-users</phrase>, and be able to structure and organize your story for maximum impact. Throughout the course you will apply more advanced functions within Tableau, such as hierarchies, actions and parameters to guide user interactions.  For your final project, you will create a compelling <phrase>narrative</phrase> to be delivered in a meeting, as a static <phrase>report</phrase>, or in an interactive display online.
本课程 (Please click https://www.coursera.org/learn/<phrase>python</phrase>-<phrase>data</phrase>-processing for <phrase>English</phrase> version) 主要面向非计算机专业学生，从<phrase>Python</phrase>基本语法开始，到<phrase>Python</phrase>中如何从本地和<phrase>网络</phrase>上进行数据获取，如何解析和表示数据，再到如何利用<phrase>Python</phrase>开源生态系统<phrase>SciPy</phrase>对数据进行基础和高级的统计分析及可视化，到最后如何设计一个简单的<phrase>GUI</phrase>界面来表示和处理数据，层层推进。 整个课程以财经数据为基础，通过构建一个个喜闻乐见的案例，让大家可以以更直观的方式领略<phrase>Python</phrase>的简洁、优雅和健壮，同时探讨<phrase>Python</phrase>除了在商业领域之外在文学、社会学和新闻等人文社科类领域以及在数学和生物等理工类领域同样拥有便捷高效的数据处理能力，并可以触类旁通将其灵活应用于各专业中。  近期（2017年8月14日周内更新完毕）本课程进行了全面改版，新版主要在以下几个方面做了改变： 1. 由<phrase>Python</phrase> 2.x换成<phrase>Python</phrase> 3.x； 2. 增加<phrase>网络</phrase>爬虫基础实践网页爬取和解析，<phrase>Web API</phrase>等内容； 3. 其他包括调整了部分课程顺序，丰富了课程内容特别是项目实践部分的内容。
This course introduces an overview of financial analytics. You will learn why, when, and how to apply financial analytics in <phrase>real-world</phrase> situations. You will explore techniques to analyze <phrase>time series</phrase> <phrase>data</phrase> and how to evaluate the <phrase>risk</phrase>-reward <phrase>trade</phrase> off expounded in <phrase>modern portfolio theory</phrase>. While most of the focus will be on the prices, returns, and <phrase>risk</phrase> of corporate stocks, the analytical techniques can be leverages in other domains. Finally, a <phrase>short</phrase> introduction to <phrase>algorithmic trading</phrase> concludes the course.  After completing this course, you should be able to understand <phrase>time series</phrase> <phrase>data</phrase>, create forecasts, and determine the efficacy of the estimates. Also, you will be able to create a portfolio of assets using actual <phrase>stock</phrase> price <phrase>data</phrase> while optimizing <phrase>risk</phrase> and reward. Understanding financial <phrase>data</phrase> is an important skill as an analyst, <phrase>manager</phrase>, or consultant.
Ubiquitous, <phrase>on-demand</phrase> <phrase>network access</phrase> to shared pools of configurable <phrase>computing</phrase> resources ideally requires minimal <phrase>management</phrase> effort or <phrase>service provider</phrase> interaction. This course covers the essential characteristics of <phrase>data</phrase> processing in the <phrase>cloud</phrase>, service and <phrase>deployment models</phrase>, and <phrase>key components</phrase> of implementing <phrase>Amazon Web Services</phrase>, as well as constructing <phrase>Hadoop</phrase> clusters and performing <phrase>MapReduce</phrase> operations.
The purpose of this course is to summarize new directions in <phrase>Chinese</phrase> <phrase>history</phrase> and <phrase>social science</phrase> <phrase>produced</phrase> by the creation and analysis of big historical datasets based on newly opened <phrase>Chinese</phrase> archival holdings, and to organize this <phrase>knowledge</phrase> in a framework that encourages learning about <phrase>China</phrase> in comparative perspective.  Our course demonstrates how a new <phrase>scholarship</phrase> of discovery is redefining what is singular about modern <phrase>China</phrase> and modern <phrase>Chinese</phrase> <phrase>history</phrase>. Current understandings of <phrase>human</phrase> <phrase>history</phrase> and <phrase>social theory</phrase> are based largely on <phrase>Western</phrase> experience or on non-<phrase>Western</phrase> experience seen through a <phrase>Western</phrase> <phrase>lens</phrase>. This course offers <phrase>alternative</phrase> perspectives derived from <phrase>Chinese</phrase> experience over the last three centuries. We present specific <phrase>case</phrase> studies of this new <phrase>scholarship</phrase> of discovery divided into two <phrase>stand-alone</phrase> parts, which means that students can take any part without prior or subsequent attendance of the other part.  Part 1 (https://www.coursera.org/learn/understanding-<phrase>china</phrase>-<phrase>history</phrase>-part-1) focuses on comparative inequality and opportunity and addresses two related questions ‘Who rises to the top?’ and ‘Who gets what?’.   Part 2 (this course) turns to an arguably even more important question ‘Who are we?’ as seen through the framework of comparative <phrase>population</phrase> behavior - mortality, <phrase>marriage</phrase>, and reproduction – and their interaction with economic conditions and <phrase>human</phrase> values. We do so because mortality and reproduction are fundamental and <phrase>universal</phrase>, because they differ historically just as radically between <phrase>China</phrase> and the West as patterns of inequality and opportunity, and because these differences demonstrate the mutability of <phrase>human</phrase> behavior and values.  Course Overview <phrase>video</phrase>: https://youtu.be/dzUPRyJ4ETk
In this course, you will gain a thorough understanding of the blockchain and distributed ledger technologies, including <phrase>an introduction</phrase> to the necessary foundations in <phrase>cryptography</phrase>. The course will discuss blockchain as a distributed ledger and <phrase>introduce</phrase> distributed consensus as a mechanism to maintain the <phrase>integrity</phrase> of the blockchain. The other revolutionary technologies that are changing the world as we speak are <phrase>artificial intelligence</phrase> and <phrase>machine learning</phrase>. You will learn about the three <phrase>major</phrase> types of <phrase>AI</phrase> <phrase>algorithms</phrase>: supervised and unsupervised <phrase>machine learning</phrase>, as well as <phrase>reinforcement learning</phrase>.   You will learn about the <phrase>application</phrase> of blockchain outside of <phrase>finance</phrase>. In particular, how blockchain fundamentally changes the way we deal with our <phrase>personal data</phrase>. You will see how the <phrase>web 2.0</phrase> <phrase>model</phrase>--where big companies like <phrase>facebook</phrase> and <phrase>google</phrase> collect as much of your <phrase>personal data</phrase> as possible to sell it to <phrase>third parties</phrase>--is coming to an end. The new <phrase>web 3.0</phrase> is decentralized and uses the power of the blockchain to put users in full control over their own <phrase>data</phrase>. Finally, we will look at the benefits and considerations of blockchain and whether blockchain is the right <phrase>solution</phrase> for your problem.  #UCTFintech
В этом курсе мы разберем основные методы описания взаимосвязей между количественными признаками. Если корреляционный анализ позволяет количественно оценить силу и направление связи между двумя величинами, то построение регрессионных моделей дает более широкие возможности. При помощи регрессионного анализа можно количественно описывать поведение изучаемых величин в зависимости от переменных-предикторов и получать предсказания на новых данных. Вы узнаете, как строить простые и множественные линейные модели с использованием языка R. У всякого метода есть свои ограничения, поэтому мы поможем вам разобраться, в каких ситуациях можно, а в каких нельзя применять линейную регрессию, и научим вас методам диагностики подобранных моделей. Специальное место в курсе отводится глубинной анатомии регрессионного анализа: вы освоите операции с матрицами, которые лежат в основе линейной регрессии, чтобы получить возможность разбираться в более сложных разновидностях линейных моделей.  Если вы сталкиваетесь с необходимостью поиска и описания взаимосвязей между теми или иными явлениями, которые могут быть измерены количественно, тогда этот курс - хорошая возможность понять, как устроены простая и множественная линейная регрессия, узнать о возможностях и ограничениях этих методов. Курс рассчитан на тех, кто уже знаком с базовыми приемами анализа данных с использованием языка R и с созданием простейших .<phrase>html</phrase> документов при помощи rmarkdown и knitr.
Welcome to the specialization course of Designing <phrase>data</phrase>-intensive applications.  This course will be completed on four weeks, it will be supported with videos and exercises.  By the end of this specialization, learners will be able to propose, <phrase>design</phrase>, justify and develop <phrase>high</phrase> reliable <phrase>information</phrase> systems according to type of <phrase>data</phrase> and volume of <phrase>information</phrase>, <phrase>response time</phrase>, type of processing and queries in <phrase>order</phrase> to support <phrase>scalability</phrase>, <phrase>maintainability</phrase>, <phrase>security</phrase> and <phrase>reliability</phrase> considering the last <phrase>information</phrase> technologies.  <phrase>Software</phrase> to download: <phrase>MySQL</phrase> <phrase>Workbench</phrase>  Rapidminer <phrase>Hadoop</phrase> framework Hortonworks <phrase>MongoDB</phrase>  In <phrase>case</phrase> you have a <phrase>Mac</phrase> / <phrase>IOS</phrase> <phrase>operating system</phrase> you need to perform an <phrase>action</phrase> called <phrase>VirtualBox</phrase>.
Курс включает рассмотрение всех основных этапов статистического анализа, начиная от изучения предметной области и правильного сбора данных, заканчивая оценкой адекватности построенной модели и ее интерпретации на языке исходной проблемы. Программа курса построена таким образом, что, начинаясь с основ, будет понятна и доступна слушателям, не имеющим специальной подготовки в области статистического анализа. Однако, по мере продвижения и углубления, в рамках программы рассматриваются более серьезные и глубокие методы исследования. В рамках курса слушатели приобретут базовые навыки работы в программах статистической обработки данных <phrase>SPSS</phrase>, Statistica; особый акцент делается на пакет R. Возможна разработка заданий различной сложности для слушателей различного уровня подготовки.  Цель: ознакомить слушателей с основными статистическими методами, применяемыми при анализе данных в различных областях гуманитарных наук, психологии, социологии, лингвистики и пр., научить решать задачи статистического анализа данных, начиная от формулирования исходных задач соответствующей предметной области на языке прикладной статистики, выбора методов решения и критериев качества полученных решений и заканчивая формулировкой полученных выводов на языке предметной области.   По итогам курса слушатели смогут: 1. Проводить предварительную обработку данных для статистических исследований 2. Применять статистические методы для анализа данных 3. Применять пакеты прикладных программ для реализации статистических методов 4. Интерпретировать полученные результаты  Сертификат о прохождении данного курса дает дополнительные баллы при поступлении в магистратуру Национального исследовательского Томского государственного университета. Перечень магистерских программ находится по ссылке: https://pro-online.tsu.ru/edu/<phrase>student</phrase>/table.php
The <phrase>Library</phrase> of Integrative Network-based Cellular Signatures (LINCS) is an <phrase>NIH</phrase> Common Fund program. The idea is to perturb different types of <phrase>human</phrase> cells with many different types of perturbations such as: <phrase>drugs</phrase> and other small <phrase>molecules</phrase>; <phrase>genetic</phrase> manipulations such as knockdown or overexpression of <phrase>single</phrase> <phrase>genes</phrase>; manipulation of the <phrase>extracellular</phrase> microenvironment conditions, for example, growing cells on different surfaces, and more. These perturbations are applied to various types of <phrase>human</phrase> cells including induced pluripotent <phrase>stem cells</phrase> from patients, differentiated into various lineages such as <phrase>neurons</phrase> or cardiomyocytes. Then, to better understand the molecular networks that are affected by these perturbations, changes in level of many different variables are measured including: <phrase>mRNAs</phrase>, <phrase>proteins</phrase>, and <phrase>metabolites</phrase>, as well as cellular <phrase>phenotypic</phrase> changes such as changes in <phrase>cell</phrase> <phrase>morphology</phrase>. The BD2K-LINCS <phrase>Data</phrase> Coordination and <phrase>Integration</phrase> <phrase>Center</phrase> (DCIC) is <phrase>commissioned</phrase> to organize, analyze, visualize and integrate this <phrase>data</phrase> with other <phrase>publicly available</phrase> relevant resources. In this course we briefly <phrase>introduce</phrase> the DCIC and the various Centers that <phrase>collect data</phrase> for LINCS. We then <phrase>cover</phrase> <phrase>metadata</phrase> and how <phrase>metadata</phrase> is linked to <phrase>ontologies</phrase>. We then present <phrase>data</phrase> processing and normalization methods to clean and harmonize LINCS <phrase>data</phrase>. This follow discussions about how <phrase>data</phrase> is served as RESTful <phrase>APIs</phrase>. Most importantly, the course covers <phrase>computational methods</phrase> including: <phrase>data</phrase> clustering, <phrase>gene-set enrichment analysis</phrase>, interactive <phrase>data</phrase> visualization, and <phrase>supervised learning</phrase>. Finally, we <phrase>introduce</phrase> <phrase>crowdsourcing</phrase>/<phrase>citizen-science</phrase> projects where students can work together in teams to extract expression signatures from <phrase>public</phrase> <phrase>databases</phrase> and then query such collections of signatures against LINCS <phrase>data</phrase> for predicting small <phrase>molecules</phrase> as potential therapeutics.
This module aims at introducing fundamental concepts of <phrase>visual perception</phrase> applied to <phrase>information visualization</phrase>. These concepts help the <phrase>student</phrase> ideate and evaluate visualization designs in terms of how well they leverage the capabilities of the <phrase>human</phrase> <phrase>perceptual</phrase> machinery.
This course will help us to evaluate and compare the models we have developed in previous courses. So far we have developed techniques for <phrase>regression</phrase> and classification, but how low should the error of a classifier be (for example) before we decide that the classifier is "good enough"? Or how do we decide which of two <phrase>regression</phrase> <phrase>algorithms</phrase> is better?  By the end of this course you will be familiar with diagnostic techniques that allow you to evaluate and compare classifiers, as well as <phrase>performance measures</phrase> that can be used in different <phrase>regression</phrase> and classification scenarios. We will also study the training/validation/<phrase>test</phrase> <phrase>pipeline</phrase>, which can be used to ensure that the models you develop will generalize well to new (or "unseen") <phrase>data</phrase>.
In this course you will learn how to use survey weights to estimate <phrase>descriptive statistics</phrase>, like means and totals, and more complicated quantities like <phrase>model</phrase> parameters for linear and logistic regressions.  <phrase>Software</phrase> capabilities will be <phrase>covered</phrase> with R® receiving particular emphasis.  The course will also <phrase>cover</phrase> the basics of <phrase>record linkage</phrase> and statistical matching—both of which are becoming more important as ways of combining <phrase>data</phrase> from different sources.  Combining of datasets raises <phrase>ethical</phrase> issues which the course reviews.  <phrase>Informed consent</phrase> may have to be obtained from persons to allow their <phrase>data</phrase> to be linked. You will learn about differences in the <phrase>legal requirements</phrase> in different countries.
Qu'est-<phrase>ce</phrase> que le <phrase>machine learning</phrase> et quels types de problèmes permet-il de résoudre ? <phrase>Google</phrase> adopte une approche particulière du <phrase>machine learning</phrase> qui s'appuie non seulement sur <phrase>les</phrase> données, mais également sur <phrase>la</phrase> logique. Nous expliquerons l'intérêt que présente cette conception pour <phrase>la</phrase> création d'un <phrase>pipeline</phrase> de modèles de <phrase>ML</phrase>. Ensuite, nous examinerons <phrase>les</phrase> cinq phases permettant de convertir un <phrase>cas</phrase> d'utilisation devant être traité à l'aide du <phrase>machine learning</phrase> et étudierons pourquoi chaque étape est importante. Enfin, nous identifierons <phrase>les</phrase> biais que le <phrase>machine learning</phrase> est susceptible d'amplifier et apprendrons à <phrase>les</phrase> repérer.
<phrase>SAS</phrase> Viya is an in-<phrase>memory</phrase> <phrase>distributed environment</phrase> used to analyze <phrase>big data</phrase> quickly and efficiently. In this course, you’ll learn how to use the <phrase>SAS</phrase> Viya <phrase>APIs</phrase> to take control of <phrase>SAS</phrase> <phrase>Cloud</phrase> Analytic Services from a Jupyter Notebook using R or <phrase>Python</phrase>. You’ll learn to <phrase>upload</phrase> <phrase>data</phrase> into the <phrase>cloud</phrase>, analyze <phrase>data</phrase>, and create predictive models with <phrase>SAS</phrase> Viya using familiar <phrase>open source</phrase> functionality via the <phrase>SWAT</phrase> package -- the <phrase>SAS</phrase> <phrase>Scripting</phrase> Wrapper for Analytics Transfer. You’ll learn how to create both <phrase>machine learning</phrase> and <phrase>deep learning</phrase> models to tackle a <phrase>variety</phrase> of <phrase>data</phrase> sets and <phrase>complex problems</phrase>. And once <phrase>SAS</phrase> Viya has done the heavy lifting, you’ll be able to download <phrase>data</phrase> to the client and use <phrase>native</phrase> <phrase>open source</phrase> <phrase>syntax</phrase> to compare <phrase>results</phrase> and create <phrase>graphics</phrase>.
In this third course of the specialization, we’ll drill deeper into the tools Tableau offers in the areas of charting, dates, table calculations and mapping. We’ll explore the best choices for charts, based on the type of <phrase>data</phrase> you are using. We’ll look at specific types of charts including scatter plots, Gantt charts, histograms, bullet charts and several others, and we’ll <phrase>address</phrase> charting guidelines. We’ll define discrete and continuous dates, and examine when to use each one to explain your <phrase>data</phrase>.  You’ll learn how to create custom and quick table calculations and how to create parameters. We’ll also <phrase>introduce</phrase> mapping and explore how Tableau can use different types of <phrase>geographic data</phrase>, how to connect to multiple <phrase>data</phrase> sources and how to create custom maps.
This one-week course describes the process of <phrase>analyzing data</phrase> and how to manage that process. We describe the iterative <phrase>nature</phrase> of <phrase>data analysis</phrase> and the role of stating a sharp question, <phrase>exploratory data analysis</phrase>, inference, formal <phrase>statistical modeling</phrase>, interpretation, and <phrase>communication</phrase>. In addition, we will describe how to direct analytic activities within a team and to drive the <phrase>data</phrase> analysis process towards coherent and useful <phrase>results</phrase>.   This is a focused course designed to rapidly get you up to speed on the process of <phrase>data analysis</phrase> and how it can be managed. Our goal was to make this as convenient as possible for you without sacrificing any essential content. We've left the <phrase>technical information</phrase> aside so that you can focus on managing your team and moving it <phrase>forward</phrase>.  After completing this course you will know how to….  1. Describe the <phrase>basic</phrase> <phrase>data analysis</phrase> <phrase>iteration</phrase> 2. Identify different types of questions and translate them to specific datasets 3. Describe different types of <phrase>data</phrase> pulls 4. Explore datasets to determine if <phrase>data</phrase> are appropriate for a given question 5. Direct <phrase>model</phrase> building efforts in common <phrase>data</phrase> analyses 6. Interpret the <phrase>results</phrase> from common <phrase>data</phrase> analyses 7. Integrate statistical findings to form coherent <phrase>data</phrase> analysis presentations  Commitment: 1 week of study, 4-6 hours  Course <phrase>cover</phrase> image by fdecomite. <phrase>Creative Commons</phrase> BY https://flic.kr/p/4HjmvD
This course introduces students to the <phrase>science</phrase> of <phrase>business</phrase> analytics while casting a keen eye toward the artful use of numbers found in the <phrase>digital</phrase> space. The goal is to provide businesses and managers with the foundation needed to apply <phrase>data</phrase> analytics to <phrase>real-world</phrase> challenges they confront <phrase>daily</phrase> in their <phrase>professional</phrase> lives. Students will learn to identify the ideal analytic tool for their specific needs; understand valid and reliable ways to collect, analyze, and visualize <phrase>data</phrase>; and utilize <phrase>data</phrase> in <phrase>decision making</phrase> for their agencies, organizations or clients.
In the capstone, students will build a series of applications to retrieve, process and visualize <phrase>data</phrase> using <phrase>Python</phrase>.   The projects will involve all the elements of the specialization.  In the first part of the capstone, students will do some visualizations to become familiar with the technologies in use and then will pursue their own project to visualize some other <phrase>data</phrase> that they have or can find.  Chapters 15 and 16 from the <phrase>book</phrase> “<phrase>Python</phrase> for Everybody” will serve as the backbone for the capstone. This course covers <phrase>Python</phrase> 3.
People analytics is a <phrase>data-driven</phrase> approach to managing people at work. For the first time in <phrase>history</phrase>, <phrase>business</phrase> leaders can make decisions about their people based on deep analysis of <phrase>data</phrase> rather than the <phrase>traditional methods</phrase> of personal relationships, <phrase>decision making</phrase> based on experience, and <phrase>risk</phrase> avoidance. In this <phrase>brand</phrase> new course, three of Wharton’s top professors, all pioneers in the field of people analytics, will explore the <phrase>state</phrase>-of-the-<phrase>art</phrase> techniques used to recruit and retain great people, and demonstrate how these techniques are used at <phrase>cutting-edge</phrase> companies. They’ll explain how <phrase>data</phrase> and sophisticated analysis is brought to <phrase>bear</phrase> on people-<phrase>related issues</phrase>, such as recruiting, <phrase>performance evaluation</phrase>, <phrase>leadership</phrase>, hiring and promotion, job <phrase>design</phrase>, compensation, and collaboration. This course is <phrase>an introduction</phrase> to the theory of people analytics, and is not intended to prepare learners to perform complex <phrase>talent management</phrase> <phrase>data</phrase> analysis. By the end of this course, you’ll understand how and when hard <phrase>data</phrase> is used to make soft-skill decisions about hiring and talent development, so that you can position yourself as a strategic partner in your company’s <phrase>talent management</phrase> decisions. This course is intended to introduced you to Organizations flourish when the people who work in them flourish. Analytics can help make both happen. This course in People Analytics is designed to help you flourish in your career, too.
This course will <phrase>introduce</phrase> the learner to <phrase>text mining</phrase> and text manipulation basics. The course begins with an understanding of how text is handled by <phrase>python</phrase>, the structure of text both to the machine and to humans, and an overview of the nltk framework for manipulating text. The second week focuses on common manipulation needs, including <phrase>regular expressions</phrase> (<phrase>searching</phrase> for text), cleaning text, and preparing text for use by <phrase>machine learning</phrase> processes. The third week will apply <phrase>basic</phrase> <phrase>natural language processing</phrase> methods to text, and demonstrate how text classification is accomplished. The final week will explore more advanced methods for detecting the topics in documents and grouping them by similarity (topic modelling).   This course should be taken after: Introduction to <phrase>Data Science</phrase> in <phrase>Python</phrase>, Applied Plotting, Charting & <phrase>Data</phrase> Representation in <phrase>Python</phrase>, and Applied <phrase>Machine Learning</phrase> in <phrase>Python</phrase>.
An increasing volume of <phrase>data</phrase> is becoming available in <phrase>biomedicine</phrase> and <phrase>healthcare</phrase>, from <phrase>genomic</phrase> <phrase>data</phrase>, to <phrase>electronic</phrase> patient records and <phrase>data</phrase> collected by wearable devices. <phrase>Recent advances</phrase> in <phrase>data science</phrase> are transforming the <phrase>life</phrase> sciences, leading to precision <phrase>medicine</phrase> and stratified <phrase>healthcare</phrase>.   In this course, you will learn about some of the different types of <phrase>data</phrase> and <phrase>computational methods</phrase> involved in stratified <phrase>healthcare</phrase> and precision <phrase>medicine</phrase>.  You will have a hands-on experience of working with such <phrase>data</phrase>.  And you will learn from leaders in the field about successful <phrase>case</phrase> studies.   Topics include: (i) <phrase>Sequence</phrase> Processing, (<phrase>ii</phrase>) <phrase>Image Analysis</phrase>, (<phrase>iii</phrase>) Network Modelling, (<phrase>iv</phrase>) Probabilistic Modelling, (v) <phrase>Machine Learning</phrase>, (vi) <phrase>Natural Language</phrase> Processing, (<phrase>vii</phrase>) <phrase>Process Modelling</phrase> and (viii) <phrase>Graph</phrase> <phrase>Data</phrase>.  <phrase>Watch</phrase> the course promo <phrase>video</phrase> here: http://edin.ac/2pn350P
<phrase>La</phrase> digitalización, <phrase>la</phrase> informática <phrase>e</phrase> <phrase>Internet</phrase> <phrase>han</phrase> producido lo que <phrase>se</phrase> puede denominar una revolución <phrase>en</phrase> <phrase>la</phrase> acumulación y utilización de datos. Podemos almacenar y conservar más datos que nunca antes <phrase>en</phrase> <phrase>la</phrase> historia. Podemos estudiarlos y analizarlos para tomar <phrase>decisiones</phrase> y mejorar procesos. Esta nueva capacidad tiene un enorme impacto <phrase>en</phrase> todos los ámbitos <phrase>de la</phrase> vida social.  A lo largo de este curso:  •	Conoceremos qué <phrase>es</phrase> <phrase>el</phrase> <phrase>Big Data</phrase> y cuáles son sus características fundamentales •	Exploraremos <phrase>el</phrase> crecimiento continuo de datos, analizaremos <phrase>el</phrase> impacto potencial <phrase>en</phrase> muchos campos <phrase>de la</phrase> actividad humana y <phrase>nos</phrase> preguntaremos por los retos y desafíos que suponen <phrase>en</phrase> todos los órdenes <phrase>de la</phrase> vida social.  •	Conoceremos las características de cada una de las fases del procesamiento <phrase>Big Data</phrase>, adquiriendo un lenguaje adecuado para <phrase>la</phrase> descripción de los procesos. Dispondremos así de una visión de conjunto sobre sistema de tratamiento de grandes datos <phrase>en</phrase> <phrase>la</phrase> actualidad. •	Conoceremos las principales áreas de aplicación de los datos masivos. Qué tipos de transformaciones están imponiendo <phrase>en</phrase> <phrase>la</phrase> organización del trabajo y <phrase>en</phrase> <phrase>la</phrase> gestión. Qué desafíos imponen <phrase>en</phrase> <phrase>la</phrase> gobernanza, <phrase>la</phrase> economía y <phrase>el</phrase> trabajo. Qué mejoras introducen y qué riesgos representan. •	Estudiaremos las principales tecnologías <phrase>e</phrase> infraestructuras para <phrase>el</phrase> almacenamiento y procesado de grandes volúmenes de datos.
This course will <phrase>introduce</phrase> the core <phrase>data</phrase> structures of the <phrase>Python programming language</phrase>. We will move past the basics of <phrase>procedural programming</phrase> and explore how we can use the <phrase>Python</phrase> built-in <phrase>data</phrase> structures such as lists, dictionaries, and tuples to perform <phrase>increasingly complex</phrase> <phrase>data</phrase> analysis. This course will <phrase>cover</phrase> Chapters 6-10 of the <phrase>textbook</phrase> “<phrase>Python</phrase> for Everybody”.  This course covers <phrase>Python</phrase> 3.
This course will teach you the "magic" of getting <phrase>deep learning</phrase> to work well. Rather than the <phrase>deep learning</phrase> process being a <phrase>black</phrase> box, you will understand what drives performance, and be able to more systematically get good <phrase>results</phrase>. You will also learn TensorFlow.   After 3 weeks, you will:  - Understand <phrase>industry</phrase> <phrase>best-practices</phrase> for building <phrase>deep learning</phrase> applications.  - Be able to effectively use the common <phrase>neural network</phrase> "tricks", including initialization, <phrase>L2</phrase> and dropout regularization, Batch normalization, <phrase>gradient</phrase> checking,  - Be able to implement and apply a <phrase>variety</phrase> of <phrase>optimization algorithms</phrase>, such as <phrase>mini</phrase>-batch <phrase>gradient</phrase> descent, <phrase>Momentum</phrase>, RMSprop and <phrase>Adam</phrase>, and check for their convergence.  - Understand new <phrase>best-practices</phrase> for the <phrase>deep learning</phrase> <phrase>era</phrase> of how to set up <phrase>train</phrase>/dev/<phrase>test</phrase> sets and analyze <phrase>bias/variance</phrase> - Be able to implement a <phrase>neural network</phrase> in TensorFlow.   This is the second course of the <phrase>Deep Learning</phrase> Specialization.
<phrase>Confidence intervals</phrase> and <phrase>Hypothesis</phrase> <phrase>tests</phrase> are very important tools in the <phrase>Business</phrase> <phrase>Statistics</phrase> toolbox. A mastery over these topics will help enhance your <phrase>business</phrase> <phrase>decision making</phrase> and allow you to understand and measure the extent of ‘<phrase>risk</phrase>’ or ‘uncertainty’ in various <phrase>business</phrase> processes.  This is the third course in the specialization "<phrase>Business</phrase> <phrase>Statistics</phrase> and Analysis" and the course  advances your <phrase>knowledge</phrase> about <phrase>Business</phrase> <phrase>Statistics</phrase> by introducing you to <phrase>Confidence Intervals</phrase> and <phrase>Hypothesis</phrase> Testing. We first conceptually understand these tools and their <phrase>business</phrase> <phrase>application</phrase>. We then <phrase>introduce</phrase> various calculations to constructing <phrase>confidence intervals</phrase> and to conduct different kinds of <phrase>Hypothesis</phrase> <phrase>Tests</phrase>. These are done by <phrase>easy to understand</phrase> applications.  To successfully complete course assignments, students must have access to a <phrase>Windows</phrase> version of <phrase>Microsoft Excel</phrase> 2010 or later. Please note that <phrase>earlier versions</phrase> of <phrase>Microsoft Excel</phrase> (2007 and earlier) will not be compatible to some <phrase>Excel</phrase> functions <phrase>covered</phrase> in this course.    WEEK 1 Module 1: <phrase>Confidence Interval</phrase> - Introduction In this module you will get to conceptually understand what a <phrase>confidence interval</phrase> is and how is its constructed. We will <phrase>introduce</phrase> the various <phrase>building blocks</phrase> for the <phrase>confidence interval</phrase> such as the t-distribution, the t-statistic, the z-statistic and their various <phrase>excel</phrase> formulas. We will then use these <phrase>building blocks</phrase> to construct <phrase>confidence intervals</phrase>.  Topics <phrase>covered</phrase> include: •	Introducing the t-distribution, the T.DIST and T.INV <phrase>excel</phrase> functions •	Conceptual understanding of a <phrase>Confidence Interval</phrase> •	The z-statistic and the t-statistic •	Constructing a <phrase>Confidence Interval</phrase> using z-statistic and t-statistic    WEEK 2 Module 2: <phrase>Confidence Interval</phrase> - Applications This module presents various <phrase>business</phrase> applications of the <phrase>confidence interval</phrase> including an <phrase>application</phrase> where we use the <phrase>confidence interval</phrase> to calculate an appropriate <phrase>sample size</phrase>. We also <phrase>introduce</phrase> with an <phrase>application</phrase>, the <phrase>confidence interval</phrase> for a <phrase>population</phrase> proportion. Towards the close of module we start introducing the concept of <phrase>Hypothesis</phrase> Testing.  Topics <phrase>covered</phrase> include: •	Applications of <phrase>Confidence Interval</phrase> •	<phrase>Confidence Interval</phrase> for a <phrase>Population</phrase> Proportion •	<phrase>Sample Size</phrase> Calculation •	<phrase>Hypothesis</phrase> Testing, <phrase>An Introduction</phrase>   WEEK 3 Module 3: <phrase>Hypothesis</phrase> Testing This module introduces <phrase>Hypothesis</phrase> Testing. You get to understand the <phrase>logic</phrase> behind <phrase>hypothesis</phrase> <phrase>tests</phrase>. The four steps for <phrase>conducting</phrase> a <phrase>hypothesis</phrase> <phrase>test</phrase> are introduced and you get to apply them for <phrase>hypothesis</phrase> <phrase>tests</phrase> for a <phrase>population</phrase> mean as well as <phrase>population</phrase> proportion. You will understand the difference between <phrase>single</phrase> tail <phrase>hypothesis</phrase> <phrase>tests</phrase> and two tail <phrase>hypothesis</phrase> <phrase>tests</phrase> and also the Type I and <phrase>Type II</phrase> errors associated with <phrase>hypothesis</phrase> <phrase>tests</phrase> and ways to reduce such errors.   Topics <phrase>covered</phrase> include: •	The <phrase>Logic</phrase> of <phrase>Hypothesis</phrase> Testing •	The Four Steps for <phrase>conducting</phrase> a <phrase>Hypothesis</phrase> <phrase>Test</phrase> •	<phrase>Single</phrase> Tail and Two Tail <phrase>Hypothesis</phrase> <phrase>Tests</phrase> •	Guidelines, Formulas and an <phrase>Application</phrase> of <phrase>Hypothesis</phrase> <phrase>Test</phrase> •	<phrase>Hypothesis</phrase> <phrase>Test</phrase> for a <phrase>Population</phrase> Proportion •	Type I and <phrase>Type II</phrase> Errors in a <phrase>Hypothesis</phrase>    WEEK 4 Module 4: <phrase>Hypothesis</phrase> <phrase>Test</phrase> - Differences in Mean In this module, you'll apply <phrase>Hypothesis</phrase> <phrase>Tests</phrase> to <phrase>test</phrase> the difference between two different <phrase>data</phrase>, such <phrase>hypothesis</phrase> <phrase>tests</phrase> are called difference in means <phrase>tests</phrase>. We will <phrase>introduce</phrase> the three kinds of difference in <phrase>means test</phrase> and apply them to various <phrase>business</phrase> applications. We will also <phrase>introduce</phrase> the <phrase>Excel</phrase> <phrase>dialog box</phrase> to conduct such <phrase>hypothesis</phrase> <phrase>tests</phrase>.  Topics <phrase>covered</phrase> include: •	Introducing the Difference-In-Means <phrase>Hypothesis</phrase> <phrase>Test</phrase> •	Applications of the Difference-In-Means <phrase>Hypothesis</phrase> <phrase>Test</phrase> •	The Equal & Unequal <phrase>Variance</phrase> Assumption and the Paired t-<phrase>test</phrase> for difference in means. •	Some more applications
Welcome to the second course in the <phrase>Data</phrase> Analytics for <phrase>Business</phrase> specialization!   This course will <phrase>introduce</phrase> you to some of the most widely used predictive <phrase>modeling techniques</phrase> and their core principles. By taking this course, you will form a solid foundation of <phrase>predictive analytics</phrase>, which refers to tools and techniques for building statistical or <phrase>machine learning</phrase> models to make predictions based on <phrase>data</phrase>. You will learn how to <phrase>carry out</phrase> <phrase>exploratory data analysis</phrase> to gain insights and prepare <phrase>data</phrase> for predictive modeling, an essential skill valued in the <phrase>business</phrase>.   You’ll also learn how to summarize and visualize datasets using plots so that you can present your <phrase>results</phrase> in a compelling and meaningful way. We will use a practical predictive modeling <phrase>software</phrase>, XLMiner, which is a popular <phrase>Excel</phrase> plug-in. This course is designed for anyone who is interested in using <phrase>data</phrase> to gain insights and make better <phrase>business</phrase> decisions. The techniques discussed are applied in all functional areas within <phrase>business</phrase> organizations including <phrase>accounting</phrase>, <phrase>finance</phrase>, <phrase>human resource management</phrase>, <phrase>marketing</phrase>, operations, and <phrase>strategic planning</phrase>.   The expected prerequisites for this course include a prior working <phrase>knowledge</phrase> of <phrase>Excel</phrase>, introductory level <phrase>algebra</phrase>, and <phrase>basic</phrase> <phrase>statistics</phrase>.
This course is for <phrase>business</phrase> analysts and <phrase>SAS</phrase> programmers who want to learn <phrase>data</phrase> manipulation techniques using the <phrase>SAS</phrase> <phrase>DATA</phrase> <phrase>step</phrase> and procedures to access, transform, and summarize <phrase>data</phrase>. The course builds on the concepts that are presented in the <phrase>Getting Started</phrase> with <phrase>SAS</phrase> <phrase>Programming</phrase> course and is not recommended for beginning <phrase>SAS</phrase> <phrase>software</phrase> users.  In this course you learn how to understand and <phrase>control DATA</phrase> <phrase>step</phrase> processing, create an accumulating column and process <phrase>data</phrase> in groups, manipulate <phrase>data</phrase> with functions, convert column type, create custom formats, concatenate and merge tables, process repetitive code, and restructure tables. This course addresses Base <phrase>SAS</phrase> <phrase>software</phrase>.  Before attending this course, you should be able to write <phrase>DATA</phrase> <phrase>step</phrase> code to access <phrase>data</phrase>, <phrase>subset</phrase> <phrase>rows and columns</phrase>, compute new columns, and process <phrase>data</phrase> conditionally. You should also be able to sort tables using the SORT procedure and apply <phrase>SAS</phrase> formats.
This course covers commonly used <phrase>statistical inference</phrase> methods for numerical and categorical <phrase>data</phrase>. You will learn how to set up and perform <phrase>hypothesis</phrase> <phrase>tests</phrase>, interpret p-values, and <phrase>report</phrase> the <phrase>results</phrase> of your analysis in a way that is interpretable for clients or the <phrase>public</phrase>. Using numerous <phrase>data</phrase> examples, you will learn to <phrase>report</phrase> estimates of quantities in a way that expresses the uncertainty of the quantity of interest. You will be guided through installing and using R and RStudio (<phrase>free</phrase> statistical <phrase>software</phrase>), and will use this <phrase>software</phrase> for lab exercises and a final project. The course introduces practical tools for performing <phrase>data</phrase> analysis and explores the fundamental concepts necessary to interpret and <phrase>report</phrase> <phrase>results</phrase> for both categorical and numerical <phrase>data</phrase>
This course introduces the <phrase>Bayesian</phrase> approach to <phrase>statistics</phrase>, starting with the concept of <phrase>probability</phrase> and moving to the analysis of <phrase>data</phrase>. We will learn about the <phrase>philosophy</phrase> of the <phrase>Bayesian</phrase> approach as well as how to implement it for common types of <phrase>data</phrase>. We will compare the <phrase>Bayesian</phrase> approach to the more commonly-taught Frequentist approach, and see some of the benefits of the <phrase>Bayesian</phrase> approach. In particular, the <phrase>Bayesian</phrase> approach allows for better <phrase>accounting</phrase> of uncertainty, <phrase>results</phrase> that have more intuitive and interpretable meaning, and more explicit statements of assumptions. This course combines lecture videos, <phrase>computer</phrase> demonstrations, readings, exercises, and discussion boards to create an <phrase>active learning</phrase> experience. For <phrase>computing</phrase>, you have the choice of using <phrase>Microsoft Excel</phrase> or the <phrase>open-source</phrase>, <phrase>freely available</phrase> statistical package R, with equivalent content for both options. The lectures provide some of the <phrase>basic</phrase> <phrase>mathematical</phrase> development as well as explanations of <phrase>philosophy</phrase> and interpretation. Completion of this course will give you an understanding of the concepts of the <phrase>Bayesian</phrase> approach, understanding the key <phrase>differences between</phrase> <phrase>Bayesian</phrase> and Frequentist approaches, and the ability to do <phrase>basic</phrase> <phrase>data</phrase> analyses.
This course will expose you to the <phrase>data</phrase> analytics practices executed in the <phrase>business</phrase> world. We will explore such key areas as the analytical process, how <phrase>data</phrase> is created, stored, accessed, and how the <phrase>organization</phrase> works with <phrase>data</phrase> and creates the environment in which analytics can flourish.  What you learn in this course will give you a strong foundation in all the areas that support analytics and will help you to better position yourself for success within your <phrase>organization</phrase>. You’ll develop skills and a perspective that will make you more productive faster and allow you to become a valuable asset to your <phrase>organization</phrase>.  This course also provides a basis for going deeper into advanced investigative and <phrase>computational methods</phrase>, which you have an opportunity to explore in future courses of the <phrase>Data</phrase> Analytics for <phrase>Business</phrase> specialization.
This course allows you to apply the <phrase>SQL</phrase> skills taught in “<phrase>SQL</phrase> for <phrase>Data Science</phrase>” to four <phrase>increasingly complex</phrase> and authentic <phrase>data science</phrase> inquiry <phrase>case</phrase> studies. We'll learn how to convert timestamps of all types to common formats and perform <phrase>date/time</phrase> calculations. We'll select and perform the optimal JOIN for a <phrase>data science</phrase> inquiry and clean <phrase>data</phrase> within an analysis dataset by deduping, running quality checks, backfilling, and handling nulls. We'll learn how to segment and analyze <phrase>data</phrase> per segment using windowing functions and <phrase>use case</phrase> statements to execute conditional <phrase>logic</phrase> to <phrase>address</phrase> a <phrase>data science</phrase> inquiry. We'll also describe how to convert a query into a scheduled job and how to insert <phrase>data</phrase> into a date <phrase>partition</phrase>. Finally, given a predictive analysis need, we'll <phrase>engineer</phrase> a feature from <phrase>raw data</phrase> using the tools and skills we've built over the course. The <phrase>real-world</phrase> <phrase>application</phrase> of these skills will give you the framework for performing the analysis of an <phrase>AB</phrase> <phrase>test</phrase>.
This is the third course of the Advanced <phrase>Machine Learning</phrase> on GCP specialization. In this course, We will take a look at different strategies for building an image classifier using <phrase>convolutional neural networks</phrase>. We'll improve the model's accuracy with augmentation, <phrase>feature extraction</phrase>, and <phrase>fine-tuning</phrase> hyperparameters while trying to avoid <phrase>overfitting</phrase> our <phrase>data</phrase>. We will also look at <phrase>practical issues</phrase> that arise, for example, when you don’t have enough <phrase>data</phrase> and how to incorporate the latest <phrase>research</phrase> findings into our models.  You will get hands-on practice building and optimizing your own <phrase>image classification</phrase> models on a <phrase>variety</phrase> of <phrase>public</phrase> datasets in the labs we’ll work on together.    Prerequisites: <phrase>Basic</phrase> <phrase>SQL</phrase>, familiarity with <phrase>Python</phrase> and TensorFlow
A learner will be able to write an <phrase>application</phrase> that leverages multiple Watson <phrase>AI</phrase> services (Discovery, Speech to Text, Assistant, and <phrase>Text to Speech</phrase>). By the end of the course, they’ll learn <phrase>best practices</phrase> of combining Watson services, and how they can build interactive <phrase>information retrieval</phrase> systems with Discovery + Assistant.
This course immerses learners in <phrase>deep learning</phrase>, preparing them to solve <phrase>computer vision</phrase> problems. Learners plunge into the field of <phrase>computer vision</phrase> that deals with recognizing, identifying and understanding <phrase>visual information</phrase> from visual <phrase>data</phrase>, whether the <phrase>information</phrase> is from a <phrase>single</phrase> image or <phrase>video</phrase> <phrase>sequence</phrase>. Topics include object detection, <phrase>face detection</phrase> and recognition (using Adaboost and Eigenfaces), and the progression of <phrase>deep learning</phrase> techniques (<phrase>CNN</phrase>, AlexNet, REsNet, and Generative Models.)          This course is ideal for anyone curious about or interested in exploring the concepts of visual recognition and <phrase>deep learning</phrase> <phrase>computer vision</phrase>. Learners should have <phrase>basic</phrase> <phrase>programming</phrase> skills and experience (understanding of for loops, if/else statements), specifically in <phrase>MATLAB</phrase> (<phrase>free</phrase> introductory <phrase>tutorial</phrase>: https://www.mathworks.com/learn/tutorials/<phrase>matlab</phrase>-onramp.html). Learners should also be familiar with the following: <phrase>basic</phrase> <phrase>linear algebra</phrase> (matrix <phrase>vector</phrase> operations and <phrase>notation</phrase>), 3D co-ordinate systems and transformations, <phrase>basic</phrase> <phrase>calculus</phrase> (derivatives and <phrase>integration</phrase>) and <phrase>basic</phrase> <phrase>probability</phrase> (<phrase>random variables</phrase>). It is highly recommended that learners take the “<phrase>Deep Learning</phrase> Onramp” course available at https://matlabacademy.mathworks.com/.     Material includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing <phrase>computer vision</phrase> programs through online labs using <phrase>MATLAB</phrase>* and supporting toolboxes.  This is the fourth course in the <phrase>Computer</phrase> Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a <phrase>video</phrase> overview at https://youtu.be/OfxVUSCPXd0.    * A <phrase>free</phrase> license to <phrase>install</phrase> <phrase>MATLAB</phrase> for the duration of the course is available from <phrase>MathWorks</phrase>.
If I Googled you, what would I find?  As we move around the online world we leave tracks and traces of our <phrase>activity</phrase> all the time: <phrase>social media</phrase> accounts, tagged images, <phrase>professional</phrase> presences, scraps of text, but also many artefacts we don't always realise we are leaving behind, or that others leave about us.    In this course you will hear from a <phrase>range</phrase> of experts and you will have an opportunity to explore and reflect on your own online tracks and traces, to understand why your <phrase>digital</phrase> footprint is important. We will <phrase>introduce</phrase> you to some of the tools and approaches to effectively manage your online presence (or <phrase>digital</phrase> footprint).    The course will focus on the different dimensions of a <phrase>digital</phrase> footprint, including developing an effective online presence, managing your <phrase>privacy</phrase>, creating opportunities for networking, balancing and managing <phrase>professional</phrase> and personal presences (eprofessionalism). By the end of this course (<phrase>MOOC</phrase>) you should be equipped to ensure that your <phrase>digital</phrase> footprint works for you, whether you want to be more <phrase>private</phrase> online, or are looking to create a more effective and impactful presence.    You can also join the conversation on <phrase>Twitter</phrase> using the <phrase>hashtag</phrase> #DFMOOC and follow us @DFMOOC  We hope you enjoy the course!
The capstone course, <phrase>Design</phrase> and Build a <phrase>Data Warehouse</phrase> for <phrase>Business Intelligence</phrase> Implementation, features a <phrase>real-world</phrase> <phrase>case</phrase> study that integrates your learning across all courses in the specialization. In response to <phrase>business</phrase> requirements presented in a <phrase>case</phrase> study, you’ll <phrase>design</phrase> and build a small <phrase>data warehouse</phrase>, create <phrase>data integration</phrase> workflows to refresh the <phrase>warehouse</phrase>, write <phrase>SQL</phrase> statements to support analytical and <phrase>summary</phrase> query requirements, and use the <phrase>MicroStrategy</phrase> <phrase>business intelligence</phrase> platform to create dashboards and visualizations.  In the first part of the capstone course, you’ll be introduced to a <phrase>medium-sized</phrase> <phrase>firm</phrase>, learning about their <phrase>data warehouse</phrase> and <phrase>business intelligence</phrase> requirements and existing <phrase>data</phrase> sources. You’ll first <phrase>architect</phrase> a <phrase>warehouse</phrase> schema and <phrase>dimensional model</phrase> for a small <phrase>data warehouse</phrase>. You’ll then create <phrase>data integration</phrase> workflows using Pentaho <phrase>Data Integration</phrase> to refresh your <phrase>data warehouse</phrase>. Next, you’ll write <phrase>SQL</phrase> statements for analytical query requirements and create materialized views to support <phrase>summary</phrase> <phrase>data management</phrase>. Finally, you will use <phrase>MicroStrategy</phrase> <phrase>OLAP</phrase> capabilities to gain insights into your <phrase>data warehouse</phrase>. In the completed project, you’ll have built a small <phrase>data warehouse</phrase> containing a schema <phrase>design</phrase>, <phrase>data integration</phrase> workflows, analytical queries, materialized views, dashboards and visualizations that you’ll be proud to show to your current and prospective employers.
Este curso intensivo sob demanda tem duração de <phrase>uma</phrase> semana <phrase>e</phrase> foi elaborado com base <phrase>nos</phrase> cursos <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> <phrase>e</phrase> <phrase>Machine Learning</phrase> Fundamentals. Com videoaulas, demonstrações <phrase>e</phrase> laboratórios práticos, você aprenderá a criar canais de dados de <phrase>streaming</phrase> usando o <phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub <phrase>e</phrase> o <phrase>Dataflow</phrase> para a tomada de decisões <phrase>em</phrase> <phrase>tempo</phrase> real. Você também aprenderá a criar painéis com conteúdo personalizado para vários públicos.  Pré-requisitos: • conclusão <phrase>dos</phrase> cursos <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> <phrase>e</phrase> <phrase>Machine Learning</phrase> Fundamentals (ou experiência equivalente) • conhecimentos básicos de <phrase>Java</phrase>  Objetivos: • compreender casos de análise de <phrase>streaming</phrase> <phrase>em</phrase> <phrase>tempo</phrase> real • usar o serviço de mensagens assíncronas do <phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub para gerenciar eventos de dados • escrever canais de <phrase>streaming</phrase> <phrase>e</phrase> fazer transformações quando necessário • conhecer as duas vias de um <phrase>canal</phrase> de <phrase>streaming</phrase>: produção <phrase>e</phrase> consumo • usar o <phrase>Dataflow</phrase>, o BigQuery <phrase>e</phrase> o <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub juntos para análise <phrase>e</phrase> <phrase>streaming</phrase> <phrase>em</phrase> <phrase>tempo</phrase> real
إذا كنت ترغب في اختراق عالم الذكاء الاصطناعي شديد التطور، فسوف تساعدك هذه الدورة التدريبية على تحقيق ذلك. إن مهندسي التعلم العميق مطلوبون بشدة، كما أن إتقان التعلم العميق يمنحك العديد من فرص المستقبل المهني الجديدة. إن التعلم العميق يعد بمثابة "قوة عظمى" جديدة كذلك تساعدك على بناء أنظمة الذكاء الاصطناعي التي لم يكن بالإمكان الوصول إليها منذ عدة سنوات قليلة مضت.   في هذه الدورة التدريبية، سوف تتعرف على أسس التعلم العميق. عندما تنتهي من هذا الفصل الدراسي، سيكون بإمكانك ما يلي: - فهم اتجاهات التقنيات الرئيسية التي تدفع التفكير العميق قدمًا - التمكن من بناء شبكات عصبية متصلة بشكل كامل وتتسم بالعمق وتدريب تلك الشبكات وتطبيقها  - إدراك كيفية تنفيذ الشبكات العصبية الفعالة (الموجهة)  - إدراك المعاملات الرئيسية في بنية الشبكة العصبية   كما تعلمك هذه الدورة التدريبية كذلك كيف يعمل التعلم العميق بشكل فعلي كذلك، بدلاً من تقديم وصف سريع أو سطحي فقط. لذا، بعد إكمال هذه الدورة التدريبية، سوف تكون قادرًا على تطبيق التعلم العميق على التطبيقات الخاصة بك. إذا كنت تبحث عن وظيفة في مجال الذكاء الاصطناعي، بعد إتمام هذه الدورة التدريبية، سوف تكون قادرًا كذلك على الإجابة على أسئلة المقابلات الشخصية الأساسية.   هذه الدورة التدريبية هي الأولى في تخصص التعلم العميق.
مقدمة عن البيانات الضخمة هل أنت مهتم بزيادة معرفتك بأبرز سمات البيانات الضخمة؟ هذه الدورة التدريبية مخصصة للمستجدين في علوم البيانات والمهتمين بفهم أسباب ظهور عصر البيانات الضخمة. فهي مخصصة لمن يريدون الإلمام بالمصطلحات والمفاهيم الأساسية الخاصة بمشكلات البيانات الضخمة وتطبيقاتها وأنظمتها. إنها لمن يريدون البدء في التفكير بشأن الطريقة التي يمكن أن تفيدهم البيانات الضخمة بها في عملهم أو مسيرتهم المهنية. حيث تتعرض مقدمة عن أحد أكثر أطر العمل الشائعة ألا وهو <phrase>Hadoop</phrase>، والذي زاد من سهولة تحليل البيانات الضخمة وإمكانية الوصول إليها، فقد زاد من احتمالية تطوير البيانات الضخمة لعالمنا!  وفي نهاية الدورة التدريبية، ستتمكن مما يلي:  *  وصف أبرز سمات البيانات الضخمة بما في ذلك الأمثلة على مشكلات البيانات الضخمة على أرض الواقع التي تتضمن ثلاثة مصادر أساسية للبيانات الضخمة وهي الأفراد والمؤسسات وأدوات الاستشعار.  * شرح خصائص البيانات الضخمة التي تبدأ بالحرف V مثل (volume (الحجم)، وvelocity (السرعة)، وvariety (التنوع)، وveracity (الصحة)، وvalence (التكافؤ)، وvalue (القيمة)) ولماذا تؤثر كل خاصية من تلك الخصائص في جمع البيانات ومتابعتها وتخزينها وتحليلها والإبلاغ عنها  * الاستفادة بقيمة البيانات الضخمة عن طريق استخدام عملية مكونة من 5 خطوات لهيكلة تحليلك.   * تحديد المشكلات التي تندرج تحت البيانات الضخمة والتي لا تندرج تحتها، والقدرة على إعادة تشكيل مشكلات البيانات الضخمة مثل مسائل علوم البيانات.  * تقديم تفسير للمكونات الهندسية والنماذج البرمجية التي تستخدم في التحليل القابل للتوسيع للبيانات الضخمة.  * تلخيص ميزات المكونات الأساسية لمكدس <phrase>Hadoop</phrase> وقيمتها بما في ذلك مورد <phrase>YARN</phrase> ونظام إدارة الوظائف، ونظام ملفات HDFS، ونموذج برمجة <phrase>MapReduce</phrase>.  * تثبيت البرامج وتشغيلها باستخدام إطار عمل <phrase>Hadoop</phrase>!  هذه الدورة التدريبية موجهة للمستجدين في علوم البيانات.  لا يلزم توافر خبرة برمجية مسبقة، على الرغم من ضرورة توافر القدرة على تثبيت التطبيقات واستخدام الأجهزة الظاهرية لإنجاز الواجبات العملية.    متطلبات الأجهزة: (أ) معالج رباعي النواة (يوصى بمعالج يدعم ميزة <phrase>VT</phrase>-x أو <phrase>AMD</phrase>-V)، 64 بت؛ (ب) ذاكرة وصول عشوائي بحجم 8 جيجابايت؛ (ج) مساحة خالية بحجم 20 جيجابايت.  طريقة العثور على معلومات الأجهزة: (نظام <phrase>Windows</phrase>): افتح النظام عن طريق الضغط على زر Start (بدء التشغيل)، وانقر بزر الفأرة الأيمن على أيقونة <phrase>Computer</phrase> (جهاز الكمبيوتر)، ثم انقر على Properties (خصائص)؛ (نظام <phrase>Mac</phrase>): افتح Overview (نظرة عامة) عن طريق الضغط على قائمة <phrase>Apple</phrase> والنقر على "About This <phrase>Mac</phrase>." سيتوفر الحد الأدنى من المتطلبات في معظم أجهزة الكمبيوتر ذات الذاكرة العشوائية سعة 8 جيجابايت والتي تم شراؤها في آخر 3 أعوام. وستحتاج إلى سرعة اتصال عالية بالإنترنت لأنك ستقوم بتنزيل ملفات يصل حجمها إلى 4 جيجابايت.  المتطلبات البرمجية: تعتمد هذه الدورة التدريبية على العديد من الأدوات البرمجية مفتوحة المصدر، ومنها <phrase>Apache Hadoop</phrase>. ويمكن تنزيل جميع البرامج المطلوبة وتثبيتها مجانًا. تتضمن المتطلبات البرمجية ما يلي: <phrase>Windows</phrase> 7+ أو <phrase>Mac OS</phrase> X 10.10+ أو <phrase>Ubuntu</phrase> 14.04+ أو <phrase>CentOS</phrase> 6+ <phrase>VirtualBox</phrase> 5+.
The <phrase>data science</phrase> <phrase>revolution</phrase> has <phrase>produced</phrase> reams of new <phrase>data</phrase> from a wide <phrase>variety</phrase> of new sources. These new datasets are being used to answer new questions in way never before conceived. Visualization remains one of the most powerful ways draw conclusions from <phrase>data</phrase>, but the influx of new <phrase>data</phrase> types requires the development of new visualization techniques and <phrase>building blocks</phrase>. This course provides you with the skills for creating those new visualization <phrase>building blocks</phrase>. We focus on the ggplot2 framework and describe how to use and extend the system to suit the specific needs of your <phrase>organization</phrase> or team. Upon completing this course, learners will be able to build the tools needed to visualize a wide <phrase>variety</phrase> of <phrase>data</phrase> types and will have the fundamentals needed to <phrase>address</phrase> new <phrase>data</phrase> types as they come about.
Курс направлен на овладение статистическими методами и моделями, необходимыми для самостоятельного анализа данных количественных исследований в психологии и смежных областях, на формирование умений пользоваться простыми методами анализа данных, правильно выбирать и интерпретировать результаты применения сложных технологий анализа данных, на развитие навыков корректной интерпретации измерений и результатов их статистического анализа.  По завершении курса учащиеся будут: Знать систему математических моделей исследования – измерительных, описательных и статистического вывода; систему многомерных математических методов и моделей анализа данных; назначение каждого элемента этих систем с точки зрения границ его применения и интерпретации результатов. Уметь планировать исследование с учетом возможностей измерений и анализа количественных результатов; применять на практике одномерные и двумерные методы анализа данных; правильно выбирать адекватный эмпирическим данным и задачам исследования многомерный метод анализа и исчерпывающе интерпретировать результаты его применения; Владеть системой статистических понятий, статистическим дискурсом, как необходимой составной частью методологического дискурса психологии и смежных дисциплин.
<phrase>En</phrase> este curso integrarás lo aprendido <phrase>en</phrase> los cursos que componen esta especialidad. A partir <phrase>de la</phrase> resolución de casos <phrase>en</phrase> diferentes industrias representativas, utilizarás <phrase>el</phrase> módulo de ensamble <phrase>en</phrase> Watson Analytics a efecto de conjuntar <phrase>la</phrase> información que obtengas <phrase>en</phrase> <phrase>la</phrase>  exploración y las predicciones realizadas con tus datos. <phrase>El</phrase> producto final <phrase>es</phrase> un informe de negocio con dashboards <phrase>e</phrase> infográficos interactivos que explicarán <phrase>el</phrase> problema de negocio <phrase>en</phrase> estos casos selectos.  Agradecemos a Fundación Televisa por <phrase>su</phrase> participación <phrase>en</phrase> <phrase>la</phrase> producción de este curso; con lo cual colabora a inspirar y desarrollar <phrase>el</phrase> potencial de las personas, a través de <phrase>su</phrase> compromiso con <phrase>la</phrase> educación y <phrase>la</phrase> cultura.
The <phrase>Business</phrase> Analytics Capstone Project gives you the opportunity to apply what you've learned about how to make <phrase>data</phrase>-driven decisions to a real <phrase>business</phrase> challenge faced by global <phrase>technology</phrase> companies like <phrase>Yahoo</phrase>, <phrase>Google</phrase>, and <phrase>Facebook</phrase>. At the end of this Capstone, you'll be able to ask the right questions of the <phrase>data</phrase>, and know how to use <phrase>data</phrase> effectively to <phrase>address</phrase> <phrase>business</phrase> challenges of your own. You’ll understand how <phrase>cutting-edge</phrase> businesses use <phrase>data</phrase> to optimize <phrase>marketing</phrase>, maximize <phrase>revenue</phrase>, make operations efficient, and make hiring and <phrase>management</phrase> decisions so that you can apply these strategies to your own <phrase>company</phrase> or <phrase>business</phrase>. Designed with <phrase>Yahoo</phrase> to give you invaluable experience in evaluating and creating <phrase>data</phrase>-driven decisions, the <phrase>Business</phrase> Analytics Capstone Project provides the chance for you to devise a plan of <phrase>action</phrase> for optimizing <phrase>data</phrase> itself to provide key insights and analysis, and to describe the interaction between key financial and non-financial indicators. Once you complete your analysis, you'll be better prepared to make better <phrase>data</phrase>-driven <phrase>business</phrase> decisions of your own.
This course covers the <phrase>design</phrase>, acquisition, and analysis of <phrase>Functional Magnetic Resonance Imaging</phrase> (<phrase>fMRI</phrase>) <phrase>data</phrase>.  A <phrase>book</phrase> related to the class can be found here: https://leanpub.com/principlesoffmri
This course provides a rigorous introduction to the R <phrase>programming language</phrase>, with a  particular focus on using R for <phrase>software development</phrase> in a <phrase>data science</phrase> setting. Whether you are part of a <phrase>data science</phrase> team or working individually within a <phrase>community</phrase> of developers, this course will give you the <phrase>knowledge</phrase> of R needed to make useful contributions in those settings. As the first course in the Specialization, the course provides the essential foundation of R needed for the following courses. We <phrase>cover</phrase> <phrase>basic</phrase> R concepts and <phrase>language</phrase> fundamentals, <phrase>key concepts</phrase> like tidy <phrase>data</phrase> and related "tidyverse" tools, processing and manipulation of complex and <phrase>large datasets</phrase>, handling <phrase>textual data</phrase>, and <phrase>basic</phrase> <phrase>data science</phrase> tasks. Upon completing this course, learners will have fluency at the R <phrase>console</phrase> and will be able to create tidy datasets from a wide <phrase>range</phrase> of possible <phrase>data</phrase> sources.
This course aims at introducing the fundamental concepts of <phrase>Reinforcement Learning</phrase> (<phrase>RL</phrase>), and develop <phrase>use cases</phrase> for applications of <phrase>RL</phrase> for <phrase>option</phrase> valuation, trading, and <phrase>asset management</phrase>.   By the end of this course, students will be able to - Use <phrase>reinforcement learning</phrase> to solve <phrase>classical</phrase> problems of <phrase>Finance</phrase> such as portfolio optimization, optimal trading, and <phrase>option</phrase> pricing and <phrase>risk management</phrase>. - Practice on valuable examples such as famous <phrase>Q-learning</phrase> using financial problems. - Apply their <phrase>knowledge</phrase> acquired in the course to a simple <phrase>model</phrase> for market dynamics that is obtained using <phrase>reinforcement learning</phrase> as the course project.  Prerequisites are the courses "Guided <phrase>Tour</phrase> of <phrase>Machine Learning</phrase> in <phrase>Finance</phrase>" and "Fundamentals of <phrase>Machine Learning</phrase> in <phrase>Finance</phrase>". Students are expected to know the lognormal process and how it can be simulated. <phrase>Knowledge</phrase> of <phrase>option</phrase> pricing is not assumed but desirable.
This course is all about presenting the story of the <phrase>data</phrase>, using <phrase>PowerPoint</phrase>. You'll learn how to structure a presentation, to include insights and supporting <phrase>data</phrase>. You'll also learn some <phrase>design</phrase> principles for effective visuals and slides. You'll gain skills for client-facing <phrase>communication</phrase> - including <phrase>public</phrase> speaking, executive presence and compelling <phrase>storytelling</phrase>. Finally, you'll be given a client profile, a <phrase>business</phrase> problem, and a set of <phrase>basic</phrase> <phrase>Excel</phrase> charts, which you'll need to turn into a presentation - which you'll deliver with iterative peer <phrase>feedback</phrase>.  This course was created by <phrase>PricewaterhouseCoopers</phrase> LLP with an <phrase>address</phrase> at 300 <phrase>Madison</phrase> Avenue, <phrase>New York</phrase>, <phrase>New York</phrase>, 10017.
One of the skills that characterizes great <phrase>business</phrase> <phrase>data</phrase> analysts is the ability to communicate practical implications of quantitative analyses to any kind of audience <phrase>member</phrase>.  Even the most sophisticated statistical analyses are not useful to a <phrase>business</phrase> if they do not <phrase>lead</phrase> to actionable <phrase>advice</phrase>, or if the answers to those <phrase>business</phrase> questions are not conveyed in a way that non-technical people can understand.    In this course you will learn how to become a <phrase>master</phrase> at communicating <phrase>business</phrase>-relevant implications of <phrase>data</phrase> analyses.  By the end, you will know how to structure your <phrase>data analysis</phrase> projects to ensure the <phrase>fruits</phrase> of your hard <phrase>labor</phrase> yield <phrase>results</phrase> for your stakeholders.  You will also know how to <phrase>streamline</phrase> your analyses and highlight their implications efficiently using visualizations in Tableau, the most popular visualization program in the <phrase>business</phrase> world.  Using other Tableau features, you will be able to make effective visualizations that <phrase>harness</phrase> the <phrase>human</phrase> brain’s innate <phrase>perceptual</phrase> and <phrase>cognitive</phrase> tendencies to convey conclusions directly and clearly.  Finally, you will be practiced in designing and persuasively presenting <phrase>business</phrase> “<phrase>data</phrase> stories” that use these visualizations, capitalizing on <phrase>business</phrase>-tested methods and <phrase>design</phrase> principles.
This 1-week, accelerated course builds upon previous courses in the <phrase>Data</phrase> <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform specialization. Through a combination of <phrase>video</phrase> lectures, demonstrations, and hands-on labs, you'll learn how to create and manage <phrase>computing</phrase> clusters to <phrase>run</phrase> <phrase>Hadoop</phrase>, Spark, <phrase>Pig</phrase> and/or Hive jobs on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform.  You will also learn how to access various <phrase>cloud storage</phrase> options from their compute clusters and integrate Google’s <phrase>machine learning</phrase> capabilities into their analytics programs.    In the hands-on labs, you will create and manage Dataproc Clusters using the Web <phrase>Console</phrase> and the <phrase>CLI</phrase>, and use cluster to <phrase>run</phrase> Spark and <phrase>Pig</phrase> jobs. You will then create iPython notebooks that integrate with BigQuery and storage and utilize Spark. Finally, you integrate the <phrase>machine learning</phrase> <phrase>APIs</phrase> into your <phrase>data analysis</phrase>.  Pre-requisites • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> & <phrase>Machine Learning</phrase> Fundamentals (or equivalent experience) • Some <phrase>knowledge</phrase> of <phrase>Python</phrase>
Машинное обучение (<phrase>Machine Learning</phrase>, или <phrase>ML</phrase>) — это дисциплина о том, как на основе различных алгоритмов обучить компьютер распознавать, классифицировать и предсказывать объекты. Машинное обучение подарило нам эффективный поиск и персонализированный контент в интернете, а в последнее время активно используется в финансах и банковской сфере — наш курс именно об этом!  Применение методов <phrase>ML</phrase> помогает банку более оперативно принимать решения. Сможет ли вернуть кредит конкретный клиент? Как изменится объем вкладов и кредитов в ближайшей перспективе? Как оптимизировать внутренние процессы?  Эти и многие другие проблемы финансовой сферы помогают решать на практике передовые методы <phrase>ML</phrase>.  Если вы студент и видите свое будущее в <phrase>ML</phrase> в финансах, но еще не до конца понимаете, чем будете заниматься; или уже работаете в банковской/IT сфере и хотите улучшить свои знания и квалификацию, а может быть, вы просто активно интересуетесь последними тенденциями применения <phrase>ML</phrase> — добро пожаловать на онлайн-курс «Машинное обучение в финансах» от команды финансистов Сбербанка!  Наш курс практико-ориентированный: вы узнаете о внедрении и применении <phrase>ML</phrase> на примере трейдинга, прогнозировании операционного дохода банка, автоматизации внутренних процессов и др., а также пройдете несколько практических заданий с использованием языка программирования <phrase>Python</phrase>. На второй неделе курса используется вероятностный язык программирования Stan. В лекциях и домашних заданиях по прогнозированию представлены базовые примеры моделей в Stan и ссылки на более детальное ознакомление с языком. Освоив эту программу, слушатель научится применять на практике многие методы <phrase>ML</phrase> и получит конкурентное преимущество для трудоустройства в финансовой и IT сфере.
This course will expose you to the transformation <phrase>taking place</phrase>, throughout the world, in the way that <phrase>products</phrase> are being designed and manufactured. The transformation is happening through <phrase>digital</phrase> <phrase>manufacturing</phrase> and <phrase>design</phrase> (DM&D) – a shift from <phrase>paper</phrase>-based processes to <phrase>digital</phrase> processes in the <phrase>manufacturing</phrase> <phrase>industry</phrase>. By the end of this course, you’ll understand what DMD is and how it is impacting careers, practices and processes in companies both large and small.    You will gain an understanding of and appreciation for the role that <phrase>technology</phrase> is playing in this <phrase>transition</phrase>. The <phrase>technology</phrase> we use every day – whether it is communicating with <phrase>friends</phrase> and <phrase>family</phrase>, purchasing <phrase>products</phrase> or <phrase>streaming</phrase> <phrase>entertainment</phrase> – can benefit <phrase>design</phrase> and <phrase>manufacturing</phrase>, making companies and workers more competitive, <phrase>agile</phrase> and productive. Discover how this new approach to making <phrase>products</phrase> makes companies more responsive, and employees more involved and engaged, as new career paths in advanced <phrase>manufacturing</phrase> evolve.  Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the first course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,” aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
This course covers the <phrase>theoretical foundation</phrase> for different techniques associated with supervised <phrase>machine learning</phrase> models. In addition, a <phrase>business case</phrase> study is defined to guide participants through all steps of the analytical <phrase>life</phrase> cycle, from problem understanding to <phrase>model</phrase> deployment, through <phrase>data</phrase> preparation, <phrase>feature selection</phrase>, <phrase>model</phrase> training and validation, and <phrase>model</phrase> assessment. A series of demonstrations and exercises is used to reinforce the concepts and the analytical approach to solving <phrase>business</phrase> problems.   This course uses <phrase>Model</phrase> Studio, the <phrase>pipeline</phrase> flow interface in <phrase>SAS</phrase> Viya that enables you to prepare, develop, compare, and deploy advanced analytics models. You learn to <phrase>train</phrase> supervised <phrase>machine learning</phrase> models to make better decisions on <phrase>big data</phrase>. The <phrase>SAS</phrase> applications used in this course make <phrase>machine learning</phrase> possible without <phrase>programming</phrase> or coding.
This course will teach you how to build <phrase>convolutional neural networks</phrase> and apply it to <phrase>image data</phrase>. Thanks to <phrase>deep learning</phrase>, <phrase>computer vision</phrase> is working far better than just two <phrase>years ago</phrase>, and this is enabling numerous exciting applications ranging from safe autonomous driving, to accurate <phrase>face recognition</phrase>, to automatic <phrase>reading</phrase> of <phrase>radiology</phrase> images.   You will: - Understand how to build a <phrase>convolutional neural network</phrase>, including recent variations such as residual networks. - Know how to apply convolutional networks to visual detection and recognition tasks. - Know to use neural style transfer to generate <phrase>art</phrase>. - Be able to apply these <phrase>algorithms</phrase> to a <phrase>variety</phrase> of image, <phrase>video</phrase>, and other 2D or 3D <phrase>data</phrase>.  This is the fourth course of the <phrase>Deep Learning</phrase> Specialization.
<phrase>Machine learning</phrase> is the <phrase>science</phrase> of getting <phrase>computers</phrase> to <phrase>act</phrase> without being explicitly programmed. In the past decade, <phrase>machine learning</phrase> has given us <phrase>self-driving cars</phrase>, practical <phrase>speech recognition</phrase>, effective <phrase>web search</phrase>, and a vastly improved understanding of <phrase>the human genome</phrase>. <phrase>Machine learning</phrase> is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards <phrase>human</phrase>-level <phrase>AI</phrase>. In this class, you will learn about the most effective <phrase>machine learning</phrase> techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of <phrase>Silicon</phrase> Valley's <phrase>best practices</phrase> in <phrase>innovation</phrase> as it pertains to <phrase>machine learning</phrase> and <phrase>AI</phrase>.  This course provides a broad introduction to <phrase>machine learning</phrase>, datamining, and statistical <phrase>pattern recognition</phrase>. Topics include: (i) <phrase>Supervised learning</phrase> (parametric/non-parametric <phrase>algorithms</phrase>, <phrase>support vector machines</phrase>, kernels, <phrase>neural networks</phrase>). (<phrase>ii</phrase>) <phrase>Unsupervised learning</phrase> (clustering, <phrase>dimensionality reduction</phrase>, <phrase>recommender systems</phrase>, <phrase>deep learning</phrase>). (<phrase>iii</phrase>) <phrase>Best practices</phrase> in <phrase>machine learning</phrase> (<phrase>bias/variance</phrase> theory; <phrase>innovation</phrase> process in <phrase>machine learning</phrase> and <phrase>AI</phrase>). The course will also draw from numerous <phrase>case</phrase> studies and applications, so that you'll also learn how to apply <phrase>learning algorithms</phrase> to building smart <phrase>robots</phrase> (<phrase>perception</phrase>, control), text understanding (<phrase>web search</phrase>, <phrase>anti</phrase>-spam), <phrase>computer vision</phrase>, <phrase>medical</phrase> informatics, audio, <phrase>database</phrase> <phrase>mining</phrase>, and other areas.
Organizaciones tan diferentes como <phrase>Amazon</phrase>, Cemex o Costco utilizan <phrase>el</phrase> análisis de datos para una mejor toma de <phrase>decisiones</phrase> que <phrase>les</phrase> permita identificar a sus clientes más rentables, retener y contratar talento o bien optimizar sus cadenas de suministro. Lo cierto <phrase>es</phrase> que las organizaciones compiten ahora tomando <phrase>decisiones</phrase> basadas <phrase>en</phrase> <phrase>la</phrase> explotación de los datos y uso de métodos estadísticos. <phrase>Sin</phrase> <phrase>embargo</phrase>, <phrase>la</phrase> forma <phrase>en</phrase> que estos datos <phrase>se</phrase> obtienen, manipulan y utilizan, puede definir <phrase>el</phrase> fracaso o <phrase>el</phrase> éxito de estas organizaciones.  <phrase>El</phrase> primer paso antes de convertir estos datos <phrase>en</phrase> información que agregue valor a <phrase>la</phrase> organización <phrase>es</phrase> recolectar y limpiar dichos datos. <phrase>En</phrase> este curso podrás reconocer y recolectar los tipos de datos disponibles de manera estructurada y no estructurada <phrase>en</phrase> <phrase>el mundo</phrase> de negocios actual,  así como preparar, aplicar y limpiar estos datos para <phrase>su</phrase> posterior explotación. Adicionalmente, serás capaz de realizar una exploración de los datos que <phrase>te</phrase> permita elaborar un análisis preliminar del proceso de negocio bajo estudio.  Al  terminar este curso habrás desarrollado <phrase>la</phrase> capacidad de comprender y aplicar de manera efectiva las metodologías y técnicas relacionadas con <phrase>la</phrase> recopilación de datos, utilizando un <phrase>software</phrase> de última generación, como <phrase>el</phrase> Watson Analytics.  Agradecemos a Fundación Televisa por <phrase>su</phrase> participación <phrase>en</phrase> <phrase>la</phrase> producción de este curso; con lo cual colabora a inspirar y desarrollar <phrase>el</phrase> potencial de las personas, a través de <phrase>su</phrase> compromiso con <phrase>la</phrase> educación y <phrase>la</phrase> cultura.
This is the fourth course in the <phrase>Data Warehouse</phrase> for <phrase>Business Intelligence</phrase> specialization. Ideally, the courses should be taken in <phrase>sequence</phrase>.  In this course, you will gain the <phrase>knowledge</phrase> and skills for using <phrase>data</phrase> warehouses for <phrase>business intelligence</phrase> purposes and for working as a <phrase>business intelligence</phrase> developer. You’ll have the opportunity to work with <phrase>large data sets</phrase> in a <phrase>data warehouse</phrase> environment and will learn the use of MicroStrategy's <phrase>Online Analytical Processing</phrase> (<phrase>OLAP</phrase>) and Visualization capabilities to create visualizations and dashboards.   The course gives an overview of how <phrase>business intelligence</phrase> technologies can support <phrase>decision making</phrase> across any number of <phrase>business</phrase> sectors. These technologies have had a profound impact on corporate strategy, performance, and competitiveness and broadly encompass  <phrase>decision support systems</phrase>, <phrase>business intelligence</phrase> systems, and <phrase>visual analytics</phrase>. Modules are organized around the <phrase>business intelligence</phrase> concepts, tools, and applications, and the use of <phrase>data warehouse</phrase> for <phrase>business</phrase> reporting and <phrase>online analytical processing</phrase>, for creating visualizations and dashboards, and for <phrase>business</phrase> <phrase>performance management</phrase> and descriptive analytics.
本課程是為非資料科學專業者設計的大數據領域入門課程，偏商管應用，非資訊技術教學。透過修習本課程，學員將能對資料科學商管領域的範疇與分類建立基本的觀念，並且瞭解其在商管領域的各種應用。在學的學生可藉此為職涯做準備，在職的社會人士則可拓展自己對資料科學的想像，進一步思考在自身工作場域應用資料科學的可能性。  本課程共計六週，第一週為學界與業界對談，透過直播企劃呈現大數據應用的議題，作為課程的開端，二到五週由臺灣大學教授進行授課，分別就金融、行銷、社群媒體、輿情分析、行銷智慧等議題，介紹大數據在領域的應用，課程以闡述應用為主，但不會花很多時間在演算法的技術細節。第六週則由玉山金控李正國數位金融長主講，帶入玉山金控積極應用大數據於銀行業的策略，產學合作課程確實結合學界與業界的專家，就資料科學的商管應用做不同面向的介紹。  課程設計中安排一位主持人的課前提問、單元介紹引言、延伸提問等等，引導學生學習與思考，各週授課教師與課程主題概述如下：  第一週：臺灣大學資訊管理學系魏志平教授、玉山金控李正國數位金融長 -- 課程簡介、與大數據的午餐約會直播活動 第二週：臺灣大學工商管理學系與資訊管理學系合聘楊立偉教授 -- 資料分析在金融及財務上的應用 第三週：臺灣大學工商管理學系與資訊管理學系合聘楊立偉教授 -- 資料分析在零售及行銷上的應用 第四週：臺灣大學資訊管理學系陳建錦教授 -- 社群媒體之輿情分析 第五週：臺灣大學資訊管理學系魏志平教授 -- 社群媒體分析與行銷智慧 第六週：玉山金控李正國數位金融長 -- 大數據的商業應用策略
<phrase>Database Management</phrase> Essentials provides the foundation you need for a career in <phrase>database</phrase> development, <phrase>data</phrase> warehousing, or <phrase>business intelligence</phrase>, as well as for the entire <phrase>Data</phrase> Warehousing for <phrase>Business Intelligence</phrase> specialization. In this course, you will create <phrase>relational databases</phrase>, write <phrase>SQL</phrase> statements to extract <phrase>information</phrase> to satisfy <phrase>business</phrase> reporting requests, create <phrase>entity relationship</phrase> diagrams (ERDs) to <phrase>design</phrase> <phrase>databases</phrase>, and analyze table designs for excessive redundancy. As you develop these skills, you will use either <phrase>Oracle</phrase> or <phrase>MySQL</phrase> to execute <phrase>SQL</phrase> statements and a <phrase>database</phrase> diagramming tool such as the <phrase>ER</phrase> Assistant or Visual <phrase>Paradigm</phrase> to create ERDs. We’ve designed this course to ensure a common foundation for specialization learners. Everyone taking the course can jump right in with writing <phrase>SQL</phrase> statements in <phrase>Oracle</phrase> or <phrase>MySQL</phrase>.
Objetivos Generales: Al finalizar <phrase>el</phrase> curso, podrás:  1.- ENTENDER y profundizar convenientemente aspectos específicos de   diferentes formas de trabajo (individual o grupal),  2.- EVALUAR <phrase>el</phrase> uso de funciones avanzadas para manipular datos y CREAR tus propios análisis utilizando técnicas específicas tales como tablas dinámicas, análisis de hipótesis, administración de escenarios, tablas de simple y doble entrada, análisis de optimización de recursos.  3.- ANALIZAR cómo vincular <phrase>Excel</phrase> con otras aplicaciones importando información desde archivos de texto y bases de datos, exportando información a archivos de texto de diversas formas.  4.- ENTENDER  <phrase>el</phrase> uso de macros lo que <phrase>te</phrase> permitirá vislumbrar otro universo de aplicaciones que harán mucho más productiva <phrase>tu</phrase> <phrase>labor</phrase>.  Los ejemplos sobre los cuales <phrase>se</phrase> apoyan los contenidos dictados por los profesores, tienen una profunda aplicabilidad al mundo de los negocios, con lo que <phrase>su</phrase> inmediata utilización empresarial está al alcance <phrase>de la</phrase> mano. Finalmente, los profesores que <phrase>han</phrase> diseñado y elaborado este curso para <phrase>ti</phrase>, no solamente dan una visión académica de los usos avanzados del <phrase>software</phrase>, <phrase>sino</phrase> que, debido a <phrase>su</phrase> gran trayectoria profesional apoyada justamente <phrase>en</phrase> un uso intensivo y profundo de <phrase>Excel</phrase>, <phrase>te</phrase> transmitirán <phrase>su</phrase> propia vivencia, lo cual <phrase>te</phrase> permitirá tener una visión más concreta de las posibilidades que <phrase>te</phrase> brinda esta herramienta.
<phrase>Case</phrase> Studies: Analyzing Sentiment & Loan Default Prediction  In our <phrase>case</phrase> study on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, <phrase>user profile</phrase> <phrase>information</phrase>,...).  In our second <phrase>case</phrase> study for this course, loan default prediction, you will tackle financial <phrase>data</phrase>, and predict when a loan is likely to be risky or safe for the <phrase>bank</phrase>. These tasks are an examples of classification, one of the most widely used areas of <phrase>machine learning</phrase>, with a broad <phrase>array</phrase> of applications, including <phrase>ad</phrase> targeting, spam detection, <phrase>medical</phrase> diagnosis and <phrase>image classification</phrase>.   In this course, you will create classifiers that provide <phrase>state</phrase>-of-the-<phrase>art</phrase> performance on a <phrase>variety</phrase> of tasks.  You will become familiar with  the most successful techniques, which are most widely used in practice, including <phrase>logistic regression</phrase>, <phrase>decision trees</phrase> and <phrase>boosting</phrase>.  In addition, you will be able to <phrase>design</phrase> and implement the underlying <phrase>algorithms</phrase> that can learn these models at scale, using <phrase>stochastic</phrase> <phrase>gradient</phrase> ascent.  You will implement these technique on <phrase>real-world</phrase>, <phrase>large-scale</phrase> <phrase>machine learning</phrase> tasks.  You will also <phrase>address</phrase> significant tasks you will face in <phrase>real-world</phrase> applications of <phrase>ML</phrase>, including handling <phrase>missing data</phrase> and measuring <phrase>precision and recall</phrase> to evaluate a classifier.  This course is hands-on, <phrase>action</phrase>-packed, and full of visualizations and illustrations of how these techniques will behave on real <phrase>data</phrase>.  We've also included optional content in every module, covering <phrase>advanced topics</phrase> for those who want to go even deeper!   Learning Objectives: By the end of this course, you will be able to:    -Describe the <phrase>input and output</phrase> of a classification <phrase>model</phrase>.    -Tackle both <phrase>binary</phrase> and multiclass classification problems.    -Implement a <phrase>logistic regression</phrase> <phrase>model</phrase> for <phrase>large-scale</phrase> classification.      -Create a <phrase>non-linear</phrase> <phrase>model</phrase> using <phrase>decision trees</phrase>.    -Improve the performance of any <phrase>model</phrase> using <phrase>boosting</phrase>.    -Scale your methods with <phrase>stochastic</phrase> <phrase>gradient</phrase> ascent.    -Describe the underlying decision boundaries.      -Build a classification <phrase>model</phrase> to predict sentiment in a product review dataset.      -Analyze financial <phrase>data</phrase> to predict loan defaults.    -Use techniques for handling <phrase>missing data</phrase>.    -Evaluate your models using precision-recall metrics.    -Implement these techniques in <phrase>Python</phrase> (or in the <phrase>language</phrase> of your choice, though <phrase>Python</phrase> is highly recommended).
Who is this course for?   This course is designed for students, <phrase>business</phrase> analysts, and <phrase>data</phrase> scientists who want to apply statistical <phrase>knowledge</phrase> and techniques to <phrase>business</phrase> contexts. For example, it may be suited to experienced statisticians, analysts, engineers who want to move more into a <phrase>business</phrase> role.   You will find this course exciting and rewarding if you already have a background in <phrase>statistics</phrase>, can use R or another <phrase>programming language</phrase> and are familiar with <phrase>databases</phrase> and <phrase>data analysis</phrase> techniques such as <phrase>regression</phrase>, classification, and clustering. However, it contains a number of recitals and R Studio tutorials which will consolidate your competences, enable you to <phrase>play</phrase> more freely with <phrase>data</phrase> and explore new features and statistical functions in R.  With this course, you’ll have a first overview on Strategic <phrase>Business</phrase> Analytics topics. We’ll discuss a wide <phrase>variety</phrase> of applications of <phrase>Business</phrase> Analytics. From <phrase>Marketing</phrase> to <phrase>Supply Chain</phrase> or Credit Scoring and HR Analytics, etc. We’ll <phrase>cover</phrase> many different <phrase>data</phrase> analytics techniques, each time explaining how to be relevant for your <phrase>business</phrase>.  We’ll pay special attention to how you can produce convincing, actionable, and efficient insights. We'll also present you with different <phrase>data</phrase> analytics tools to be applied to different types of issues. By doing so, we’ll help you develop four sets of skills needed to leverage value from <phrase>data</phrase>: Analytics, IT, <phrase>Business</phrase> and <phrase>Communication</phrase>.   By the end of this <phrase>MOOC</phrase>, you should be able to approach a <phrase>business</phrase> issue using Analytics by (1) qualifying the issue at hand in quantitative terms, (2) <phrase>conducting</phrase> relevant <phrase>data</phrase> analyses, and (3) presenting your conclusions and recommendations in a <phrase>business</phrase>-oriented, actionable and efficient way.  Prerequisites : 1/ Be able to use R or to program 2/ To know the fundamentals of <phrase>databases</phrase>, <phrase>data</phrase> analysis (<phrase>regression</phrase>, classification, clustering)  We give credit to Pauline Glikman, Albane Gaubert, Elias Abou Khalil-Lanvin (Students at <phrase>ESSEC BUSINESS SCHOOL</phrase>) for their contribution to this course <phrase>design</phrase>.
This course will <phrase>cover</phrase> the <phrase>major</phrase> techniques for <phrase>mining</phrase> and analyzing text <phrase>data</phrase> to discover interesting patterns, extract useful <phrase>knowledge</phrase>, and support <phrase>decision making</phrase>, with an emphasis on statistical approaches that can be generally applied to arbitrary text <phrase>data</phrase> in any <phrase>natural language</phrase> with no or minimum <phrase>human</phrase> effort.   Detailed analysis of text <phrase>data</phrase> requires understanding of <phrase>natural language</phrase> text, which is known to be a difficult <phrase>task</phrase> for <phrase>computers</phrase>. However, a number of statistical approaches have been shown to work well for the "shallow" but robust analysis of text <phrase>data</phrase> for pattern finding and <phrase>knowledge</phrase> discovery. You will learn the <phrase>basic</phrase> concepts, principles, and <phrase>major</phrase> <phrase>algorithms</phrase> in <phrase>text mining</phrase> and their <phrase>potential applications</phrase>.
This class provides <phrase>an introduction</phrase> to the <phrase>Python programming language</phrase> and the iPython notebook. This is the third course in the <phrase>Genomic</phrase> <phrase>Big Data</phrase> <phrase>Science</phrase> Specialization from <phrase>Johns Hopkins University</phrase>.
In this course, we’re going to go over analytical solutions to common <phrase>healthcare</phrase> problems. I will review these <phrase>business</phrase> problems and you’ll build out various <phrase>data</phrase> structures to organize your <phrase>data</phrase>. We’ll then explore ways to group <phrase>data</phrase> and categorize <phrase>medical</phrase> codes into analytical categories. You will then be able to extract, transform, and load <phrase>data</phrase> into <phrase>data</phrase> structures required for solving <phrase>medical</phrase> problems and be able to also harmonize <phrase>data</phrase> from multiple sources. Finally, you will create a <phrase>data dictionary</phrase> to communicate the source and value of <phrase>data</phrase>. Creating these artifacts of <phrase>data</phrase> processes is a key skill when working with <phrase>healthcare</phrase> <phrase>data</phrase>.
The Executive <phrase>Data Science</phrase> Capstone, the specialization’s culminating project, is an opportunity for people who have completed all four EDS courses to apply what they've learned to a <phrase>real-world</phrase> scenario developed in collaboration with Zillow, a <phrase>data</phrase>-driven online <phrase>real estate</phrase> and rental <phrase>marketplace</phrase>, and DataCamp, a <phrase>web-based</phrase> platform for <phrase>data science</phrase> <phrase>programming</phrase>. Your <phrase>task</phrase> will be to <phrase>lead</phrase> a virtual <phrase>data science</phrase> team and make key decisions along the way to demonstrate that you have what it takes to shepherd a <phrase>complex analysis</phrase> project from start to finish.  For the final project, you will prepare and submit a presentation, which will be evaluated and graded by your <phrase>fellow</phrase> capstone participants.  Course <phrase>cover</phrase> image by Luckey_sun. <phrase>Creative Commons</phrase> BY-SA https://flic.kr/p/bx1jvU
Learn fundamental concepts in <phrase>data analysis</phrase> and <phrase>statistical inference</phrase>, focusing on one and two <phrase>independent</phrase> samples.
<phrase>Ce</phrase> cours à <phrase>la</phrase> demande accéléré sur une semaine couvre <phrase>les</phrase> fonctionnalités <phrase>Big Data</phrase> et <phrase>Machine Learning</phrase> (apprentissage automatique) de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform (GCP). Il présente rapidement <phrase>Google</phrase> <phrase>Cloud</phrase> Platform et décrit <phrase>en</phrase> détail <phrase>les</phrase> fonctionnalités de traitement <phrase>des</phrase> données.  À <phrase>la</phrase> <phrase>fin</phrase> de <phrase>ce</phrase> cours, <phrase>les</phrase> participants seront <phrase>en</phrase> mesure de réaliser <phrase>les</phrase> tâches suivantes : • <phrase>Identifier</phrase> l'objectif et <phrase>la</phrase> valeur <phrase>des</phrase> principaux produits <phrase>Big Data</phrase> et <phrase>machine learning</phrase> de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform • Utiliser CloudSQL et <phrase>Cloud</phrase> Dataproc pour migrer <phrase>les</phrase> charges de travail <phrase>MySQL</phrase> et <phrase>Hadoop</phrase>/<phrase>Pig</phrase>/Spark/Hive existantes vers <phrase>Google</phrase> <phrase>Cloud</phrase> Platform • Utiliser BigQuery et <phrase>Cloud</phrase> Datalab pour réaliser une analyse de données interactive • Choisir entre <phrase>Cloud</phrase> <phrase>SQL</phrase>, <phrase>BigTable</phrase> et Datastore • Entraîner et utiliser un réseau de neurones à l'aide de TensorFlow • Choisir entre différents produits de traitement <phrase>des</phrase> données sur <phrase>Google</phrase> <phrase>Cloud</phrase> Platform  Pour pouvoir s'inscrire à <phrase>ce</phrase> cours, <phrase>les</phrase> participants doivent avoir environ un (1) an d'expérience dans l'un ou plusieurs <phrase>des</phrase> domaines suivants : • Langage de requête commun, tel que <phrase>SQL</phrase> • Activités d'extraction, de transformation et de chargement • Modélisation de données • <phrase>Machine learning</phrase> et/ou statistiques • Programmation dans <phrase>Python</phrase>  Remarques sur le compte <phrase>Google</phrase> : • Pour bénéficier d'une version d'essai gratuite de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform, un compte <phrase>Google</phrase>/<phrase>Gmail</phrase> et une carte de crédit ou un compte bancaire sont requis (<phrase>les</phrase> services <phrase>Google</phrase> ne sont actuellement <phrase>pas</phrase> disponibles <phrase>en</phrase> Chine). • <phrase>Si</phrase> vous êtes un client <phrase>Google</phrase> <phrase>Cloud</phrase> Platform avec une adresse de facturation située dans l'Union européenne (UE) ou <phrase>en</phrase> Russie, consultez le document VAT Overview à l'adresse : https://cloud.google.com/billing/docs/resources/vat-overview • <phrase>Des</phrase> questions fréquentes sur <phrase>la</phrase> version d'essai gratuite de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform sont disponibles à l'adresse : https://cloud.google.com/<phrase>free</phrase>-trial/  Buscando <phrase>la</phrase> versión <phrase>en</phrase> español de este curso? Visita https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>es</phrase>/  このコースの日本語版をお探しですか？https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>jp</phrase>/
This course is <phrase>an introduction</phrase> into <phrase>formal concept analysis</phrase> (<phrase>FCA</phrase>), a <phrase>mathematical</phrase> theory oriented at applications in <phrase>knowledge representation</phrase>, <phrase>knowledge</phrase> acquisition, <phrase>data</phrase> analysis and visualization. It provides tools for understanding the <phrase>data</phrase> by representing it as a hierarchy of concepts or, more exactly, a concept lattice. <phrase>FCA</phrase> can help in processing a wide class of <phrase>data</phrase> types providing a framework in which various <phrase>data</phrase> analysis and <phrase>knowledge</phrase> acquisition techniques can be formulated. In this course, we focus on some of these techniques, as well as <phrase>cover</phrase> the <phrase>theoretical foundations</phrase> and algorithmic issues of <phrase>FCA</phrase>. Upon completion of the course, the students will be able to use the <phrase>mathematical</phrase> techniques and <phrase>computational tools</phrase> of <phrase>formal concept analysis</phrase> in their own <phrase>research</phrase> projects involving <phrase>data</phrase> processing. Among other things, the students will learn about <phrase>FCA</phrase>-<phrase>based approaches</phrase> to clustering and dependency <phrase>mining</phrase>. The course is self-contained, although <phrase>basic</phrase> <phrase>knowledge</phrase> of <phrase>elementary</phrase> <phrase>set theory</phrase>, <phrase>propositional logic</phrase>, and <phrase>probability theory</phrase> would help. End-of-the-week quizzes include easy questions aimed at checking <phrase>basic</phrase> understanding of the topic, as well as more advanced problems that may require some effort to be solved.  Do you have technical problems? Write to us: <phrase>coursera</phrase>@hse.ru
In this Capstone Project, you'll bring together all the new skills and insights you've learned through the four courses. You'll be given a 'mock' client problem and a <phrase>data</phrase> set. You'll need to analyze the <phrase>data</phrase> to gain <phrase>business</phrase> insights, <phrase>research</phrase> the client's <phrase>domain</phrase> <phrase>area</phrase>, and create recommendations. You'll then need to visualize the <phrase>data</phrase> in a client-facing presentation. You'll bring it all together in a recorded <phrase>video</phrase> presentation.  This course was created by <phrase>PricewaterhouseCoopers</phrase> LLP with an <phrase>address</phrase> at 300 <phrase>Madison</phrase> Avenue, <phrase>New York</phrase>, <phrase>New York</phrase>, 10017.
The purpose of this course is to summarize new directions in <phrase>Chinese</phrase> <phrase>history</phrase> and <phrase>social science</phrase> <phrase>produced</phrase> by the creation and analysis of big historical datasets based on newly opened <phrase>Chinese</phrase> archival holdings, and to organize this <phrase>knowledge</phrase> in a framework that encourages learning about <phrase>China</phrase> in comparative perspective.  Our course demonstrates how a new <phrase>scholarship</phrase> of discovery is redefining what is singular about modern <phrase>China</phrase> and modern <phrase>Chinese</phrase> <phrase>history</phrase>. Current understandings of <phrase>human</phrase> <phrase>history</phrase> and <phrase>social theory</phrase> are based largely on <phrase>Western</phrase> experience or on non-<phrase>Western</phrase> experience seen through a <phrase>Western</phrase> <phrase>lens</phrase>. This course offers <phrase>alternative</phrase> perspectives derived from <phrase>Chinese</phrase> experience over the last three centuries. We present specific <phrase>case</phrase> studies of this new <phrase>scholarship</phrase> of discovery divided into two <phrase>stand-alone</phrase> parts, which means that students can take any part without prior or subsequent attendance of the other part.  Part 1 (this course) focuses on comparative inequality and opportunity and addresses two related questions ‘Who rises to the top?’ and ‘Who gets what?’.  Part 2 (https://www.coursera.org/learn/understanding-<phrase>china</phrase>-<phrase>history</phrase>-part-2) turns to an arguably even more important question ‘Who are we?’ as seen through the framework of comparative <phrase>population</phrase> behavior - mortality, <phrase>marriage</phrase>, and reproduction – and their interaction with economic conditions and <phrase>human</phrase> values. We do so because mortality and reproduction are fundamental and <phrase>universal</phrase>, because they differ historically just as radically between <phrase>China</phrase> and the West as patterns of inequality and opportunity, and because these differences demonstrate the mutability of <phrase>human</phrase> behavior and values.  Course Overview <phrase>video</phrase>: https://youtu.be/dzUPRyJ4ETk
Курс посвящен статистическому сравнению характеристик групп и категорий.  В первой части курса мы рассказываем о параметрических и непараметрических тестах сравнения средних и распределений, какие возможности и ограничения связаны с разными методами сравнения групп, говорим о сравнении связанных и несвязанных выборок.  Различаются ли регионы (или аудитории) по доходу или возрасту? Как отличается пользовательская активность в разные времена года? Случайны различия между группами или закономерны?  Курс научит искать ответы на такие вопросы.  Вторая половина курсов посвящена выделению групп на основе эмпирических данных. Есть ли структура в данных? Можно ли говорить о том, что люди, компании или университеты группируются в отличительные, узнаваемые классы? Как найти и охарактеризовать такие группы? Мы покажем основные алгоритмы кластеризации, которые позволяют решать такие задачи. В практических видео курса мы покажем реализацию основных инструментов сравнения и выделения групп, а также предложим практические задачи и задания для отработки полученных навыков.
Welcome to the Advanced <phrase>Linear Models</phrase> for <phrase>Data Science</phrase> Class 2: Statistical <phrase>Linear Models</phrase>. This class is <phrase>an introduction</phrase> to <phrase>least squares</phrase> from a linear <phrase>algebraic</phrase> and <phrase>mathematical</phrase> perspective. Before beginning the class make sure that you have the following:  - A <phrase>basic</phrase> understanding of <phrase>linear algebra</phrase> and multivariate <phrase>calculus</phrase>. - A <phrase>basic</phrase> understanding of <phrase>statistics</phrase> and <phrase>regression</phrase> models. - At least a little familiarity with proof based <phrase>mathematics</phrase>. - <phrase>Basic</phrase> <phrase>knowledge</phrase> of the R <phrase>programming language</phrase>.  After taking this course, students will have a <phrase>firm</phrase> foundation in a linear <phrase>algebraic</phrase> treatment of <phrase>regression</phrase> modeling. This will greatly augment applied <phrase>data</phrase> scientists' <phrase>general</phrase> understanding of <phrase>regression</phrase> models.
The course presents an overview of the theory behind <phrase>biological diversity</phrase> <phrase>evolution</phrase> and dynamics and of methods for diversity calculation and estimation. We will become familiar with the <phrase>major</phrase> alpha, beta, and gamma diversity <phrase>estimation techniques</phrase>.  Understanding how <phrase>biodiversity</phrase> evolved and is evolving on <phrase>Earth</phrase> and how to correctly use and interpret <phrase>biodiversity</phrase> <phrase>data</phrase> is important for all students interested in <phrase>conservation biology</phrase> and <phrase>ecology</phrase>, whether they pursue careers in <phrase>academia</phrase> or as policy makers and other professionals (students graduating from our programs do both). Academics need to be able to use the theories and indices correctly, whereas policy makers must be able to understand and interpret the conclusions offered by the academics.  The course has the following expectations and <phrase>results</phrase>:  - covering the theoretical and <phrase>practical issues</phrase> involved in <phrase>biodiversity</phrase> theory, - <phrase>conducting</phrase> surveys and inventories of <phrase>biodiversity</phrase>, - analyzing the <phrase>information</phrase> gathered, - and applying their analysis to <phrase>ecological</phrase> and conservation problems.  Needed Learner Background:  - basics of <phrase>Ecology</phrase> and <phrase>Calculus</phrase> - good understanding of <phrase>English</phrase>
This introductory course is for <phrase>SAS</phrase> <phrase>software</phrase> users who perform statistical analyses using <phrase>SAS</phrase>/STAT <phrase>software</phrase>. The focus is on t <phrase>tests</phrase>, <phrase>ANOVA</phrase>, and <phrase>linear regression</phrase>, and includes a brief introduction to <phrase>logistic regression</phrase>.
Useful quantitative models help you to make informed decisions both in situations in which the factors affecting your decision are clear, as well as in situations in which some <phrase>important factors</phrase> are not clear at all. In this course, you can learn how to create quantitative models to reflect complex realities, and how to include in your <phrase>model</phrase> elements of <phrase>risk</phrase> and uncertainty. You’ll also learn the methods for creating predictive models for identifying optimal choices; and how those choices change in response to changes in the model’s assumptions. You’ll also learn the basics of the measurement and <phrase>management</phrase> of <phrase>risk</phrase>. By the end of this course, you’ll be able to build your own models with your own <phrase>data</phrase>, so that you can begin making <phrase>data</phrase>-informed decisions. You’ll also be prepared for the next course in the Specialization.
Buscaremos introduzir aos alunos métodos de estimação de modelos lineares que relacionam variáveis econômicas. Espera-<phrase>se</phrase> que o aluno seja capaz de entender modelos simples <phrase>e</phrase> testar hipóteses sobre <phrase>os</phrase> modelos de interesse.
Process <phrase>mining</phrase> is the missing link between <phrase>model</phrase>-based process analysis and <phrase>data</phrase>-oriented <phrase>analysis techniques</phrase>. Through <phrase>concrete</phrase> <phrase>data</phrase> sets and easy to use <phrase>software</phrase> the course provides <phrase>data science</phrase> <phrase>knowledge</phrase> that can be applied directly to analyze and improve processes in a <phrase>variety</phrase> of domains.  <phrase>Data science</phrase> is the profession of the future, because organizations that are unable to use (big) <phrase>data</phrase> in a smart way will not survive. It is not sufficient to focus on <phrase>data</phrase> storage and <phrase>data analysis</phrase>. The <phrase>data</phrase> <phrase>scientist</phrase> also needs to relate <phrase>data</phrase> to process analysis. Process <phrase>mining</phrase> bridges the gap between traditional <phrase>model</phrase>-based process analysis (e.g., <phrase>simulation</phrase> and other <phrase>business process management</phrase> techniques) and <phrase>data</phrase>-centric <phrase>analysis techniques</phrase> such as <phrase>machine learning</phrase> and <phrase>data mining</phrase>. Process <phrase>mining</phrase> seeks the confrontation between event <phrase>data</phrase> (i.e., observed behavior) and <phrase>process models</phrase> (hand-made or discovered automatically). This <phrase>technology</phrase> has become available only recently, but it can be applied to any type of operational processes (organizations and systems). Example <phrase>applications include</phrase>: analyzing treatment processes in <phrase>hospitals</phrase>, improving <phrase>customer service</phrase> processes in a <phrase>multinational</phrase>, understanding the browsing behavior of customers using booking site, analyzing failures of a baggage handling system, and improving the <phrase>user interface</phrase> of an <phrase>X-ray</phrase> machine. All of these applications have in common that dynamic behavior needs to be related to <phrase>process models</phrase>. Hence, we refer to this as "<phrase>data science</phrase> in <phrase>action</phrase>".  The course explains the key <phrase>analysis techniques</phrase> in process <phrase>mining</phrase>. Participants will learn various process discovery <phrase>algorithms</phrase>. These can be used to automatically learn <phrase>process models</phrase> from raw event <phrase>data</phrase>. Various other process <phrase>analysis techniques</phrase> that use event <phrase>data</phrase> will be presented. Moreover, the course will provide easy-to-use <phrase>software</phrase>, <phrase>real-life</phrase> <phrase>data</phrase> sets, and practical skills to directly apply the theory in a <phrase>variety</phrase> of <phrase>application</phrase> domains.  This course starts with an overview of approaches and technologies that use event <phrase>data</phrase> to support <phrase>decision making</phrase> and <phrase>business process</phrase> (re)<phrase>design</phrase>. Then the course focuses on process <phrase>mining</phrase> as a <phrase>bridge</phrase> between <phrase>data mining</phrase> and <phrase>business process modeling</phrase>. The course is at an introductory level with various practical assignments.  The course covers the three main types of process <phrase>mining</phrase>.  1. The first type of process <phrase>mining</phrase> is discovery. A discovery technique takes an event log and produces a <phrase>process model</phrase> without using any a-priori <phrase>information</phrase>. An example is the Alpha-<phrase>algorithm</phrase> that takes an event log and produces a <phrase>process model</phrase> (a <phrase>Petri net</phrase>) explaining the behavior recorded in the log.  2. The second type of process <phrase>mining</phrase> is conformance. Here, an existing <phrase>process model</phrase> is compared with an event log of the same process. Conformance checking can be used to check if <phrase>reality</phrase>, as recorded in the log, conforms to the <phrase>model</phrase> and <phrase>vice versa</phrase>.  3. The third type of process <phrase>mining</phrase> is enhancement. Here, the idea is to extend or improve an existing <phrase>process model</phrase> using <phrase>information</phrase> about the actual process recorded in some event log. Whereas conformance checking measures the alignment between <phrase>model</phrase> and <phrase>reality</phrase>, this third type of process <phrase>mining</phrase> aims at changing or extending the a-priori <phrase>model</phrase>. An example is the extension of a <phrase>process model</phrase> with performance <phrase>information</phrase>, e.g., showing bottlenecks. Process <phrase>mining</phrase> techniques can be used in an offline, but also online setting. The latter is known as operational support. An example is the detection of non-conformance at the moment the <phrase>deviation</phrase> actually <phrase>takes place</phrase>. Another example is time prediction for running cases, i.e., given a partially executed <phrase>case</phrase> the remaining processing time is estimated based on historic <phrase>information</phrase> of similar cases.  Process <phrase>mining</phrase> provides not only a <phrase>bridge</phrase> between <phrase>data mining</phrase> and <phrase>business process management</phrase>; it also helps to <phrase>address</phrase> the <phrase>classical</phrase> divide between "<phrase>business</phrase>" and "IT". <phrase>Evidence-based</phrase> <phrase>business process management</phrase> based on process <phrase>mining</phrase> helps to create a common ground for <phrase>business process</phrase> improvement and <phrase>information systems</phrase> development.  The course uses many examples using <phrase>real-life</phrase> event logs to illustrate the concepts and <phrase>algorithms</phrase>. After taking this course, one is able to <phrase>run</phrase> process <phrase>mining</phrase> projects and have a good understanding of the <phrase>Business</phrase> Process <phrase>Intelligence</phrase> field.  After taking this course you should: - have a good understanding of <phrase>Business</phrase> Process <phrase>Intelligence</phrase> techniques (in particular process <phrase>mining</phrase>), - understand the role of <phrase>Big Data</phrase> in today’s <phrase>society</phrase>, - be able to relate process <phrase>mining</phrase> techniques to other <phrase>analysis techniques</phrase> such as <phrase>simulation</phrase>, <phrase>business intelligence</phrase>, <phrase>data mining</phrase>, <phrase>machine learning</phrase>, and verification, - be able to apply <phrase>basic</phrase> process discovery techniques to learn a <phrase>process model</phrase> from an event log (both manually and using tools), - be able to apply <phrase>basic</phrase> conformance checking techniques to compare event logs and <phrase>process models</phrase> (both manually and using tools), - be able to extend a <phrase>process model</phrase> with <phrase>information</phrase> extracted from the event log (e.g., show bottlenecks), - have a good understanding of the <phrase>data</phrase> needed to start a process <phrase>mining</phrase> project, - be able to characterize the questions that can be answered based on such event <phrase>data</phrase>, - explain how process <phrase>mining</phrase> can also be used for operational support (prediction and recommendation), and - be able to conduct process <phrase>mining</phrase> projects in a structured manner.
<phrase>Spreadsheet</phrase> <phrase>software</phrase> remains one of the most ubiquitous pieces of <phrase>software</phrase> used in workplaces across the world. Learning to confidently operate this <phrase>software</phrase> means adding a highly valuable asset to your employability portfolio. In this third course of our <phrase>Excel</phrase> specialization <phrase>Excel</phrase> Skills for <phrase>Business</phrase> you will delve deeper into some of the most powerful features <phrase>Excel</phrase> has to offer. When you have successfully completed the course you will be able to  Check for and prevent errors in <phrase>spreadsheets</phrase>;  Create powerful <phrase>automation</phrase> in <phrase>spreadsheets</phrase>;  Apply advanced formulas and conditional <phrase>logic</phrase> to help make informed <phrase>business</phrase> decisions; and Create <phrase>spreadsheets</phrase> that help forecast and <phrase>model</phrase> <phrase>data</phrase>.   Once again, we have brought together a great teaching team that will be with you every <phrase>step</phrase> of the way. Nicky, Prashan and myself will guide you through each week. As we are exploring these more <phrase>advanced topics</phrase>, we are following Alex who is an <phrase>Excel</phrase> consultant called in by businesses that experience issues with their <phrase>spreadsheets</phrase>.
This course is for users who want to learn how to write <phrase>SAS</phrase> programs to access, explore, prepare, and analyze <phrase>data</phrase>. It is the <phrase>entry point</phrase> to learning <phrase>SAS</phrase> <phrase>programming</phrase> for <phrase>data science</phrase>, <phrase>machine learning</phrase>, and <phrase>artificial intelligence</phrase>. It is a prerequisite to many other <phrase>SAS</phrase> courses.  By the end of this course, you will know how to use <phrase>SAS</phrase> Studio to write and submit <phrase>SAS</phrase> programs that access <phrase>SAS</phrase>, <phrase>Microsoft Excel</phrase>, and text <phrase>data</phrase>. You will know how to explore and validate <phrase>data</phrase>, prepare <phrase>data</phrase> by subsetting rows and <phrase>computing</phrase> new columns, analyze and <phrase>report</phrase> on <phrase>data</phrase>, <phrase>export</phrase> <phrase>data</phrase> and <phrase>results</phrase> to other formats, use <phrase>SQL</phrase> in <phrase>SAS</phrase> to query and join tables.  Prerequisites: Learners should have experience using <phrase>computer</phrase> <phrase>software</phrase>. Specifically, you should be able to understand file structures and system commands on your <phrase>operating systems</phrase> and access <phrase>data</phrase> files on your <phrase>operating systems</phrase>. No prior <phrase>SAS</phrase> experience is needed.
You may never be sure whether you have an effective <phrase>user experience</phrase> until you have tested it with users. In this course, you’ll learn how to <phrase>design</phrase> <phrase>user-centered</phrase> experiments, how to <phrase>run</phrase> such experiments, and how to analyze <phrase>data</phrase> from these experiments in <phrase>order</phrase> to evaluate and validate user experiences. You will work through <phrase>real-world</phrase> examples of experiments from the fields of <phrase>UX</phrase>, IxD, and HCI, understanding issues in experiment <phrase>design</phrase> and analysis. You will analyze multiple <phrase>data</phrase> sets using recipes given to you in the R statistical <phrase>programming language</phrase> -- no prior <phrase>programming</phrase> experience is assumed or required, but you will be required to read, understand, and modify code snippets provided to you. By the end of the course, you will be able to knowledgeably <phrase>design</phrase>, <phrase>run</phrase>, and analyze your own experiments that give statistical weight to your designs.
<phrase>Data science</phrase> — одна из самых горячих областей на сегодняшний день, а <phrase>Python</phrase> — один из самых популярных инструментов для анализа данных. В этом курсе вы узнаете, как применять свои навыки программирования для построения предиктивных моделей, визуализации данных и работы с нейросетями. Курс ориентирован на практику и позволит вам сразу приступить к работе с данными и построению моделей.
Методы машинного обучения — будь то алгоритмы классификации или регрессии, методы кластеризации или алгоритмы понижения размерности — применяются к подготовленным данным с вычисленными признаками для решения уже сформулированной задачи. Однако специалисты по анализу данных редко оказываются в такой идеальной ситуации. Обычно перед ними ставят задачи, которые нуждаются в уточнении формулировки, выборе метрики качества и протокола тестирования итоговой модели. Данные, с которыми нужно работать, часто представлены в непригодном виде: они зашумлены, содержат ошибки и выбросы, хранятся в неудобном формате и т. д.  В этом курсе мы разберем прикладные задачи из различных областей анализа данных: анализ текста и информационный поиск, коллаборативная фильтрация и рекомендательные системы, бизнес-аналитика, прогнозирование временных рядов. На их примере вы узнаете, как извлекать признаки из разнородных данных, какие при этом возникают проблемы и как их решать. Вы научитесь сводить задачу заказчика к формальной постановке задачи машинного обучения и поймёте, как проверять качество построенной модели на исторических данных и в онлайн-эксперименте. На каждой задаче мы изучим плюсы и минусы пройденных алгоритмов машинного обучения.  Прослушав этот курс, вы познакомитесь с распространенными типами прикладных задач и будете понимать схемы их решения.  Задания и видео курса разработаны на <phrase>Python</phrase> 2.
If you want to break into <phrase>cutting-edge</phrase> <phrase>AI</phrase>, this course will help you do so. <phrase>Deep learning</phrase> engineers are highly sought after, and <phrase>mastering</phrase> <phrase>deep learning</phrase> will give you numerous new career opportunities. <phrase>Deep learning</phrase> is also a new "superpower" that will let you build <phrase>AI</phrase> systems that just weren't possible a few <phrase>years ago</phrase>.   In this course, you will learn the foundations of <phrase>deep learning</phrase>. When you finish this class, you will: - Understand the <phrase>major</phrase> <phrase>technology</phrase> trends driving <phrase>Deep Learning</phrase> - Be able to build, <phrase>train</phrase> and apply fully connected <phrase>deep neural networks</phrase>  - Know how to implement efficient (vectorized) <phrase>neural networks</phrase>  - Understand the key parameters in a neural network's <phrase>architecture</phrase>   This course also teaches you how <phrase>Deep Learning</phrase> actually works, rather than presenting only a cursory or <phrase>surface</phrase>-level description. So after completing it, you will be able to apply <phrase>deep learning</phrase> to a your own applications. If you are looking for a job in <phrase>AI</phrase>, after this course you will also be able to answer <phrase>basic</phrase> <phrase>interview</phrase> questions.   This is the first course of the <phrase>Deep Learning</phrase> Specialization.
This course will <phrase>introduce</phrase> the learner to the basics of the <phrase>python</phrase> <phrase>programming environment</phrase>, including fundamental <phrase>python</phrase> <phrase>programming</phrase> techniques such as lambdas, <phrase>reading</phrase> and manipulating csv files, and the <phrase>numpy</phrase> <phrase>library</phrase>. The course will <phrase>introduce</phrase> <phrase>data</phrase> manipulation and cleaning techniques using the popular <phrase>python</phrase> <phrase>pandas</phrase> <phrase>data science</phrase> <phrase>library</phrase> and <phrase>introduce</phrase> the abstraction of the Series and DataFrame as the central <phrase>data</phrase> structures for <phrase>data analysis</phrase>, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular <phrase>data</phrase>, clean it, manipulate it, and <phrase>run</phrase> <phrase>basic</phrase> inferential statistical analyses.   This course should be taken before any of the other Applied <phrase>Data Science</phrase> with <phrase>Python</phrase> courses: Applied Plotting, Charting & <phrase>Data</phrase> Representation in <phrase>Python</phrase>, Applied <phrase>Machine Learning</phrase> in <phrase>Python</phrase>, Applied <phrase>Text Mining</phrase> in <phrase>Python</phrase>, Applied <phrase>Social Network</phrase> Analysis in <phrase>Python</phrase>.
We will learn <phrase>computational methods</phrase> -- <phrase>algorithms</phrase> and <phrase>data</phrase> structures -- for analyzing <phrase>DNA sequencing</phrase> <phrase>data</phrase>. We will learn a little about <phrase>DNA</phrase>, <phrase>genomics</phrase>, and how <phrase>DNA sequencing</phrase> is used.  We will use <phrase>Python</phrase> to implement key <phrase>algorithms</phrase> and <phrase>data</phrase> structures and to analyze real <phrase>genomes</phrase> and <phrase>DNA sequencing</phrase> datasets.
The capstone project class will allow students to create a usable/<phrase>public</phrase> <phrase>data</phrase> product that can be used to show your skills to potential employers. Projects will be drawn from <phrase>real-world</phrase> problems and will be conducted with <phrase>industry</phrase>, <phrase>government</phrase>, and <phrase>academic</phrase> partners.
Before you can work with <phrase>data</phrase> you have to get some. This course will <phrase>cover</phrase> the <phrase>basic</phrase> ways that <phrase>data</phrase> can be obtained. The course will <phrase>cover</phrase> obtaining <phrase>data</phrase> from the web, from <phrase>APIs</phrase>, from <phrase>databases</phrase> and from colleagues in various formats. It will also <phrase>cover</phrase> the basics of <phrase>data</phrase> cleaning and how to make <phrase>data</phrase> “tidy”. Tidy <phrase>data</phrase> dramatically speed downstream <phrase>data</phrase> analysis tasks. The course will also <phrase>cover</phrase> the components of a complete <phrase>data</phrase> set including <phrase>raw data</phrase>, processing instructions, codebooks, and processed <phrase>data</phrase>. The course will <phrase>cover</phrase> the basics needed for collecting, cleaning, and sharing <phrase>data</phrase>.
Welcome! Do you wish to know how to analyze and solve <phrase>business</phrase> and economic questions with <phrase>data analysis</phrase> tools? Then <phrase>Econometrics</phrase> by <phrase>Erasmus University Rotterdam</phrase> is the right course for you, as you learn how to translate <phrase>data</phrase> into models to make forecasts and to support <phrase>decision making</phrase>.  * What do I learn? When you know <phrase>econometrics</phrase>, you are able to translate <phrase>data</phrase> into models to make forecasts and to support <phrase>decision making</phrase> in a wide <phrase>variety</phrase> of fields, ranging from <phrase>macroeconomics</phrase> to <phrase>finance</phrase> and <phrase>marketing</phrase>. Our course starts with introductory lectures on simple and multiple <phrase>regression</phrase>, followed by topics of special interest to deal with <phrase>model</phrase> specification, endogenous variables, <phrase>binary</phrase> choice <phrase>data</phrase>, and <phrase>time series</phrase> <phrase>data</phrase>.  You learn these key topics in <phrase>econometrics</phrase> by watching the videos with in-<phrase>video</phrase> quizzes and by making post-<phrase>video</phrase> training exercises.   * Do I need <phrase>prior knowledge</phrase>? The course is suitable for (advanced <phrase>undergraduate</phrase>) students in <phrase>economics</phrase>, <phrase>finance</phrase>, <phrase>business</phrase>, <phrase>engineering</phrase>, and <phrase>data</phrase> analysis, as well as for those who work in these fields. The course requires some basics of matrices, <phrase>probability</phrase>, and <phrase>statistics</phrase>, which are reviewed in the <phrase>Building Blocks</phrase> module.   * What <phrase>literature</phrase> can I consult to support my studies? You can follow the <phrase>MOOC</phrase> without studying additional sources. Further <phrase>reading</phrase> of the discussed topics (including the <phrase>Building Blocks</phrase>) is provided in the <phrase>textbook</phrase> that we wrote and on which the <phrase>MOOC</phrase> is based: <phrase>Econometric</phrase> Methods with Applications in <phrase>Business</phrase> and <phrase>Economics</phrase>, <phrase>Oxford University</phrase> Press. The connection between the <phrase>MOOC</phrase> modules and the <phrase>book</phrase> chapters is shown in the Course Guide – Further <phrase>Information</phrase> – How can I continue my studies.  * Will there be teaching assistants active to guide me through the course? Staff and <phrase>PhD</phrase> students of our <phrase>Econometric</phrase> Institute will provide guidance in January and February of each year. In other periods, we provide only <phrase>elementary</phrase> guidance. We always advise you to connect with <phrase>fellow</phrase> learners of this course to discuss topics and exercises.  * How will I get a certificate? To gain the certificate of this course, you are asked to make six <phrase>Test</phrase> Exercises (one per module) and a <phrase>Case</phrase> Project. Further, you perform peer-reviewing activities of the work of three of your <phrase>fellow</phrase> learners of this <phrase>MOOC</phrase>. You gain the certificate if you <phrase>pass</phrase> all seven assignments.  Have a <phrase>nice</phrase> journey into the world of <phrase>Econometrics</phrase>! The <phrase>Econometrics</phrase> team
Welcome to Practical <phrase>Time Series</phrase> Analysis!  Many of us are "accidental" <phrase>data</phrase> analysts. We trained in the sciences, <phrase>business</phrase>, or <phrase>engineering</phrase> and then found ourselves confronted with <phrase>data</phrase> for which we have no formal analytic training.  This course is designed for people with some technical competencies who would like more than a "<phrase>cookbook</phrase>" approach, but who still need to concentrate on the routine sorts of presentation and analysis that deepen the understanding of our <phrase>professional</phrase> topics.   In practical <phrase>Time Series</phrase> Analysis we look at <phrase>data</phrase> sets that represent sequential <phrase>information</phrase>, such as <phrase>stock</phrase> prices, annual rainfall, <phrase>sunspot</phrase> <phrase>activity</phrase>, the price of <phrase>agricultural</phrase> <phrase>products</phrase>, and more.  We look at several <phrase>mathematical</phrase> models that might be used to describe the processes which generate these types of <phrase>data</phrase>. We also look at <phrase>graphical</phrase> representations that provide insights into our <phrase>data</phrase>. Finally, we also learn how to make forecasts that say intelligent things about what we might expect in the future.  Please take a few minutes to explore the course site. You will find <phrase>video</phrase> lectures with supporting written materials as well as quizzes to help emphasize important points. The <phrase>language</phrase> for the course is R, a <phrase>free</phrase> implementation of the S <phrase>language</phrase>. It is a <phrase>professional</phrase> environment and fairly easy to learn.  You can discuss material from the course with your <phrase>fellow</phrase> learners. Please take a moment to <phrase>introduce</phrase> yourself!  <phrase>Time Series</phrase> Analysis can take effort to learn- we have tried to present those ideas that are "<phrase>mission critical</phrase>" in a way where you understand enough of the <phrase>math</phrase> to <phrase>fell</phrase> satisfied while also being immediately productive. We hope you enjoy the class!
At the end of the course, you will be able to:  *<phrase>Retrieve data</phrase> from example <phrase>database</phrase> and <phrase>big data</phrase> <phrase>management</phrase> systems  *Describe the connections between <phrase>data management</phrase> operations and the <phrase>big data</phrase> processing patterns needed to utilize them in <phrase>large-scale</phrase> analytical applications *Identify when a <phrase>big data</phrase> problem needs <phrase>data integration</phrase> *Execute simple <phrase>big data</phrase> <phrase>integration</phrase> and processing on <phrase>Hadoop</phrase> and Spark platforms  This course is for those new to <phrase>data science</phrase>.  Completion of Intro to <phrase>Big Data</phrase> is recommended.  No prior <phrase>programming</phrase> experience is needed, although the ability to <phrase>install</phrase> applications and utilize a <phrase>virtual machine</phrase> is necessary to complete the hands-on assignments.  Refer to the specialization <phrase>technical requirements</phrase> for complete <phrase>hardware</phrase> and <phrase>software</phrase> specifications.  <phrase>Hardware</phrase> Requirements:  (A) <phrase>Quad Core</phrase> Processor (<phrase>VT</phrase>-x or <phrase>AMD</phrase>-V support recommended), <phrase>64-bit</phrase>; (B) 8 <phrase>GB</phrase> <phrase>RAM</phrase>; (C) 20 <phrase>GB</phrase> disk <phrase>free</phrase>. How to find your <phrase>hardware</phrase> <phrase>information</phrase>: (<phrase>Windows</phrase>): <phrase>Open System</phrase> by clicking the Start button, right-clicking <phrase>Computer</phrase>, and then clicking Properties; (<phrase>Mac</phrase>): Open Overview by clicking on the <phrase>Apple</phrase> menu and clicking “About This <phrase>Mac</phrase>.” Most <phrase>computers</phrase> with 8 <phrase>GB</phrase> <phrase>RAM</phrase> purchased in the last 3 years will meet the minimum requirements.You will need a <phrase>high</phrase> speed <phrase>internet</phrase> connection because you will be downloading files up to 4 <phrase>Gb</phrase> in size.   <phrase>Software</phrase> Requirements:  This course relies on several <phrase>open-source software</phrase> tools, including <phrase>Apache Hadoop</phrase>. All required <phrase>software</phrase> can be downloaded and installed <phrase>free</phrase> of charge (except for <phrase>data</phrase> charges from your <phrase>internet provider</phrase>). <phrase>Software</phrase> requirements include: <phrase>Windows</phrase> 7+, <phrase>Mac OS</phrase> X 10.10+, <phrase>Ubuntu</phrase> 14.04+ or <phrase>CentOS</phrase> 6+ <phrase>VirtualBox</phrase> 5+.
No doubt working with huge <phrase>data</phrase> volumes is hard, but to move a <phrase>mountain</phrase>, you have to deal with a lot of small <phrase>stones</phrase>. But why strain yourself? Using  <phrase>Mapreduce</phrase> and Spark you tackle the issue partially, thus leaving some space for <phrase>high</phrase>-level tools. Stop  struggling to make your <phrase>big data</phrase> <phrase>workflow</phrase> productive and efficient,  make use of the tools we are offering you.   This course will teach you how to: - <phrase>Warehouse</phrase> your <phrase>data</phrase> efficiently using Hive, Spark <phrase>SQL</phrase> and Spark DataFframes.  - Work with large <phrase>graphs</phrase>, such as social <phrase>graphs</phrase> or networks.  - Optimize your Spark applications for maximum performance.  Precisely, you will <phrase>master</phrase> your <phrase>knowledge</phrase> in: - Writing and executing Hive & Spark <phrase>SQL</phrase> queries; - Reasoning how the queries are <phrase>translated into</phrase> actual execution primitives (be it <phrase>MapReduce</phrase> jobs or Spark transformations); - Organizing your <phrase>data</phrase> in Hive to optimize <phrase>disk space</phrase> usage and execution times; - Constructing Spark DataFrames and using them to write <phrase>ad-hoc</phrase> analytical jobs easily; - Processing large <phrase>graphs</phrase> with Spark GraphFrames; - <phrase>Debugging</phrase>, profiling and optimizing Spark <phrase>application</phrase> performance.   Still in doubt? Check this out. Become a <phrase>data</phrase> <phrase>ninja</phrase> by taking this course!  Special thanks to: - Prof. Mikhail Roytberg, APT <phrase>dept</phrase>., MIPT, who was the initial reviewer of the project, the supervisor and mentor of half of the BigData team. He was the one, who helped to get this show on the <phrase>road</phrase>. - Oleg Sukhoroslov (<phrase>PhD</phrase>, Senior Researcher at IITP RAS), who has been teaching  <phrase>MapReduce</phrase>, <phrase>Hadoop</phrase> and <phrase>friends</phrase> since 2008. Now he is leading the <phrase>infrastructure</phrase> team. - Oleg Ivchenko (<phrase>PhD</phrase> <phrase>student</phrase> APT <phrase>dept</phrase>., MIPT), Pavel Akhtyamov (<phrase>MSc</phrase>. <phrase>student</phrase> at APT <phrase>dept</phrase>., MIPT) and Vladimir Kuznetsov (Assistant at P.G. Demidov Yaroslavl <phrase>State</phrase> <phrase>University</phrase>), superbrains who have developed and now maintain the <phrase>infrastructure</phrase> used for practical assignments in this course. - Asya Roitberg, <phrase>Eugene</phrase> Baulin, <phrase>Marina</phrase> Sudarikova. These people never <phrase>sleep</phrase> to babysit this course day and night, to make your learning experience productive, smooth and exciting.
This course will provide you with a <phrase>basic</phrase>, intuitive and practical introduction into <phrase>Probability</phrase> Theory. You will be able to learn how to apply <phrase>Probability</phrase> Theory in different scenarios and you will earn a "toolbox" of methods to deal with uncertainty in your <phrase>daily</phrase> <phrase>life</phrase>.   The course is <phrase>split</phrase> in 5 modules. In each module you will first have an easy introduction into the topic, which will serve as a basis to further develop your <phrase>knowledge</phrase> about the topic and acquire the "tools" to deal with uncertainty. Additionally, you will have the opportunity to complete 5 exercise sessions to reflect about the content learned in each module and start applying your earned <phrase>knowledge</phrase> right away.   The topics <phrase>covered</phrase> are: "<phrase>Probability</phrase>", "<phrase>Conditional Probability</phrase>", "Applications", "<phrase>Random Variables</phrase>", and "<phrase>Normal Distribution</phrase>".  You will see how the modules are taught in a lively way, focusing on having an entertaining and useful learning experience! We are looking <phrase>forward</phrase> to see you online!
This course will assist you with recreating work that a previous coworker completed, revisiting a project you abandoned some time ago, or simply reproducing a document with a consistent format and <phrase>workflow</phrase>. Incomplete <phrase>information</phrase> about how the work was done, where the files are, and which is the most recent version can give rise to many complications. This course  focuses on the proper documentation creation process, allowing you and your colleagues to easily reproduce the components of your <phrase>workflow</phrase>. Throughout this course, you'll receive helpful demonstrations of RStudio and the R <phrase>Markdown</phrase> <phrase>language</phrase> and engage in <phrase>active learning</phrase> opportunities to help you build a <phrase>professional</phrase> online portfolio.
O que é aprendizado de máquina <phrase>e</phrase> que tipos de problema ele pode resolver? O <phrase>Google</phrase> pensa no aprendizado de máquina de <phrase>uma</phrase> maneira um pouco diferente. Ele <phrase>se</phrase> concentra mais <phrase>na</phrase> lógica, <phrase>em</phrase> vez de apenas <phrase>em</phrase> dados. Discutimos por que esse modelo é útil quando pensamos <phrase>na</phrase> criação de canais de modelos de aprendizado de máquina. <phrase>Em</phrase> seguida, falamos sobre as cinco fases <phrase>da</phrase> conversão de um possível caso de uso a ser realizado por aprendizado de máquina <phrase>e</phrase> vemos a importância de não ignorar essas fases. Finalizamos com a identificação das tendências que podem ser ampliadas pelo aprendizado de máquina <phrase>e</phrase> como reconhecer isso.
Career prospects are bright for those qualified to work with <phrase>healthcare</phrase> <phrase>data</phrase> or as <phrase>Health</phrase> <phrase>Information Management</phrase> (HIM) professionals. Perhaps you work in <phrase>data</phrase> analytics but are considering a move into <phrase>healthcare</phrase>, or you work in <phrase>healthcare</phrase> but are considering a <phrase>transition</phrase> into a new role. In either <phrase>case</phrase>, <phrase>Healthcare</phrase> <phrase>Data</phrase> Quality and Governance will provide <phrase>insight into</phrase> how valuable <phrase>data</phrase> assets are <phrase>protected</phrase> to maintain <phrase>data</phrase> quality. This serves care providers, patients, <phrase>doctors</phrase>, clinicians, and those who <phrase>carry out</phrase> the <phrase>business</phrase> of improving <phrase>health</phrase> outcomes.   "<phrase>Big Data</phrase>" makes headlines, but that <phrase>data</phrase> must be managed to maintain quality. <phrase>High</phrase>-quality <phrase>data</phrase> is one of the most valuable assets gathered and used by any <phrase>business</phrase>. This holds greater significance in <phrase>healthcare</phrase> where the maintenance and governance of <phrase>data quality</phrase> directly impact people’s lives. This course will explain how <phrase>data quality</phrase> is improved and maintained. You’ll learn why <phrase>data quality</phrase> matters, then see how <phrase>healthcare</phrase> professionals monitor, manage and improve <phrase>data</phrase> quality. You’ll see how <phrase>human</phrase> and computerized systems interact to sustain <phrase>data</phrase> quality through <phrase>data governance</phrase>. You’ll discover how to measure <phrase>data</phrase> quality with <phrase>metadata</phrase>, <phrase>tracking</phrase> <phrase>data</phrase> <phrase>provenance</phrase>, validating and verifying <phrase>data</phrase>, along with a <phrase>communication</phrase> framework commonly used in <phrase>healthcare</phrase> settings.   This <phrase>knowledge</phrase> matters because <phrase>high</phrase>-quality <phrase>data</phrase> will be transformed into valuable insights that can save lives, <phrase>reduce costs</phrase>, to improve <phrase>healthcare</phrase> and make it more accessible and affordable. You will make yourself more of an asset in the <phrase>healthcare</phrase> field by what you gain from this course.
<phrase>Ce</phrase> cours constitue <phrase>la</phrase> seconde partie d'un enseignement consacré aux bases théoriques et pratiques <phrase>des</phrase> systèmes d’information géographique. - Il propose une introduction aux systèmes d’information géographique qui ne requiert <phrase>pas</phrase> de connaissances préalables <phrase>en</phrase> informatique. - Il donne <phrase>la</phrase> possibilité d’acquérir rapidement <phrase>les</phrase> notions de base qui vous permettent de créer <phrase>des</phrase> bases de données spatiales et de fabriquer <phrase>des</phrase> cartes géographiques. - Il s’agit d’un cours pratique qui repose sur l’utilisation de logiciels libres, notamment QGIS.  Lors <phrase>de la</phrase> première partie du cours, vous avez exploré <phrase>les</phrase> <phrase>principes</phrase> de base <phrase>de la</phrase> numérisation du territoire et du stockage <phrase>des</phrase> géodonnées. Vous avez notamment appris à : - Caractériser <phrase>des</phrase> objets et <phrase>des</phrase> phénomènes spatiaux (modélisation du territoire) du point de vue de leur positionnement dans l’espace (systèmes de coordonnées et projections, relations spatiales) et <phrase>en</phrase> fonction de leur <phrase>nature</phrase> intrinsèque (mode objet ou vecteur vs. mode image ou <phrase>raster</phrase>); - Utiliser diverses méthodes d’acquisition de données (mesure directe, géoréférencement d’images, digitalisation, source de données existantes, etc.); - Utiliser divers modes de stockage <phrase>des</phrase> géodonnées (fichiers simples et bases de données relationnelles); - Utiliser <phrase>des</phrase> outils de modélisation <phrase>des</phrase> données pour décrire et implémenter une base de données;  - Créer <phrase>des</phrase> requêtes dans un langage d’interrogation et de manipulation <phrase>des</phrase> données.  <phrase>La</phrase> seconde partie du cours porte sur <phrase>les</phrase> méthodes d'analyse spatiale et <phrase>les</phrase> techniques de représentation de l'information géoréférencée. Vous apprendrez notamment à: - Analyser <phrase>les</phrase> propriétés spatiales de variables discrètes, par exemple <phrase>en</phrase> quantifiant l’autocorrélation spatiale; - Travailler avec <phrase>des</phrase> variables continues (échantillonnage, <phrase>interpolation</phrase> et <phrase>construction</phrase> de courbes d’isovaleurs) - Utiliser <phrase>les</phrase> modèles numériques d'altitude et leurs dérivées (pente, orientation, etc.); - Utiliser <phrase>des</phrase> techniques de <phrase>superposition</phrase> <phrase>des</phrase> géodonnées; - Produire <phrase>des</phrase> documents cartographiques selon <phrase>les</phrase> règles <phrase>de la</phrase> sémiologie graphique; - Explorer d’autres formes de représentation spatiale (cartographie interactive sur <phrase>internet</phrase>, représentations 3D, et réalité augmentée).  <phrase>La</phrase> page https://www.facebook.com/moocsig fournit un forum interactif pour <phrase>les</phrase> participants à <phrase>ce</phrase> cours.
The <phrase>nature</phrase> of <phrase>digital</phrase> <phrase>manufacturing</phrase> and <phrase>design</phrase> (DM&D), and its heavy reliance on creating a <phrase>digital</phrase> thread of product and process <phrase>data</phrase> and <phrase>information</phrase>, makes it a prime <phrase>target</phrase> for hackers and counterfeiters. This course will <phrase>introduce</phrase> students to why creating a strong and secure <phrase>infrastructure</phrase> should be of <phrase>paramount</phrase> concern for anyone operating in the DM&D <phrase>domain</phrase>, and measures that can be employed to protect operational technologies, systems and resources.   Acquire <phrase>knowledge</phrase> about <phrase>security</phrase> needs and the <phrase>application</phrase> of <phrase>information security</phrase> systems. Build the foundational skills needed in performing a <phrase>risk assessment</phrase> of operational and <phrase>information technology</phrase> assets. Gain valuable insights of implementing controls to mitigate identified risks.  Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the seventh course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
この 1 週間の集中オンデマンド コースは、<phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals をベースにしています。ビデオによる講義、デモ、ハンズオンラボを通じて、<phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub と <phrase>Dataflow</phrase> を使用してストリーミング データ パイプラインを構築し、リアルタイムでの意思決定を可能にする方法を学びます。さまざまなステークホルダーに合わせた出力を表示するダッシュボードを構築する方法についても学習します。   前提条件: • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals を修了している（または、同等の経験がある） • <phrase>Java</phrase> に関するある程度の知識  目標: • リアルタイム ストリーミング分析のユースケースを理解する • <phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub の非同期メッセージング サービスを使用してデータイベントを管理する • ストリーミング パイプラインを記述して、必要に応じて変換を実行する • ストリーミング パイプラインの両面である作成と消費に精通する • リアルタイム ストリーミングと分析のために、<phrase>Dataflow</phrase>、BigQuery、<phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub を相互運用する
<phrase>En</phrase> este curso acelerado a pedido de una semana, los participantes recibirán una introducción práctica sobre cómo diseñar y compilar modelos de aprendizaje automático <phrase>en</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Mediante una serie de presentaciones, demostraciones y labs prácticos, los participantes incorporarán conceptos de aprendizaje automático (<phrase>AA</phrase>) y TensorFlow, y adquirirán habilidades prácticas para desarrollar, evaluar y producir modelos de <phrase>AA</phrase>.   OBJETIVOS   <phrase>En</phrase> este curso, los participantes adquirirán las siguientes habilidades:    ● Identificar casos prácticos de aprendizaje automático    ● Compilar un modelo de <phrase>AA</phrase> con TensorFlow    ● Compilar modelos de <phrase>AA</phrase> implementables y escalables con <phrase>Cloud</phrase> <phrase>ML</phrase>    ● Conocer <phrase>la</phrase> importancia del procesamiento previo y <phrase>la</phrase> combinación de atributos    ● Incorporar conceptos avanzados de <phrase>AA</phrase> a sus modelos    ● Producir modelos entrenados de <phrase>AA</phrase>    REQUISITOS PREVIOS   Para aprovechar al máximo este curso, los participantes deben cumplir con los siguientes requisitos previos:    ● Haber completado <phrase>el</phrase> curso "<phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals" o contar con experiencia equivalente    ● Tener un conocimiento básico del lenguaje de consulta común, como <phrase>SQL</phrase>    ● Tener experiencia con las actividades de extracción, transformación, carga y modelado de datos    ● Haber desarrollado aplicaciones mediante un lenguaje de programación común, como <phrase>Python</phrase>    ● Estar familiarizados con <phrase>el</phrase> aprendizaje automático o las estadísticas   Notas sobre <phrase>la</phrase> Cuenta de <phrase>Google</phrase>:  • Para poder registrarse <phrase>en</phrase> <phrase>la</phrase> prueba gratuita de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform, deberá tener una Cuenta de <phrase>Google</phrase> o <phrase>Gmail</phrase>, y una tarjeta de crédito o cuenta bancaria (por <phrase>el</phrase> momento, los servicios de <phrase>Google</phrase> no están disponibles <phrase>en</phrase> <phrase>China</phrase>).  • <phrase>Si</phrase> <phrase>es</phrase> cliente de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform y <phrase>su</phrase> dirección de facturación <phrase>se</phrase> encuentra <phrase>en</phrase> <phrase>la</phrase> Unión Europea (UE) o <phrase>en</phrase> Rusia, lea <phrase>la</phrase> documentación de Descripción <phrase>General</phrase> del IVA <phrase>en</phrase>: https://cloud.google.com/billing/docs/resources/vat-overview  • Puede encontrar más Preguntas frecuentes sobre <phrase>la</phrase> prueba gratuita de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>en</phrase>: https://cloud.google.com/<phrase>free</phrase>-trial/
This course provides an unique opportunity for you to learn <phrase>key components</phrase> of <phrase>text mining</phrase> and analytics aided by the <phrase>real world</phrase> datasets and the <phrase>text mining</phrase> <phrase>toolkit</phrase> <phrase>written in Java</phrase>. Hands-on experience in core <phrase>text mining</phrase> techniques including text preprocessing, <phrase>sentiment analysis</phrase>, and topic modeling help learners be trained to be a competent <phrase>data</phrase> scientists.   Empowered by bringing <phrase>lecture notes</phrase> together with lab sessions based on the y-TextMiner <phrase>toolkit</phrase> developed for the class, learners will be able to develop interesting <phrase>text mining</phrase> applications.
В этом курсе мы поговорим о трендах и классификаторах. Анализ трендов помогает ответить на вопросы вроде: растут ли продажи, увеличивается ли количество пользователей сервиса? Если есть рост, то случайность это или закономерность? Есть ли в данных сезонные колебания? Как выделить тренд и как объяснить его?  Также мы поговорим о факторном анализе, который позволяет найти скрытую переменную (или переменные), направляющие проявление множества видимых признаков. Как найти такие скрытые переменные и понять, что за ними стоит? В заключительной части курса поговорим о классификаторах, применение которых решает задачи отнесения объектов к тому или иному классу с определенной вероятностью, а также позволяет прогнозировать попадание нового объекта в определенный класс. Как предсказать исход события, зная основные характеристики действующего лица? Закончит ли слушатель курс, отдаст ли заемщик кредит? Как оценить точность прогноза и минимизировать ошибки?  Мы разберемся с устройством обозначенных методов анализа данных и попрактикуемся в их применении.
Welcome to <phrase>Supply Chain</phrase> Analytics - the <phrase>art</phrase> and <phrase>science</phrase> of applying <phrase>data</phrase> analytics to assess and improve <phrase>supply chain</phrase> performance!  A <phrase>supply chain</phrase> is a complex system with conflicting objectives of <phrase>cost efficiency</phrase> and <phrase>customer satisfaction</phrase>. <phrase>Supply chain management</phrase> is becoming increasingly <phrase>data</phrase> driven. Through the <phrase>real-life</phrase> story and <phrase>data</phrase> of a <phrase>major</phrase> US <phrase>telecommunication</phrase> <phrase>company</phrase>, you will learn the analytics tools / skills to diagnose and optimize a <phrase>supply chain</phrase>. Upon completion of this course, you will be able to  1. Use <phrase>data</phrase> analytics to assess the impact of various strategies on all aspects of a <phrase>supply chain</phrase>, from inventory, shipping, to <phrase>warehouse</phrase> <phrase>order fulfillment</phrase>, store operations and <phrase>customer satisfaction</phrase>.  2. Customize the <phrase>supply chain</phrase> strategy by product to improve the overall <phrase>cost efficiency</phrase> without sacrificing <phrase>customer service</phrase>.  3. Obtain hands-on experience on the <phrase>application</phrase> and financial impact of analytics in integrated <phrase>supply chain</phrase> and <phrase>logistics</phrase> planning.  VASTA (name disguised) is a <phrase>major</phrase> <phrase>wireless</phrase> <phrase>carrier</phrase> in the US selling <phrase>cell</phrase> <phrase>phones</phrase> through a national network of <phrase>retail</phrase> stores. Recently, it wrote off a huge amount of obsolete inventory each year and was suffering a significant cost inefficiency in an increasingly stagnant market. VASTA must assess the competitive environment, and renovate its <phrase>supply chain</phrase> to stay competitive. At the end of this course, you will help VASTA save $billions on <phrase>supply chain</phrase> cost and retain its <phrase>leadership</phrase> in a stagnant and saturated market.  I hope you enjoy the course!
Career prospects are bright for those qualified to work in <phrase>healthcare</phrase> <phrase>data</phrase> analytics. Perhaps you work in <phrase>data</phrase> analytics, but are considering a move into <phrase>healthcare</phrase> where your work can improve people’s <phrase>quality of life</phrase>. If so, this course gives you a glimpse into why this work matters, what you’d be doing in this role, and what <phrase>takes place</phrase> on the Path to Value where <phrase>data</phrase> is gathered from patients at the point of care, moves into <phrase>data</phrase> warehouses to be prepared for analysis, then moves along the <phrase>data</phrase> <phrase>pipeline</phrase> to be transformed into valuable insights that can save lives, <phrase>reduce costs</phrase>, to improve <phrase>healthcare</phrase> and make it more accessible and affordable. Perhaps you work in <phrase>healthcare</phrase> but are considering a <phrase>transition</phrase> into a new role. If so, this course will help you see if this career path is one you want to pursue. You’ll get an overview of common <phrase>data</phrase> models and their uses. You’ll learn how various systems integrate <phrase>data</phrase>, how to ensure clear <phrase>communication</phrase>, measure and improve <phrase>data</phrase> quality. <phrase>Data</phrase> analytics in <phrase>healthcare</phrase> serves <phrase>doctors</phrase>, clinicians, patients, care providers, and those who <phrase>carry out</phrase> the <phrase>business</phrase> of improving <phrase>health</phrase> outcomes. This course of study will give you a clear picture of <phrase>data analysis</phrase> in today’s fast-changing <phrase>healthcare</phrase> field and the opportunities it holds for you.
This course introduces students to the <phrase>science</phrase> of <phrase>business</phrase> analytics while casting a keen eye toward the artful use of numbers found in the <phrase>digital</phrase> space. The goal is to provide businesses and managers with the foundation needed to apply <phrase>data</phrase> analytics to <phrase>real-world</phrase> challenges they confront <phrase>daily</phrase> in their <phrase>professional</phrase> lives. Students will learn to identify the ideal analytic tool for their specific needs; understand valid and reliable ways to collect, analyze, and visualize <phrase>data</phrase>; and utilize <phrase>data</phrase> in <phrase>decision making</phrase> for their agencies, organizations or clients.
This is the second course in the <phrase>Data</phrase> Warehousing for <phrase>Business Intelligence</phrase> specialization. Ideally, the courses should be taken in <phrase>sequence</phrase>.  In this course, you will learn exciting concepts and skills for designing <phrase>data</phrase> warehouses and creating <phrase>data integration</phrase> workflows.  These are fundamental skills for <phrase>data warehouse</phrase> developers and administrators. You will have hands-on experience for <phrase>data warehouse</phrase> <phrase>design</phrase> and use <phrase>open source</phrase> <phrase>products</phrase> for manipulating pivot tables and creating <phrase>data integration</phrase> workflows.You will also gain conceptual background about maturity models, architectures, multidimensional models, and <phrase>management</phrase> practices, providing an organizational perspective about <phrase>data warehouse</phrase> development. If you are currently a <phrase>business</phrase> or <phrase>information technology</phrase> <phrase>professional</phrase> and want to become a <phrase>data warehouse</phrase> designer or administrator, this course will give you the <phrase>knowledge</phrase> and skills to do that. By the end of the course, you will have the <phrase>design</phrase> experience, <phrase>software</phrase> background, and organizational <phrase>context</phrase> that prepares you to succeed with <phrase>data warehouse</phrase> development projects.    In this course, you will create <phrase>data warehouse</phrase> designs and <phrase>data integration</phrase> workflows that satisfy the <phrase>business intelligence</phrase> needs of organizations. When you’re done with this course, you’ll be able to:    * Evaluate an <phrase>organization</phrase> for <phrase>data warehouse</phrase> maturity and <phrase>business architecture</phrase> alignment;    * Create a <phrase>data warehouse</phrase> <phrase>design</phrase> and reflect on <phrase>alternative</phrase> <phrase>design</phrase> methodologies and <phrase>design</phrase> goals;    * Create <phrase>data integration</phrase> workflows using prominent <phrase>open source software</phrase>;    * Reflect on the role of change <phrase>data</phrase>, refresh constraints, refresh <phrase>frequency</phrase> <phrase>trade</phrase>-offs, and <phrase>data</phrase> quality goals in <phrase>data integration</phrase> process <phrase>design</phrase>; and    * Perform operations on pivot tables to satisfy typical <phrase>business analysis</phrase> requests using prominent <phrase>open source software</phrase>
This course empowers learners to develop <phrase>image processing</phrase> programs and leverage <phrase>MATLAB</phrase> functionalities to implement sophisticated image applications. It provides a rich explanation of the fundamentals of <phrase>computer</phrase> vision’s <phrase>lower</phrase>- and mid-level tasks by examining several principle approaches and their historical roots. By the end of the course, learners are prepared to analyze images in <phrase>frequency domain</phrase>. Topics include image filters, <phrase>image features</phrase> and matching, and <phrase>image segmentation</phrase>.     This course is ideal for anyone curious about or interested in exploring the concepts of <phrase>computer vision</phrase>. It is also useful for those who desire a refresher course in <phrase>mathematical</phrase> concepts of <phrase>computer vision</phrase>. Learners should have <phrase>basic</phrase> <phrase>programming</phrase> skills and experience (understanding of for loops, if/else statements), specifically in <phrase>MATLAB</phrase> (<phrase>Mathworks</phrase> provides the basics here: https://www.mathworks.com/learn/tutorials/<phrase>matlab</phrase>-onramp.html).  Learners should also be familiar with the following: <phrase>basic</phrase> <phrase>linear algebra</phrase> (matrix <phrase>vector</phrase> operations and <phrase>notation</phrase>), 3D co-ordinate systems and transformations, <phrase>basic</phrase> <phrase>calculus</phrase> (derivatives and <phrase>integration</phrase>) and <phrase>basic</phrase> <phrase>probability</phrase> (<phrase>random variables</phrase>).       Material includes online lectures, videos, demos, hands-on exercises, project work, readings and discussions. Learners gain experience writing <phrase>computer vision</phrase> programs through online labs using <phrase>MATLAB</phrase>* and supporting toolboxes.  This is the second course in the <phrase>Computer</phrase> Vision specialization that lays the groundwork necessary for designing sophisticated vision applications. To learn more about the specialization, check out a <phrase>video</phrase> overview at https://youtu.be/OfxVUSCPXd0.    * A <phrase>free</phrase> license to <phrase>install</phrase> <phrase>MATLAB</phrase> for the duration of the course is available from <phrase>MathWorks</phrase>.
This course introduces the world of <phrase>database</phrase> systems. It provides the foundation that will enable learners to <phrase>master</phrase> skills in <phrase>data</phrase> modeling and <phrase>information</phrase>, as well as extract <phrase>information</phrase> using existing <phrase>database management</phrase> systems. The following main topics are <phrase>covered</phrase>: <phrase>database design</phrase>/modeling, <phrase>data</phrase> storage and indexing, <phrase>query processing</phrase>/optimization, <phrase>transaction management</phrase>, <phrase>database security</phrase>, and <phrase>data</phrase> analytics.
People apply <phrase>Bayesian</phrase> methods in many areas: from <phrase>game development</phrase> to <phrase>drug discovery</phrase>. They give superpowers to many <phrase>machine learning</phrase> <phrase>algorithms</phrase>: handling <phrase>missing data</phrase>, extracting much more <phrase>information</phrase> from small datasets. <phrase>Bayesian</phrase> methods also allow us to estimate uncertainty in predictions, which is a desirable feature for fields like <phrase>medicine</phrase>.  When applied to <phrase>deep learning</phrase>, <phrase>Bayesian</phrase> methods allow you to compress your models a hundred folds, and automatically tune hyperparameters, saving your time and <phrase>money</phrase>. In six weeks we will discuss the basics of <phrase>Bayesian</phrase> methods: from how to define a <phrase>probabilistic model</phrase> to how to make predictions from it. We will see how one can automate this <phrase>workflow</phrase> and how to speed it up using some advanced techniques.  We will also see applications of <phrase>Bayesian</phrase> methods to <phrase>deep learning</phrase> and how to generate new images with it. We will see how new <phrase>drugs</phrase> that <phrase>cure</phrase> severe diseases be found with <phrase>Bayesian</phrase> methods.  Do you have technical problems? Write to us: <phrase>coursera</phrase>@hse.ru
In this course, learners will be introduced to the field of <phrase>statistics</phrase>, including where <phrase>data</phrase> come from, <phrase>study design</phrase>, <phrase>data management</phrase>, and exploring and visualizing <phrase>data</phrase>. Learners will identify different types of <phrase>data</phrase>, and learn how to visualize, analyze, and interpret summaries for both <phrase>univariate</phrase> and multivariate <phrase>data</phrase>. Learners will also be introduced to the <phrase>differences between</phrase> <phrase>probability</phrase> and non-<phrase>probability</phrase> <phrase>sampling</phrase> from larger populations, the idea of how sample estimates vary, and how inferences can be made about larger populations based on <phrase>probability</phrase> <phrase>sampling</phrase>.  At the end of each week, learners will apply the statistical concepts they’ve learned using <phrase>Python</phrase> within the course environment. During these lab-based sessions, learners will discover the different uses of <phrase>Python</phrase> as a tool, including the <phrase>Numpy</phrase>, <phrase>Pandas</phrase>, Statsmodels, Matplotlib, and Seaborn <phrase>libraries</phrase>. <phrase>Tutorial</phrase> videos are provided to walk learners through the creation of visualizations and <phrase>data management</phrase>, all within <phrase>Python</phrase>. This course utilizes the Jupyter Notebook environment within <phrase>Coursera</phrase>.
Course Overview: https://youtu.be/JgFV5qzAYno  <phrase>Python</phrase> is now becoming the number 1 <phrase>programming language</phrase> for <phrase>data science</phrase>. Due to python’s simplicity and <phrase>high</phrase> readability, it is gaining its importance in the <phrase>financial industry</phrase>.  The course combines both <phrase>python</phrase> coding and statistical concepts and applies into analyzing financial <phrase>data</phrase>, such as <phrase>stock</phrase> <phrase>data</phrase>.  By the end of the course, you can achieve the following using <phrase>python</phrase>:  - <phrase>Import</phrase>, pre-process, save and visualize financial <phrase>data</phrase> into <phrase>pandas</phrase> Dataframe  - Manipulate the existing financial <phrase>data</phrase> by generating new variables using multiple columns  - Recall and apply the important statistical concepts (<phrase>random variable</phrase>, <phrase>frequency</phrase>, distribution, <phrase>population</phrase> and sample, <phrase>confidence interval</phrase>, <phrase>linear regression</phrase>, etc. ) into financial contexts  - Build a trading <phrase>model</phrase> using multiple <phrase>linear regression</phrase> <phrase>model</phrase>   - Evaluate the performance of the trading <phrase>model</phrase> using different <phrase>investment</phrase> indicators  Jupyter Notebook environment is configured in the course platform for practicing <phrase>python</phrase> coding without installing any <phrase>client applications</phrase>.
This course will continue the introduction to <phrase>Python</phrase> <phrase>programming</phrase> that started with <phrase>Python</phrase> <phrase>Programming</phrase> Essentials and <phrase>Python</phrase> <phrase>Data</phrase> Representations.  We'll learn about <phrase>reading</phrase>, storing, and processing tabular <phrase>data</phrase>, which are common tasks.  We will also teach you about CSV files and Python's support for <phrase>reading</phrase> and writing them.  CSV files are a generic, <phrase>plain text</phrase> <phrase>file format</phrase> that allows you to exchange tabular <phrase>data</phrase> between different programs. These concepts and skills will help you to further extend your <phrase>Python</phrase> <phrase>programming</phrase> <phrase>knowledge</phrase> and allow you to process more complex <phrase>data</phrase>.  By the end of the course, you will be comfortable working with tabular <phrase>data</phrase> in <phrase>Python</phrase>. This will extend your <phrase>Python</phrase> <phrase>programming</phrase> <phrase>expertise</phrase>, enabling you to write a wider <phrase>range</phrase> of scripts using <phrase>Python</phrase>.  This course uses <phrase>Python</phrase> 3.  While most <phrase>Python</phrase> programs continue to use <phrase>Python</phrase> 2, <phrase>Python</phrase> 3 is the future of the <phrase>Python programming language</phrase>. This course uses <phrase>basic</phrase> desktop <phrase>Python</phrase> <phrase>development environments</phrase>, allowing you to <phrase>run</phrase> <phrase>Python</phrase> programs directly on your <phrase>computer</phrase>.
Spatial (map) is considered as a core <phrase>infrastructure</phrase> of modern IT world, which is substantiated by <phrase>business</phrase> transactions of <phrase>major</phrase> IT companies such as <phrase>Apple</phrase>, <phrase>Google</phrase>, <phrase>Microsoft</phrase>, <phrase>Amazon</phrase>, <phrase>Intel</phrase>, and Uber, and even motor companies such as <phrase>Audi</phrase>, <phrase>BMW</phrase>, and <phrase>Mercedes</phrase>. Consequently, they are bound to hire more and more <phrase>spatial data</phrase> scientists.  Based on such <phrase>business</phrase> trend, this course is designed to present a <phrase>firm</phrase> understanding of <phrase>spatial data</phrase> <phrase>science</phrase> to the learners, who would have a <phrase>basic</phrase> <phrase>knowledge</phrase> of <phrase>data science</phrase> and <phrase>data analysis</phrase>, and eventually to make their <phrase>expertise</phrase> differentiated from other nominal <phrase>data</phrase> scientists and <phrase>data</phrase> analysts.  Additionally, this course could make learners realize the value of spatial <phrase>big data</phrase> and the power of <phrase>open source</phrase> software's to deal with <phrase>spatial data</phrase> <phrase>science</phrase> problems.  This course will start with defining <phrase>spatial data</phrase> <phrase>science</phrase> and answering why spatial is special from three different perspectives - <phrase>business</phrase>, <phrase>technology</phrase>, and <phrase>data</phrase> in the first week.  In the second week, four disciplines related to <phrase>spatial data</phrase> <phrase>science</phrase> - <phrase>GIS</phrase>, <phrase>DBMS</phrase>, <phrase>Data</phrase> Analytics, and <phrase>Big Data</phrase> Systems, and the related <phrase>open source</phrase> software's - QGIS, <phrase>PostgreSQL</phrase>, <phrase>PostGIS</phrase>, R, and <phrase>Hadoop</phrase> tools are introduced together.  During the third, fourth, and fifth weeks, you will learn the four disciplines one by one from the principle to applications.  In the final week, five <phrase>real world</phrase> problems and the corresponding solutions are presented with <phrase>step</phrase>-by-<phrase>step</phrase> procedures in environment of <phrase>open source</phrase> software's.
The <phrase>art</phrase> of uncovering the insights and trends in <phrase>data</phrase> has been around since ancient times. The ancient <phrase>Egyptians</phrase> used <phrase>census</phrase> <phrase>data</phrase> to <phrase>increase efficiency</phrase> in <phrase>tax</phrase> collection and they accurately predicted the flooding of the <phrase>Nile river</phrase> every year. Since then, people working in <phrase>data science</phrase> have carved out a unique and distinct field for the work they do. This field is <phrase>data science</phrase>. In this course, we will meet some <phrase>data science</phrase> practitioners and we will get an overview of what <phrase>data science</phrase> is today.  LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
Discover the <phrase>basic</phrase> concepts of <phrase>cluster analysis</phrase>, and then study a set of typical clustering methodologies, <phrase>algorithms</phrase>, and applications. This includes partitioning methods such as k-means, hierarchical methods such as <phrase>BIRCH</phrase>, and <phrase>density</phrase>-<phrase>based methods</phrase> such as <phrase>DBSCAN/OPTICS</phrase>. Moreover, learn methods for clustering validation and evaluation of clustering quality. Finally, see examples of <phrase>cluster analysis</phrase> in applications.
In this course, we will learn all the core techniques needed to make effective use of <phrase>H2O</phrase>. Even if you have no prior experience of <phrase>machine learning</phrase>, even if your <phrase>math</phrase> is weak, by the end of this course you will be able to make <phrase>machine learning</phrase> models using a <phrase>variety</phrase> of <phrase>algorithms</phrase>. We will be using <phrase>linear models</phrase>, <phrase>random forest</phrase>, GBMs and of course <phrase>deep learning</phrase>, as well as some <phrase>unsupervised learning</phrase> <phrase>algorithms</phrase>. You will also be able to evaluate your models and choose the best <phrase>model</phrase> to suit not just your <phrase>data</phrase> but the other <phrase>business</phrase> restraints you may be under.
The Code <phrase>Free</phrase> <phrase>Data Science</phrase> class is designed for learners seeking to gain or expand their <phrase>knowledge</phrase> in the <phrase>area</phrase> of <phrase>Data Science</phrase>.  Participants will receive the <phrase>basic</phrase> training in effective predictive analytic approaches accompanying the growing discipline of <phrase>Data Science</phrase> without any <phrase>programming</phrase> requirements.  <phrase>Machine Learning</phrase> methods will be presented by utilizing the KNIME Analytics Platform to discover patterns and relationships in <phrase>data</phrase>. Predicting <phrase>future trends</phrase> and behaviors allows for proactive, <phrase>data</phrase>-driven decisions.  During the class learners will acquire new skills to apply predictive <phrase>algorithms</phrase> to real <phrase>data</phrase>, evaluate, validate and interpret the <phrase>results</phrase> without any pre requisites for any kind of <phrase>programming</phrase>.  Participants will gain the essential skills to <phrase>design</phrase>, build, verify and <phrase>test</phrase> predictive models.   You Will Learn •	How to <phrase>design</phrase> <phrase>Data Science</phrase> workflows without any <phrase>programming</phrase> involved •	Essential <phrase>Data Science</phrase> skills to <phrase>design</phrase>, build, <phrase>test</phrase> and evaluate predictive models •	<phrase>Data</phrase> Manipulation, preparation and Classification and clustering methods •	Ways to apply <phrase>Data Science</phrase> <phrase>algorithms</phrase> to real <phrase>data</phrase> and evaluate and interpret the <phrase>results</phrase>
You will learn how to build a successful <phrase>machine learning</phrase> project. If you aspire to be a technical leader in <phrase>AI</phrase>, and know how to set direction for your team's work, this course will show you how.  Much of this content has never been taught elsewhere, and is drawn from my experience building and shipping many <phrase>deep learning</phrase> <phrase>products</phrase>. This course also has two "flight simulators" that let you practice <phrase>decision-making</phrase> as a <phrase>machine learning</phrase> project leader. This provides "<phrase>industry</phrase> experience" that you might otherwise get only after years of <phrase>ML</phrase> work experience.  After 2 weeks, you will:  - Understand how to diagnose errors in a <phrase>machine learning</phrase> system, and  - Be able to prioritize the most promising directions for reducing error - Understand complex <phrase>ML</phrase> settings, such as mismatched training/<phrase>test</phrase> sets, and comparing to and/or surpassing <phrase>human</phrase>-level performance - Know how to apply <phrase>end-to-end</phrase> learning, transfer learning, and multi-<phrase>task</phrase> learning  I've seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time.  This is a standalone course, and you can take this so <phrase>long</phrase> as you have <phrase>basic</phrase> <phrase>machine learning</phrase> <phrase>knowledge</phrase>. This is the third course in the <phrase>Deep Learning</phrase> Specialization.
This 1-week, accelerated <phrase>on-demand</phrase> course builds upon <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals. Through a combination of <phrase>video</phrase> lectures, demonstrations, and hands-on labs, you'll learn how to build <phrase>streaming</phrase> <phrase>data</phrase> pipelines using <phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub and <phrase>Dataflow</phrase> to enable <phrase>real-time</phrase> <phrase>decision making</phrase>. You will also learn how to build dashboards to render tailored output for various stakeholder audience.  Prerequisites: • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals (or equivalent experience) • Some <phrase>knowledge</phrase> of <phrase>Java</phrase>  Objectives: • Understand <phrase>use-cases</phrase> for <phrase>real-time</phrase> <phrase>streaming</phrase> analytics • Use <phrase>Google</phrase> <phrase>Cloud</phrase> PubSub asynchronous <phrase>messaging</phrase> service to manage <phrase>data</phrase> events • Write <phrase>streaming</phrase> pipelines and <phrase>run</phrase> transformations where necessary • Get familiar with both sides of a <phrase>streaming</phrase> <phrase>pipeline</phrase>: <phrase>production</phrase> and consumption • Interoperate <phrase>Dataflow</phrase>, BigQuery and <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub for <phrase>real-time</phrase> <phrase>streaming</phrase> and analysis
<phrase>Apache</phrase> Spark is the de-facto standard for <phrase>large scale</phrase> <phrase>data</phrase> processing. This is the first course of a series of courses towards the <phrase>IBM</phrase> Advanced <phrase>Data Science</phrase> Specialization. We strongly believe that is is crucial for success to start learning a scalable <phrase>data science</phrase> platform since <phrase>memory</phrase> and <phrase>CPU</phrase> constraints are to most limiting factors when it comes to building advanced <phrase>machine learning</phrase> models.  In this course we teach you the fundamentals of <phrase>Apache</phrase> Spark using <phrase>python</phrase> and pyspark. We'll <phrase>introduce</phrase> <phrase>Apache</phrase> Spark in the first two weeks and learn how to apply it to compute <phrase>basic</phrase> exploratory and <phrase>data</phrase> <phrase>pre-processing</phrase> tasks in the last two weeks. Through this exercise you'll also be introduced to the most fundamental statistical measures and <phrase>data</phrase> visualization technologies.  This gives you enough <phrase>knowledge</phrase> to take over the role of a <phrase>data</phrase> <phrase>engineer</phrase> in any modern environment. But it gives you also the basis for advancing your career towards <phrase>data science</phrase>.   Please have a look at the full specialization <phrase>curriculum</phrase>: https://www.coursera.org/specializations/advanced-<phrase>data-science</phrase>-<phrase>ibm</phrase>  If you choose to take this course and earn the <phrase>Coursera</phrase> course certificate, you will also earn an <phrase>IBM</phrase> <phrase>digital</phrase> badge.  To find out more about <phrase>IBM</phrase> <phrase>digital</phrase> badges follow the link ibm.biz/badging.   After completing this course, you will be able to: •	Describe how <phrase>basic</phrase> statistical measures, are used to reveal  patterns within the <phrase>data</phrase>  •	Recognize <phrase>data</phrase> characteristics, patterns, trends, deviations or inconsistencies, and potential <phrase>outliers</phrase>. •	Identify useful techniques for working with <phrase>big data</phrase> such as <phrase>dimension</phrase> reduction and <phrase>feature selection</phrase> methods  •	Use advanced tools and charting <phrase>libraries</phrase> to:       o	<phrase>improve efficiency</phrase> of analysis of <phrase>big-data</phrase> with partitioning and parallel analysis        o	Visualize the <phrase>data</phrase> in an number of 2D and 3D formats (<phrase>Box Plot</phrase>, <phrase>Run</phrase> <phrase>Chart</phrase>, <phrase>Scatter Plot</phrase>, <phrase>Pareto</phrase> <phrase>Chart</phrase>, and <phrase>Multidimensional Scaling</phrase>)  For successful completion of the course, the following prerequisites are recommended:  •	<phrase>Basic</phrase> <phrase>programming</phrase> skills in <phrase>python</phrase> •	<phrase>Basic</phrase> <phrase>math</phrase> •	<phrase>Basic</phrase> <phrase>SQL</phrase> (you can get it easily from https://www.coursera.org/learn/<phrase>sql</phrase>-<phrase>data-science</phrase> if needed)  In <phrase>order</phrase> to complete this course, the following technologies will be used: (These technologies are introduced in the course as necessary so no previous <phrase>knowledge</phrase> is required.) •	Jupyter notebooks (brought to you by <phrase>IBM</phrase> Watson Studio for <phrase>free</phrase>) •	ApacheSpark (brought to you by <phrase>IBM</phrase> Watson Studio for <phrase>free</phrase>) •	<phrase>Python</phrase>  This course takes four weeks, 4-6h per week
This course prepares you to deal with advanced <phrase>clinical data</phrase> <phrase>science</phrase> topics and techniques including temporal and <phrase>research</phrase> quality analysis.
<phrase>Mathematical</phrase> Matrix Methods lie at the <phrase>root</phrase> of most methods of <phrase>machine learning</phrase> and <phrase>data analysis</phrase> of tabular <phrase>data</phrase>.  Learn the basics of Matrix Methods, including matrix-<phrase>matrix multiplication</phrase>, <phrase>solving linear</phrase> equations, <phrase>orthogonality</phrase>, and best <phrase>least squares</phrase> approximation.   Discover the <phrase>Singular Value Decomposition</phrase> that plays a fundamental role in <phrase>dimensionality reduction</phrase>, <phrase>Principal Component Analysis</phrase>, and <phrase>noise reduction</phrase>.  Optional examples using <phrase>Python</phrase> are used to illustrate the concepts and allow the learner to experiment with the <phrase>algorithms</phrase>.
Willkommen im Kurs zur Kunst und Wissenschaft <phrase>des</phrase> maschinellen Lernens. In diesem Kurs eignen Sie sich die grundlegenden Kompetenzen rund um <phrase>ML</phrase>, gutes Urteilsvermögen und Experimentierfreudigkeit an, die für die Feinabstimmung und Optimierung Ihres <phrase>ML</phrase>-Modells für bestmögliche Leistung erforderlich sind.     Außerdem informieren wir Sie über die zahlreichen Optimierungsmittel, die beim Trainieren eines Modells ins Spiel kommen. Sie werden diese zuerst manuell anpassen, um ihre Auswirkung auf die Leistung zu beobachten. Sobald Sie <phrase>mit</phrase> den Optimierungsmitteln – <phrase>auch</phrase> bekannt als Hyperparameter – vertraut sind, lernen Sie, wie Sie diese automatisch mithilfe der <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> auf der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform anpassen können.
<phrase>Health</phrase> <phrase>data</phrase> are notable for how many types there are, how complex they are, and how serious it is to get them straight. These <phrase>data</phrase> are used for treatment of the patient from whom they derive, but also for other uses. Examples of such <phrase>secondary</phrase> use of <phrase>health</phrase> <phrase>data</phrase> include <phrase>population</phrase> <phrase>health</phrase> (e.g., who requires more attention), <phrase>research</phrase> (e.g., which <phrase>drug</phrase> is more effective in practice), quality (e.g., is the institution meeting benchmarks), and <phrase>translational research</phrase> (e.g., are new technologies being applied appropriately). By the end of this course, students will recognize the different types of <phrase>health</phrase> and <phrase>healthcare</phrase> <phrase>data</phrase>, will articulate a coherent and complete question, will interpret queries designed for <phrase>secondary</phrase> use of <phrase>EHR</phrase> <phrase>data</phrase>, and will interpret the <phrase>results</phrase> of those queries.
Looking to start a career in <phrase>Deep Learning</phrase>? Look no further. This course will <phrase>introduce</phrase> you to the field of <phrase>deep learning</phrase> and help you answer many questions that people are asking nowadays, like what is <phrase>deep learning</phrase>, and how do <phrase>deep learning</phrase> models compare to <phrase>artificial neural networks</phrase>? You will learn about the different <phrase>deep learning</phrase> models and build your first <phrase>deep learning</phrase> <phrase>model</phrase> using the Keras <phrase>library</phrase>.  After completing this course, learners will be able to: •	describe what a <phrase>neural network</phrase> is, what a <phrase>deep learning</phrase> <phrase>model</phrase> is, and the difference between them. •	demonstrate an understanding of unsupervised <phrase>deep learning</phrase> models such as autoencoders and <phrase>restricted Boltzmann</phrase> machines. •	demonstrate an understanding of supervised <phrase>deep learning</phrase> models such as <phrase>convolutional neural networks</phrase> and recurrent networks. •	build <phrase>deep learning</phrase> models and networks using the Keras <phrase>library</phrase>.
Este curso rápido de <phrase>uma</phrase> semana sob demanda oferece aos participantes <phrase>uma</phrase> introdução prática <phrase>ao</phrase> <phrase>design</phrase> <phrase>e</phrase> à criação de modelos de <phrase>machine learning</phrase> no <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Com apresentações, demonstrações <phrase>e</phrase> laboratórios práticos, <phrase>os</phrase> participantes aprenderão <phrase>os</phrase> conceitos de <phrase>machine learning</phrase> (<phrase>ML</phrase>) <phrase>e</phrase> do TensorFlow, além de habilidades práticas de desenvolvimento, avaliação <phrase>e</phrase> produção de modelos de <phrase>ML</phrase>.    OBJETIVOS    <phrase>Os</phrase> participantes desenvolverão as seguintes habilidades:     ● Identificar casos de uso de <phrase>machine learning</phrase>     ● Criar um modelo de <phrase>ML</phrase> usando o TensorFlow     ● Criar modelos de <phrase>ML</phrase> escalonáveis <phrase>e</phrase> implantáveis usando o <phrase>Cloud</phrase> <phrase>ML</phrase>     ● Entender a importância do pré-processamento <phrase>e</phrase> <phrase>da</phrase> combinação de atributos     ● Incorporar conceitos avançados de <phrase>ML</phrase> <phrase>nos</phrase> modelos criados     ● Produzir modelos de <phrase>ML</phrase> treinados      PRÉ-REQUISITOS    Para aproveitar <phrase>ao</phrase> máximo este curso, <phrase>os</phrase> participantes precisam ter:     ● Concluído o curso <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals OU experiência equivalente     ● Proficiência básica <phrase>em</phrase> linguagem de consulta comum, como <phrase>SQL</phrase>     ● Experiência com atividades de extração, transformação, carga <phrase>e</phrase> modelagem de dados     ● Desenvolvido aplicativos com linguagem de programação comum, como <phrase>Python</phrase>     ● Familiaridade com <phrase>machine learning</phrase> <phrase>e</phrase>/ou estatísticas    Observações sobre a Conta do <phrase>Google</phrase>:  • Para <phrase>se</phrase> inscrever <phrase>na</phrase> avaliação gratuita do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform, você precisa de <phrase>uma</phrase> Conta do <phrase>Google</phrase>/<phrase>Gmail</phrase>, além de um cartão de crédito ou <phrase>uma</phrase> conta bancária. <phrase>Os</phrase> serviços do <phrase>Google</phrase> estão temporariamente indisponíveis <phrase>na</phrase> <phrase>China</phrase>.  • <phrase>Se</phrase> você for um cliente do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform com endereço de faturamento <phrase>na</phrase> União Europeia (UE) <phrase>e</phrase> <phrase>na</phrase> Rússia, <phrase>leia</phrase> a documentação de descrição geral sobre o Imposto sobre Valor Agregado (IVA) <phrase>em</phrase>: https://cloud.google.com/billing/docs/resources/vat-overview  • Veja mais perguntas frequentes sobre a avaliação gratuita do <phrase>Google</phrase> <phrase>Cloud</phrase> Platform no site: https://cloud.google.com/<phrase>free</phrase>-trial/
Manufacturers are increasingly utilizing <phrase>machine tools</phrase> that are self-aware – they perceive their own states and the <phrase>state</phrase> of the surrounding environment – and are able to make decisions related to machine <phrase>activity</phrase> processes. This is called intelligent <phrase>machining</phrase>, and through this course students will receive a primer on its background, tools and related terminology.   Learn how the <phrase>integration</phrase> of smart sensors and controls are helping to improve <phrase>productivity</phrase>. You’ll be exposed to various sensors and sensing techniques, <phrase>process control</phrase> strategies, and <phrase>open architecture</phrase> systems that can be leveraged to enable intelligent <phrase>machining</phrase>. This course will prepare you to contribute to the implementation of intelligent <phrase>machining</phrase> projects.   Main concepts of this course will be delivered through lectures, readings, discussions and various videos.   This is the fifth course in the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> & <phrase>Design</phrase> <phrase>Technology</phrase> specialization that explores the many facets of manufacturing’s “Fourth <phrase>Revolution</phrase>,”  aka <phrase>Industry</phrase> 4.0, and features a culminating project involving creation of a roadmap to achieve a self-established DMD-related <phrase>professional</phrase> goal.  To learn more about the <phrase>Digital</phrase> <phrase>Manufacturing</phrase> and <phrase>Design</phrase> <phrase>Technology</phrase> specialization, please <phrase>watch</phrase> the overview <phrase>video</phrase> by copying and pasting the following link into your <phrase>web browser</phrase>: https://youtu.be/wETK1O9c-<phrase>CA</phrase>
<phrase>Ce</phrase> cours à <phrase>la</phrase> demande s'appuie sur <phrase>la</phrase> formation <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals. Il s'agit d'un cours accéléré, que vous pourrez effectuer <phrase>en</phrase> une semaine. Grâce à une série de conférences vidéo, de démonstrations et d'ateliers, vous apprendrez à créer <phrase>des</phrase> pipelines de <phrase>flux</phrase> de données à l'aide de <phrase>Google</phrase> <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub et de <phrase>Dataflow</phrase>. Objectif : faciliter <phrase>la</phrase> prise de décisions <phrase>en</phrase> temps réel. Vous apprendrez également à créer <phrase>des</phrase> tableaux de bord pour présenter <phrase>des</phrase> résultats personnalisés à différents groupes d'intervenants.   Prérequis : • Avoir suivi <phrase>la</phrase> formation <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals (ou disposer d'une expérience équivalente) • Quelques connaissances <phrase>en</phrase> <phrase>Java</phrase>  Objectifs : • Savoir quand utiliser l'analyse de <phrase>flux</phrase> <phrase>en</phrase> temps réel • Gérer <phrase>les</phrase> événements de données avec le service de messagerie asynchrone <phrase>Google</phrase> <phrase>Cloud</phrase> PubSub • Rédiger <phrase>des</phrase> pipelines de <phrase>flux</phrase> de données et effectuer <phrase>des</phrase> transformations le <phrase>cas</phrase> échéant • Maîtriser <phrase>les</phrase> deux facettes d'un <phrase>pipeline</phrase> de <phrase>flux</phrase> de données : <phrase>production</phrase> et consommation • Faire interagir <phrase>Dataflow</phrase>, BigQuery et <phrase>Cloud</phrase> <phrase>Pub</phrase>/Sub pour obtenir <phrase>des</phrase> analyses et <phrase>des</phrase> <phrase>flux</phrase> <phrase>en</phrase> temps réel
これは、<phrase>Data</phrase> to Insights 専門講座の 2 番目のコースです。ここでは、新しい外部データセットを BigQuery に取り込み、<phrase>Google</phrase> データポータルで可視化する方法について説明します。また、複数テーブルの JOIN と <phrase>UNION</phrase> など、中級者向けの <phrase>SQL</phrase> の概念についても説明します。JOIN や <phrase>UNION</phrase> を使用すると、複数のデータソースのデータを分析できます。    注: すでに <phrase>SQL</phrase> に関する知識をお持ちの方も、BigQuery に固有の要素（クエリ キャッシュやテーブル ワイルドカードの処理など）について学ぶことができます。    >>> この専門講座に登録すると、よくある質問に記載されているとおり Qwiklabs の利用規約に同意したことになります。詳細については、https://qwiklabs.com/terms_of_service をご覧ください。<<<
Statistical experiment <phrase>design</phrase> and analytics are at the <phrase>heart</phrase> of <phrase>data science</phrase>.  In this course you will <phrase>design</phrase> statistical experiments and analyze the <phrase>results</phrase> using modern methods.  You will also explore the common pitfalls in interpreting statistical arguments, especially those associated with <phrase>big data</phrase>.  Collectively, this course will help you internalize a core set of practical and effective <phrase>machine learning</phrase> methods and concepts, and apply them to solve some <phrase>real world</phrase> problems.     Learning Goals: After completing this course, you will be able to: 1. <phrase>Design</phrase> effective experiments and analyze the <phrase>results</phrase> 2. Use resampling methods to make clear and bulletproof statistical arguments without invoking esoteric <phrase>notation</phrase> 3. Explain and apply a core set of classification methods of increasing complexity (rules, <phrase>trees</phrase>, random <phrase>forests</phrase>), and associated <phrase>optimization methods</phrase> (<phrase>gradient</phrase> descent and variants) 4. Explain and apply a set of <phrase>unsupervised learning</phrase> concepts and methods 5. Describe the common <phrase>idioms</phrase> of <phrase>large-scale</phrase> <phrase>graph</phrase> analytics, including structural query, traversals and recursive queries, <phrase>PageRank</phrase>, and <phrase>community detection</phrase>
<phrase>Relational Database</phrase> Support for <phrase>Data</phrase> Warehouses is the third course in the <phrase>Data</phrase> Warehousing for <phrase>Business Intelligence</phrase> specialization. In this course, you'll use analytical elements of <phrase>SQL</phrase> for answering <phrase>business intelligence</phrase> questions. You'll learn features of <phrase>relational</phrase> <phrase>database management</phrase> systems for managing <phrase>summary</phrase> <phrase>data</phrase> commonly used in <phrase>business intelligence</phrase> reporting. Because of the importance and difficulty of managing implementations of <phrase>data</phrase> warehouses, we'll also delve into storage architectures, scalable <phrase>parallel processing</phrase>, <phrase>data</phrase> governance, and <phrase>big data</phrase> impacts.
In this course, you will learn how to <phrase>solve problems</phrase> with large, <phrase>high</phrase>-dimensional, and potentially infinite <phrase>state</phrase> spaces. You will see that estimating value functions can be cast as a <phrase>supervised learning</phrase> problem---<phrase>function</phrase> approximation---allowing you to build agents that carefully balance generalization and <phrase>discrimination</phrase> in <phrase>order</phrase> to maximize reward. We will begin this journey by investigating how our policy evaluation or prediction methods like <phrase>Monte Carlo</phrase> and <phrase>TD</phrase> can be extended to the <phrase>function</phrase> approximation setting. You will learn about feature <phrase>construction</phrase> techniques for <phrase>RL</phrase>, and representation learning via <phrase>neural networks</phrase> and backprop. We conclude this course with a deep-dive into policy <phrase>gradient</phrase> methods; a way to learn policies directly without learning a value <phrase>function</phrase>. In this course you will solve two continuous-<phrase>state</phrase> control tasks and investigate the benefits of policy <phrase>gradient</phrase> methods in a continuous-<phrase>action</phrase> environment. This course strongly builds on the fundamentals of Courses 1 and 2.  By the end of this course, you will be able to:   Understand how to use <phrase>supervised learning</phrase> approaches to approximate value functions Understand objectives for prediction (value estimation) under <phrase>function</phrase> approximation Implement <phrase>TD</phrase> with <phrase>function</phrase> approximation (<phrase>state</phrase> <phrase>aggregation</phrase>), on an environment with an infinite <phrase>state space</phrase> (continuous <phrase>state space</phrase>) Understand fixed basis and <phrase>neural network</phrase> approaches to feature <phrase>construction</phrase>  Implement <phrase>TD</phrase> with <phrase>neural network</phrase> <phrase>function</phrase> approximation in a continuous <phrase>state</phrase> environment Understand new difficulties in exploration when moving to <phrase>function</phrase> approximation Contrast discounted problem formulations for control versus an <phrase>average</phrase> reward problem formulation Implement expected Sarsa and <phrase>Q-learning</phrase> with <phrase>function</phrase> approximation on a continuous <phrase>state</phrase> control <phrase>task</phrase> Understand objectives for directly estimating policies (policy <phrase>gradient</phrase> objectives) Implement a policy <phrase>gradient</phrase> <phrase>method</phrase> (called <phrase>Actor</phrase>-Critic) on a discrete <phrase>state</phrase> environment
Learn the <phrase>general</phrase> concepts of <phrase>data mining</phrase> along with <phrase>basic</phrase> methodologies and applications. Then dive into one subfield in <phrase>data mining</phrase>: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in <phrase>data mining</phrase>. We will also <phrase>introduce</phrase> methods for <phrase>pattern-based</phrase> classification and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional <phrase>data</phrase>, discuss pattern evaluation measures, and study methods for <phrase>mining</phrase> diverse kinds of patterns, sequential patterns, and sub-<phrase>graph</phrase> patterns.
The simple <phrase>spreadsheet</phrase> is one of the most powerful <phrase>data analysis</phrase> tools that exists, and it’s available to almost anyone. <phrase>Major</phrase> corporations and <phrase>small businesses</phrase> alike use <phrase>spreadsheet</phrase> models to determine where key measures of their success are now, and where they are likely to be in the future. But in <phrase>order</phrase> to get the most out of a <phrase>spreadsheet</phrase>, you have the know-how to use it. This course is designed to give you <phrase>an introduction</phrase> to <phrase>basic</phrase> <phrase>spreadsheet</phrase> tools and formulas so that you can begin <phrase>harness</phrase> the power of <phrase>spreadsheets</phrase> to map the <phrase>data</phrase> you have now and to predict the <phrase>data</phrase> you may have in the future. Through <phrase>short</phrase>, easy-to-follow demonstrations, you’ll learn how to use <phrase>Excel</phrase> or Sheets so that you can begin to build models and <phrase>decision trees</phrase> in future courses in this Specialization.  <phrase>Basic</phrase> familiarity with, and access to, <phrase>Excel</phrase> or Sheets is required.
What are some of the most popular <phrase>data science</phrase> tools, how do you use them, and what are their features? In this course, you'll learn about Jupyter Notebooks, RStudio <phrase>IDE</phrase>, <phrase>Apache</phrase> <phrase>Zeppelin</phrase> and <phrase>Data Science</phrase> Experience. You will learn about what each tool is used for, what <phrase>programming</phrase> languages they can execute, their features and limitations. With the tools hosted in the <phrase>cloud</phrase> on <phrase>Cognitive</phrase> Class Labs, you will be able to <phrase>test</phrase> each tool and follow instructions to <phrase>run</phrase> simple code in <phrase>Python</phrase>, R or Scala. To end the course, you will create a final project with a Jupyter Notebook on <phrase>IBM</phrase> <phrase>Data Science</phrase> Experience and demonstrate your proficiency preparing a notebook, writing <phrase>Markdown</phrase>, and sharing your work with your <phrase>peers</phrase>.  LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
Welcome! In this course learners will develop <phrase>expertise</phrase> in <phrase>basic</phrase> <phrase>magnetic resonance imaging</phrase> (<phrase>MRI</phrase>) <phrase>physics</phrase> and principles and gain <phrase>knowledge</phrase> of many different <phrase>data acquisition</phrase> strategies in <phrase>MRI</phrase>. In particular, learners will get to know what is <phrase>magnetic resonance</phrase> phenomenon, how <phrase>magnetic resonance</phrase> signals are generated, how an image can be formulated using <phrase>MRI</phrase>, how <phrase>soft tissue</phrase> contrast can change with imaging parameters. Also introduced will be <phrase>MR</phrase> imaging sequences of <phrase>spin</phrase> echo, <phrase>gradient</phrase> echo, fast <phrase>spin</phrase> echo, echo planar imaging, inversion recovery, etc.
Курс предназначен для опытных пользователей MS <phrase>Excel</phrase>, сталкивающихся с задачей создания системы показателей деятельности компании и представления их в наглядном и удобном для принятия управленческих решений. Данный курс - четвертый в рамках специализации "Практики анализа экономических данных: от простого к сложному".  Для эффективного анализа деятельности компании необходимо выявить и создать систему показателей ее деятельности и представить их в наглядном и удобном для принятия управленческих решений виде. Четвертый курс специализации «Практики анализа данных от простого к сложному» посвящен обсуждению кейсов, связанных с таким анализом. На примере реальных задач рассматриваются возможности создания системы показателей деятельности компании, примеры эффективного представления отчетов и демонстрируется логика и технология создания аналитических панелей в среде <phrase>Microsoft</phrase> Power <phrase>BI</phrase>. Слушатель пройдет все этапы разработки информационно-аналитических панелей: от формирования требований до полной разработки дашбордов разного уровня сложности и инструментальной насыщенности. Изучение построено по принципу: от простого к сложному. Слушатель получит практические навыки разработки аналитической среды разного уровня сложности, навыки работы с данными как ресурсом для принятия решений.
Learn the <phrase>general</phrase> concepts of <phrase>data mining</phrase> along with <phrase>basic</phrase> methodologies and applications. Then dive into one subfield in <phrase>data mining</phrase>: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in <phrase>data mining</phrase>. We will also <phrase>introduce</phrase> methods for <phrase>data</phrase>-driven phrase <phrase>mining</phrase> and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional <phrase>data</phrase>, discuss pattern evaluation measures, and study methods for <phrase>mining</phrase> diverse kinds of patterns, sequential patterns, and sub-<phrase>graph</phrase> patterns.
With <phrase>marketers</phrase> are poised to be the largest users of <phrase>data</phrase> within the <phrase>organization</phrase>, there is a need to make sense of the <phrase>variety</phrase> of <phrase>consumer</phrase> <phrase>data</phrase> that the <phrase>organization</phrase> collects. Surveys, transaction histories and billing records can all provide <phrase>insight into</phrase> consumers’ future behavior, provided that they are interpreted correctly. In Introduction to <phrase>Marketing</phrase> Analytics, we <phrase>introduce</phrase> the tools that learners will need to convert <phrase>raw data</phrase> into <phrase>marketing</phrase> insights. The included exercises are conducted using <phrase>Microsoft Excel</phrase>, ensuring that learners will have the tools they need to extract <phrase>information</phrase> from the <phrase>data</phrase> available to them. The course provides learners with exposure to essential tools including <phrase>exploratory data analysis</phrase>, as well as <phrase>regression</phrase> methods that can be used to investigate the impact of <phrase>marketing</phrase> <phrase>activity</phrase> on <phrase>aggregate data</phrase> (e.g., sales) and on individual-level choice <phrase>data</phrase> (e.g., <phrase>brand</phrase> choices).   To successfully complete the assignments in this course, you will require <phrase>Microsoft Excel</phrase>. If you do not have <phrase>Excel</phrase>, you can download a <phrase>free</phrase> 30-day trial here: https://products.office.com/<phrase>en</phrase>-us/try
Learn to use tools from the Bioconductor project to perform analysis of <phrase>genomic</phrase> <phrase>data</phrase>. This is the fifth course in the <phrase>Genomic</phrase> <phrase>Big Data</phrase> Specialization from <phrase>Johns Hopkins University</phrase>.
¿<phrase>Te</phrase> has planteado para qué sirven las humanidades <phrase>en</phrase> <phrase>el</phrase> siglo XXI? ¿Cómo <phrase>el</phrase> advenimiento de lo <phrase>Digital</phrase> <phrase>ha</phrase> cambiado nuestra forma de acercarnos a las Humanidades? ¿Y al revés? ¿De qué forma las Humanidades influencian sobre los diferentes aspectos <phrase>de la</phrase> Tecnología?  <phrase>Te</phrase> ofrecemos <phrase>el</phrase> primer curso online que <phrase>da</phrase> respuesta a estas preguntas. Una visión de las Humanidades Digitales desde <phrase>la</phrase> filosofía, <phrase>la</phrase> historia, <phrase>la</phrase> arqueología, <phrase>el</phrase> <phrase>arte</phrase>,… y desde las ciencias computacionales y <phrase>la</phrase> visión por ordenador.  <phrase>El</phrase> curso ofrece una  aproximación interdisciplinar a las Humanidades y <phrase>el</phrase> Patrimonio <phrase>Digital</phrase>, y a los métodos y herramientas utilizados (Bases de datos, <phrase>Open Data</phrase>, <phrase>Big Data</phrase>, Realidad Virtual, Redes Sociales, <phrase>image recognition</phrase>,…). También <phrase>nos</phrase> aporta una reflexión sobre los aspectos legales y éticos <phrase>de la</phrase> tecnología y sus implicaciones <phrase>en</phrase> <phrase>la</phrase> transformación <phrase>digital</phrase>, <phrase>la</phrase> innovación social, <phrase>la</phrase> política y <phrase>la</phrase> cultura.  ¿Para qué sirve este curso? <phrase>El</phrase> curso pretende ser una introducción a las Humanidades y <phrase>el</phrase> Patrimonio <phrase>Digital</phrase>. Con él tendrás una visión amplia y transversal de los diferentes aspectos implicados <phrase>en</phrase> los diferentes ámbitos. Un curso que <phrase>te</phrase> ayudará <phrase>en</phrase> <phrase>tu</phrase> camino a <phrase>la</phrase> transformación <phrase>digital</phrase> y a <phrase>la</phrase> implementación <phrase>de la</phrase> tecnología <phrase>digital</phrase> <phrase>en</phrase> las Humanidades. <phrase>Si</phrase> eres una persona con una formación humanista, ingeniería o informática, o simplemente <phrase>te</phrase> interesa <phrase>la</phrase> relación <phrase>de la</phrase> tecnología con las humanidades, este curso <phrase>es</phrase> para <phrase>ti</phrase>. Está pensado para poder ser completado por cualquier persona con interés, puesto que las presentaciones son amenas y alternativas, <phrase>eso</phrase> sí, <phrase>sin</phrase> perder nunca <phrase>el</phrase> rigor académico.  Un curso impartido por especialistas <phrase>de la</phrase> Xarxa d’Humanitats Digitals (<phrase>Red</phrase> de Humanidades Digitales) <phrase>de la</phrase> Universidad Autònoma de <phrase>Barcelona</phrase>.
Much of the world's <phrase>data</phrase> resides in <phrase>databases</phrase>. <phrase>SQL</phrase> (or <phrase>Structured Query Language</phrase>) is a powerful <phrase>language</phrase> which is used for communicating with and extracting <phrase>data</phrase> from <phrase>databases</phrase>. A working <phrase>knowledge</phrase> of <phrase>databases</phrase> and <phrase>SQL</phrase> is a must if you want to become a <phrase>data</phrase> <phrase>scientist</phrase>.  The purpose of this course is to <phrase>introduce</phrase> <phrase>relational database</phrase> concepts and help you learn and apply foundational <phrase>knowledge</phrase> of the <phrase>SQL</phrase> <phrase>language</phrase>. It is also intended to get you started with performing <phrase>SQL</phrase> access in a <phrase>data science</phrase> environment.    The emphasis in this course is on hands-on and practical learning . As such, you will work with real <phrase>databases</phrase>, real <phrase>data science</phrase> tools, and <phrase>real-world</phrase> datasets. You will create a <phrase>database</phrase> <phrase>instance</phrase> in the <phrase>cloud</phrase>. Through a series of hands-on labs you will practice building and running <phrase>SQL</phrase> queries. You will also learn how to access <phrase>databases</phrase> from Jupyter notebooks using <phrase>SQL</phrase> and <phrase>Python</phrase>.  No <phrase>prior knowledge</phrase> of <phrase>databases</phrase>, <phrase>SQL</phrase>, <phrase>Python</phrase>, or <phrase>programming</phrase> is required.  Anyone can <phrase>audit</phrase> this course at no-charge. If you choose to take this course and earn the <phrase>Coursera</phrase> course certificate, you can also earn an <phrase>IBM</phrase> <phrase>digital</phrase> badge upon successful completion of the course.  LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
In this course you will learn what <phrase>Artificial Intelligence</phrase> (<phrase>AI</phrase>) is, explore <phrase>use cases</phrase> and applications of <phrase>AI</phrase>, understand <phrase>AI</phrase> concepts and terms like <phrase>machine learning</phrase>, <phrase>deep learning</phrase> and <phrase>neural networks</phrase>. You will be exposed to various issues and concerns surrounding <phrase>AI</phrase> such as <phrase>ethics</phrase> and bias, & jobs, and get <phrase>advice</phrase> from experts about learning and starting a career in <phrase>AI</phrase>.  You will also demonstrate <phrase>AI</phrase> in <phrase>action</phrase> with a <phrase>mini</phrase> project.   This course does not require any <phrase>programming</phrase> or <phrase>computer science</phrase> <phrase>expertise</phrase> and is designed to <phrase>introduce</phrase> the basics of <phrase>AI</phrase> to anyone whether you have a technical background or not.
This is the second course in the <phrase>Data</phrase> to Insights specialization. Here we will <phrase>cover</phrase> how to ingest new external datasets into BigQuery and  visualize them with <phrase>Google</phrase> <phrase>Data</phrase> Studio. We will also <phrase>cover</phrase> intermediate <phrase>SQL</phrase> concepts like multi-table JOINs and <phrase>UNIONs</phrase> which will allow you to analyze <phrase>data</phrase> across multiple <phrase>data</phrase> sources.  Note: Even if you have a background in <phrase>SQL</phrase>, there are BigQuery specifics (like handling query cache and table wildcards) that may be new to you.  >>> By enrolling in this specialization you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
Welcome to the <phrase>Reinforcement Learning</phrase> course.   Here you will find out about:  - foundations of <phrase>RL</phrase> methods: value/policy <phrase>iteration</phrase>, <phrase>q-learning</phrase>, policy <phrase>gradient</phrase>, etc. --- with <phrase>math</phrase> & batteries included  - using <phrase>deep neural networks</phrase> for <phrase>RL</phrase> tasks --- also known as "the hype <phrase>train</phrase>"  - <phrase>state</phrase> of the <phrase>art</phrase> <phrase>RL</phrase> <phrase>algorithms</phrase> --- and how to apply <phrase>duct tape</phrase> to them for <phrase>practical problems</phrase>.  - and, of course, teaching your <phrase>neural network</phrase> to <phrase>play</phrase> <phrase>games</phrase> --- because that's what everyone thinks <phrase>RL</phrase> is about. We'll also use it for seq2seq and contextual bandits.  Jump in. It's gonna be fun!  Do you have technical problems? Write to us: <phrase>coursera</phrase>@hse.ru
In this course, you will analyze and apply essential <phrase>design</phrase> principles to your Tableau visualizations. This course assumes you understand the tools within Tableau and have some <phrase>knowledge</phrase> of the fundamental concepts of <phrase>data visualization</phrase>. You will define and examine the similarities and differences of exploratory and explanatory analysis as well as begin to ask the right questions about what’s needed in a visualization. You will assess how <phrase>data</phrase> and <phrase>design</phrase> work together, including how to choose the appropriate <phrase>visual representation</phrase> for your <phrase>data</phrase>, and the difference between effective and ineffective visuals. You will apply effective <phrase>best practice</phrase> <phrase>design</phrase> principles to your <phrase>data</phrase> visualizations and be able to illustrate examples of strategic use of contrast to highlight important elements. You will evaluate pre-attentive attributes and why they are important in visualizations. You will exam the importance of using the "right" amount of color and in the right <phrase>place</phrase> and be able to apply <phrase>design</phrase> principles to de-clutter your <phrase>data visualization</phrase>.
¿Desea saber cómo mejorar <phrase>la</phrase> precisión de sus modelos de aprendizaje automático? ¿Cuál <phrase>es</phrase> <phrase>la</phrase> forma de saber qué columnas de datos <phrase>se</phrase> prestan para las funciones más útiles? Bienvenido a Feature <phrase>Engineering</phrase> <phrase>en</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> Platform, <phrase>el</phrase> curso <phrase>en</phrase> <phrase>el</phrase> que hablaremos de cómo reconocer buenas funciones, y cómo puede preprocesarlas y transformarlas para usarlas de forma óptima <phrase>en</phrase> sus modelos de aprendizaje automático.  <phrase>En</phrase> este curso, practicará cómo elegir funciones y preprocesarlas <phrase>en</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> Platform mediante labs interactivos. Nuestros instructores lo guiarán por las soluciones de código, que también <phrase>se</phrase> harán públicas para que las consulte mientras trabaja <phrase>en</phrase> sus propios proyectos de <phrase>AA</phrase> <phrase>en</phrase> <phrase>el</phrase> futuro.
「<phrase>Art</phrase> and <phrase>Science</phrase> of <phrase>Machine Learning</phrase>」へようこそ。このコースでは、機械学習モデルの微調整と最適化を行って高いパフォーマンスを得るために不可欠な直感と判断力、実験のスキルを習得します。    このコースでは、モデルのトレーニングで調整に使用するさまざまなノブやレバー、すなわち「ハイパーパラメータ」について学びます。最初にこうしたハイパーパラメータを手動で調整してモデルのパフォーマンスに与える影響を観察し、使い方に慣れてきたら、<phrase>Google</phrase> <phrase>Cloud</phrase> Platform で <phrase>Cloud</phrase> <phrase>Machine Learning</phrase> <phrase>Engine</phrase> を使用して、自動的に調整する方法を学習します。
Welcome to the Advanced <phrase>Linear Models</phrase> for <phrase>Data Science</phrase> Class 1: <phrase>Least Squares</phrase>. This class is <phrase>an introduction</phrase> to <phrase>least squares</phrase> from a linear <phrase>algebraic</phrase> and <phrase>mathematical</phrase> perspective. Before beginning the class make sure that you have the following:  - A <phrase>basic</phrase> understanding of <phrase>linear algebra</phrase> and multivariate <phrase>calculus</phrase>. - A <phrase>basic</phrase> understanding of <phrase>statistics</phrase> and <phrase>regression</phrase> models. - At least a little familiarity with proof based <phrase>mathematics</phrase>. - <phrase>Basic</phrase> <phrase>knowledge</phrase> of the R <phrase>programming language</phrase>.  After taking this course, students will have a <phrase>firm</phrase> foundation in a linear <phrase>algebraic</phrase> treatment of <phrase>regression</phrase> modeling. This will greatly augment applied <phrase>data</phrase> scientists' <phrase>general</phrase> understanding of <phrase>regression</phrase> models.
<phrase>Biostatistics</phrase> is the <phrase>application</phrase> of statistical reasoning to the <phrase>life</phrase> sciences, and it's the key to unlocking the <phrase>data</phrase> gathered by researchers and the evidence presented in the scientific <phrase>public health</phrase> <phrase>literature</phrase>. In this course, you'll extend simple <phrase>regression</phrase> to the prediction of a <phrase>single</phrase> outcome of interest on the basis of <phrase>multiple variables</phrase>. Along the way, you'll be introduced to a <phrase>variety</phrase> of methods, and you'll practice interpreting <phrase>data</phrase> and performing calculations on real <phrase>data</phrase> from published studies.  Topics include multiple <phrase>logistic regression</phrase>, the Spline approach, <phrase>confidence intervals</phrase>, p-values, multiple Cox <phrase>regression</phrase>, adjustment, and effect modification.
Vous souhaitez découvrir comment améliorer <phrase>la</phrase> précision de vos modèles de <phrase>machine learning</phrase> (<phrase>ML</phrase>) ? Vous voulez <phrase>identifier</phrase> <phrase>les</phrase> colonnes de données offrant <phrase>les</phrase> caractéristiques <phrase>les</phrase> plus utiles ? Bienvenue dans le cours Feature <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform (Extraction de caractéristiques sur <phrase>Google</phrase> <phrase>Cloud</phrase> Platform). Nous vous expliquerons <phrase>ce</phrase> qui distingue <phrase>les</phrase> bonnes caractéristiques <phrase>des</phrase> mauvaises, puis nous vous montrerons comment prétraiter et <phrase>transformer</phrase> vos caractéristiques afin d'optimiser leur efficacité dans vos modèles.  <phrase>Des</phrase> ateliers interactifs vous permettront de mettre <phrase>en</phrase> pratique <phrase>ce</phrase> que vous avez appris. Vous sélectionnerez vous-même <phrase>des</phrase> caractéristiques, puis <phrase>les</phrase> prétraiterez dans <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. <phrase>Nos</phrase> formateurs vous aideront à comprendre <phrase>les</phrase> solutions de code. Ces solutions seront accessibles à tous, et pourront vous servir de référence <phrase>en</phrase> <phrase>cas</phrase> de besoin lorsque vous travaillerez sur vos propres projets de <phrase>ML</phrase>.
Welcome to Demand Analytics - one of the most sought-after skills in <phrase>supply chain management</phrase> and <phrase>marketing</phrase>!  Through the <phrase>real-life</phrase> story and <phrase>data</phrase> of a leading cookware manufacturer <phrase>in North America</phrase>, you will learn the <phrase>data</phrase> analytics skills for demand planning and forecasting. Upon the completion of this course, you will be able to    1. Improve the forecasting accuracy by building and validating demand prediction models.  2. Better stimulate and influence demand by identifying the drivers (e.g., time, <phrase>seasonality</phrase>, price, and other <phrase>environmental factors</phrase>) for demand and quantifying their impact.  AK is a leading cookware manufacturer <phrase>in North America</phrase>. Its newly <phrase>launched</phrase> top-line product was gaining <phrase>momentum</phrase> in the <phrase>marketplace</phrase>. However, a price adjustment at the <phrase>peak</phrase> <phrase>season</phrase> stimulated a significant demand surge which took AK completely by surprise and resulted in huge backorders. AK faced the <phrase>risk</phrase> of losing the market <phrase>momentum</phrase> due to the upset customers and the <phrase>high</phrase> cost associated with over-time <phrase>production</phrase> and expedited shipping. Accurate demand forecast is essential for increasing <phrase>revenue</phrase> and reducing cost. Identifying the drivers for demand and assessing their impact <phrase>on demand</phrase> can help companies better influence and stimulate demand.  I hope you enjoy the course!
Nach einem ersten Überblick über die Geschichte von <phrase>ML</phrase> erfahren Sie in diesem Kurs, weshalb heute mithilfe neuronaler Netzwerke viele Probleme so erfolgreich gelöst werden können. Wir erklären anschließend, wie Sie überwachtes Lernen zur Problemlösung einrichten und mithilfe <phrase>des</phrase> Gradientenverfahrens gute Ergebnisse erzielen. Dazu sind Datasets erforderlich, <phrase>mit</phrase> denen die Generalisierung möglich ist. In diesem Kurs zeigen wir Ihnen, wie Sie Datasets auf wiederholbare Weise erstellen, um Experimente zu ermöglichen.  Kursziele: Erkennen, warum <phrase>Deep Learning</phrase> derzeit beliebt ist Modelle anhand von Verlustfunktionen und Leistungsmesswerten optimieren und auswerten Häufige Probleme rund um maschinelles Lernen minimieren Wiederholbare und skalierbare Datasets zum Trainieren, Auswerten und Testen erstellen
Cette formation à <phrase>la</phrase> demande propose aux participants de s'initier par <phrase>la</phrase> pratique à <phrase>la</phrase> conception et à <phrase>la</phrase> <phrase>construction</phrase> de modèles de <phrase>machine learning</phrase> (<phrase>ML</phrase>) sur <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Il s'agit d'un cours accéléré, que vous pouvez effectuer <phrase>en</phrase> une semaine. <phrase>En</phrase> suivant une série de présentations, de démonstrations et d'ateliers, <phrase>les</phrase> participants découvriront <phrase>les</phrase> concepts du <phrase>ML</phrase> et de TensorFlow et acquerront <phrase>des</phrase> compétences pratiques pour développer, évaluer et mettre <phrase>en</phrase> <phrase>production</phrase> <phrase>des</phrase> modèles de <phrase>ML</phrase>.  OBJECTIFS  <phrase>Au</phrase> terme de cette formation, <phrase>les</phrase> participants auront acquis <phrase>les</phrase> compétences suivantes :    ● <phrase>Identifier</phrase> <phrase>les</phrase> <phrase>cas</phrase> d'utilisation du <phrase>machine learning</phrase>   ● Créer un modèle de <phrase>ML</phrase> avec TensorFlow   ● Créer <phrase>des</phrase> modèles de <phrase>ML</phrase> évolutifs et déployables à l'aide de <phrase>Cloud</phrase> <phrase>ML</phrase>   ● Reconnaître l'importance de prétraiter et de combiner <phrase>les</phrase> caractéristiques   ● Intégrer <phrase>des</phrase> concepts de <phrase>ML</phrase> avancés dans leurs modèles   ● Mettre <phrase>en</phrase> <phrase>production</phrase> <phrase>des</phrase> modèles de <phrase>ML</phrase> entraînés   PRÉREQUIS  Pour tirer pleinement parti de <phrase>ce</phrase> cours, <phrase>les</phrase> participants doivent remplir <phrase>les</phrase> prérequis suivants :    ● Avoir suivi <phrase>la</phrase> formation <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> and <phrase>Machine Learning</phrase> Fundamentals OU disposer d'une expérience équivalente   ● Maîtriser <phrase>les</phrase> <phrase>principes</phrase> de base <phrase>des</phrase> langages de requête courants tels que <phrase>SQL</phrase>   ● Avoir de l'expérience <phrase>en</phrase> modélisation, extraction, transformation et chargement <phrase>des</phrase> données   ● Savoir développer <phrase>des</phrase> applications à l'aide d'un langage de programmation courant tel que <phrase>Python</phrase>   ● Savoir utiliser le <phrase>machine learning</phrase> et/ou <phrase>les</phrase> statistiques   Remarques relatives <phrase>au</phrase> compte <phrase>Google</phrase> : • Vous avez besoin d'un compte <phrase>Google</phrase>/<phrase>Gmail</phrase> et d'une carte de crédit ou d'un compte bancaire pour vous inscrire à l'essai gratuit de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. (<phrase>Les</phrase> services <phrase>Google</phrase> sont actuellement indisponibles <phrase>en</phrase> Chine.) • <phrase>Si</phrase> votre adresse de facturation pour <phrase>les</phrase> services GCP est située <phrase>en</phrase> <phrase>Union</phrase> européenne (UE) ou <phrase>en</phrase> Russie, lisez <phrase>la</phrase> documentation relative à <phrase>la</phrase> TVA à l'adresse suivante : https://cloud.google.com/billing/docs/resources/vat-overview. • Vous trouverez d'autres questions fréquentes relatives à l'essai gratuit de GCP à l'adresse suivante : https://cloud.google.com/<phrase>free</phrase>-trial/
Este curso acelerado <phrase>on demand</phrase> de 1 semana de duración presenta a los participantes las funciones de macrodatos y aprendizaje automático de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform (GCP). Ofrece una descripción <phrase>general</phrase> breve de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform y profundiza <phrase>en</phrase> las funciones de procesamiento de datos.  Al final del curso, los participantes podrán realizar lo siguiente: • Identificar <phrase>el</phrase> propósito y <phrase>el</phrase> valor de los productos clave de <phrase>Google</phrase> <phrase>Cloud</phrase> Platform relacionados con los macrodatos y <phrase>el</phrase> aprendizaje automático • Usar CloudSQL y <phrase>Cloud</phrase> Dataproc para migrar cargas de trabajo existentes de <phrase>MySQL</phrase> y <phrase>Hadoop</phrase>/<phrase>Pig</phrase>/Spark/Hive a <phrase>Google</phrase> <phrase>Cloud</phrase> Platform • Utilizar BigQuery y <phrase>Cloud</phrase> Datalab para realizar análisis interactivos de datos • Elegir entre <phrase>Cloud</phrase> <phrase>SQL</phrase>, <phrase>BigTable</phrase> y Datastore • Entrenar y usar una <phrase>red</phrase> <phrase>neuronal</phrase> mediante TensorFlow • Elegir entre diferentes productos de procesamiento de datos <phrase>en</phrase> <phrase>Google</phrase> <phrase>Cloud</phrase> Platform  Para inscribirse <phrase>en</phrase> este curso, los participantes deben tener aproximadamente un (1) año de experiencia <phrase>en</phrase> uno o más de los siguientes: • Un lenguaje de consultas común, como <phrase>SQL</phrase> • Actividades de extracción, transformación y carga • Modelado de datos • Aprendizaje automático o estadísticas • Programación <phrase>en</phrase> <phrase>Python</phrase>  Notas sobre <phrase>la</phrase> cuenta de <phrase>Google</phrase>: • los servicios de <phrase>Google</phrase> no están disponibles <phrase>en</phrase> <phrase>China</phrase> <phrase>en</phrase> <phrase>la</phrase> actualidad.  Buscando <phrase>la</phrase> versión <phrase>en</phrase> español de este curso? Visita https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>es</phrase>/ このコースの日本語版をお探しですか？https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>jp</phrase>/
Discutiremos por que hoje as redes neurais funcionam tão <phrase>bem</phrase> para <phrase>lidar</phrase> com vários problemas, começando pela história do aprendizado de máquina. <phrase>Em</phrase> seguida, falaremos sobre como configurar um problema de aprendizado supervisionado <phrase>e</phrase> encontrar <phrase>uma</phrase> <phrase>boa</phrase> solução com gradiente descendente. Isso envolve a criação de conjuntos de dados que permitem a generalização. Esses métodos serão abordados de maneira didática para auxiliar <phrase>na</phrase> realização <phrase>dos</phrase> testes.  Objetivos do curso: Identificar por que o aprendizado profundo é mais usado hoje <phrase>em</phrase> dia Otimizar <phrase>e</phrase> avaliar modelos usando funções de perda <phrase>e</phrase> métricas de desempenho Reduzir problemas comuns que surgem no aprendizado de máquina Criar conjuntos de dados de treinamento, avaliação <phrase>e</phrase> testes que podem ser repetidos <phrase>e</phrase> escalonáveis
This course will <phrase>introduce</phrase> the learner to <phrase>network analysis</phrase> through tutorials using the NetworkX <phrase>library</phrase>. The course begins with an understanding of what <phrase>network analysis</phrase> is and motivations for why we might <phrase>model</phrase> phenomena as networks. The second week introduces the concept of connectivity and network robustness. The third week will explore ways of measuring the importance or centrality of a node in a network. The final week will explore the <phrase>evolution</phrase> of networks over time and <phrase>cover</phrase> models of network generation and the link prediction problem.   This course should be taken after: Introduction to <phrase>Data Science</phrase> in <phrase>Python</phrase>, Applied Plotting, Charting & <phrase>Data</phrase> Representation in <phrase>Python</phrase>, and Applied <phrase>Machine Learning</phrase> in <phrase>Python</phrase>.
<phrase>Unix</phrase> forms a foundation that is often very helpful for accomplishing other goals you might have for you and your <phrase>computer</phrase>, whether that goal is running a <phrase>business</phrase>, writing a <phrase>book</phrase>, curing <phrase>disease</phrase>, or creating the next great app. The means to these goals are sometimes carried out by writing <phrase>software</phrase>. <phrase>Software</phrase> can’t be mined out of the ground, nor can <phrase>software</phrase> seeds be planted in <phrase>spring</phrase> to harvest by autumn. <phrase>Software</phrase> isn’t <phrase>produced</phrase> in factories on an <phrase>assembly line</phrase>. <phrase>Software</phrase> is a hand-made, often <phrase>bespoke</phrase> good. If a <phrase>software developer</phrase> is an artisan, then <phrase>Unix</phrase> is their <phrase>workbench</phrase>. <phrase>Unix</phrase> provides an essential and simple set of tools in a distraction-<phrase>free</phrase> environment. Even if you’re not a <phrase>software developer</phrase> learning <phrase>Unix</phrase> can open you up to new methods of thinking and novel ways to scale your ideas.   This course is intended for folks who are new to <phrase>programming</phrase> and new to <phrase>Unix-like</phrase> <phrase>operating systems</phrase> like macOS and <phrase>Linux</phrase> <phrase>distributions</phrase> like <phrase>Ubuntu</phrase>. Most of the technologies discussed in this course will be accessed via a <phrase>command line interface</phrase>. <phrase>Command line</phrase> interfaces can seem alien at first, so this course attempts to draw parallels between using the <phrase>command line</phrase> and actions that you would normally take while using your <phrase>mouse</phrase> and <phrase>keyboard</phrase>. You’ll also learn how to write little pieces of <phrase>software</phrase> in a <phrase>programming language</phrase> called Bash, which allows you to connect together the tools we’ll discuss. My hope is that by the end of this course you be able to use different <phrase>Unix</phrase> tools as if they’re interconnecting <phrase>Lego</phrase> bricks.
<phrase>Biostatistics</phrase> is the <phrase>application</phrase> of statistical reasoning to the <phrase>life</phrase> sciences, and it is the key to unlocking the <phrase>data</phrase> gathered by researchers and the evidence presented in the <phrase>scientific literature</phrase>. In this course, we'll focus on the use of statistical measurement methods within the world of <phrase>public health</phrase> <phrase>research</phrase>. Along the way, you'll be introduced to a <phrase>variety</phrase> of methods and measures, and you'll practice interpreting <phrase>data</phrase> and performing calculations on real <phrase>data</phrase> from published studies.  Topics include <phrase>summary</phrase> measures, visual displays, continuous <phrase>data</phrase>, <phrase>sample size</phrase>, the <phrase>normal distribution</phrase>, <phrase>binary data</phrase>, the element of time, and the Kaplan-Meir curve.
If you are a <phrase>software developer</phrase> who wants to build scalable <phrase>AI</phrase>-powered <phrase>algorithms</phrase>, you need to understand how to use the tools to build them. This Specialization will teach you <phrase>best practices</phrase> for using TensorFlow, a popular <phrase>open-source</phrase> framework for <phrase>machine learning</phrase>.  In this fourth course, you will learn how to build <phrase>time series</phrase> models in TensorFlow. You’ll first implement <phrase>best practices</phrase> to prepare <phrase>time series</phrase> <phrase>data</phrase>. You’ll also explore how RNNs and 1D ConvNets can be used for prediction. Finally, you’ll apply everything you’ve learned throughout the Specialization to build a <phrase>sunspot</phrase> prediction <phrase>model</phrase> using <phrase>real-world</phrase> <phrase>data</phrase>!  The <phrase>Machine Learning</phrase> course and <phrase>Deep Learning</phrase> Specialization from Andrew Ng teach the most important and foundational principles of <phrase>Machine Learning</phrase> and <phrase>Deep Learning</phrase>. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to <phrase>real-world</phrase> problems. To develop a deeper understanding of how <phrase>neural networks</phrase> work, we recommend that you take the <phrase>Deep Learning</phrase> Specialization.
Learn how to analyze <phrase>data</phrase> using <phrase>Python</phrase>. This course will take you from the basics of <phrase>Python</phrase> to exploring many different types of <phrase>data</phrase>. You will learn how to prepare <phrase>data</phrase> for analysis, perform simple <phrase>statistical analysis</phrase>, create meaningful <phrase>data</phrase> visualizations, predict <phrase>future trends</phrase> from <phrase>data</phrase>, and more!  Topics <phrase>covered</phrase>:  1) Importing Datasets 2) Cleaning the <phrase>Data</phrase> 3) <phrase>Data</phrase> <phrase>frame</phrase> manipulation 4) Summarizing the <phrase>Data</phrase> 5) Building <phrase>machine learning</phrase> <phrase>Regression</phrase> models 6) Building <phrase>data</phrase> pipelines   <phrase>Data</phrase> Analysis with <phrase>Python</phrase> will be delivered through lecture, lab, and assignments. It includes following parts:  <phrase>Data</phrase> Analysis <phrase>libraries</phrase>: will learn to use <phrase>Pandas</phrase>, <phrase>Numpy</phrase> and <phrase>Scipy</phrase> <phrase>libraries</phrase> to work with a sample dataset. We will <phrase>introduce</phrase> you to <phrase>pandas</phrase>, <phrase>an open-source</phrase> <phrase>library</phrase>, and we will use it to load, manipulate, analyze, and visualize cool datasets. Then we will <phrase>introduce</phrase> you to another <phrase>open-source</phrase> <phrase>library</phrase>, <phrase>scikit-learn</phrase>, and we will use some of its <phrase>machine learning</phrase> <phrase>algorithms</phrase> to build smart models and make cool predictions.  If you choose to take this course and earn the <phrase>Coursera</phrase> course certificate, you will also earn an <phrase>IBM</phrase> <phrase>digital</phrase> badge.    LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
Статистическая обработка данных и визуализация результатов анализа - это неизбежный этап работы с данными, полученными в различных областях естественных наук, в социологии, психологии или экономике. В этом курсе мы подробно разберем основы статистики и познакомимся с основами языка статистического программирования R. Мы научим вас гибко использовать средства визуализации (диаграммы, графики и т.п.), чтобы сделать результаты анализа максимально доступными и понятными.  Вы научитесь рассчитывать основные описательные статистики: медиану и квантили, среднее и стандартное отклонение.  Вы познакомитесь с принципами использования теоретических распределений статистик для построения доверительных интервалов и тестирования гипотез (на примере t-критерия). Наконец, мы обсудим сложности, возникающие при множественном тестировании гипотез и научим вас преодолевать их.  Этот курс для людей, начинающих знакомство со статистикой, а также для тех, кто хочет не только освоить базовые возможности языка R, но и научиться строить сложные графики.
In this course we will learn about <phrase>Recommender Systems</phrase> (which we will study for the Capstone project), and also look at deployment issues for <phrase>data</phrase> <phrase>products</phrase>. By the end of this course, you should be able to implement a working <phrase>recommender system</phrase> (e.g. to predict ratings, or generate lists of related <phrase>products</phrase>), and you should understand the tools and techniques required to deploy such a working system on <phrase>real-world</phrase>, <phrase>large-scale</phrase> datasets.  This course is the final course in the <phrase>Python</phrase> <phrase>Data</phrase> <phrase>Products</phrase> for <phrase>Predictive Analytics</phrase> Specialization, building on the previous three courses (<phrase>Basic</phrase> <phrase>Data</phrase> Processing and Visualization, <phrase>Design</phrase> Thinking and <phrase>Predictive Analytics</phrase> for <phrase>Data</phrase> <phrase>Products</phrase>, and Meaningful Predictive Modeling). At each <phrase>step</phrase> in the specialization, you will gain hands-on experience in <phrase>data</phrase> manipulation and building your skills, eventually culminating in a capstone project encompassing all the concepts taught in the specialization.
This course aims to teach the concepts of <phrase>clinical data</phrase> models and common <phrase>data</phrase> models. Upon completion of this course, learners will be able to interpret and evaluate <phrase>data model</phrase> designs using <phrase>Entity-Relationship</phrase> Diagrams (ERDs), differentiate between <phrase>data</phrase> models and articulate how each are used to support clinical care and <phrase>data science</phrase>, and create <phrase>SQL</phrase> statements in <phrase>Google</phrase> BigQuery to query the MIMIC3 <phrase>clinical data</phrase> <phrase>model</phrase> and the OMOP common <phrase>data model</phrase>.
¿Estás interesado <phrase>en</phrase> tener un conocimiento más detallado sobre las herramientas y aplicaciones <phrase>Big Data</phrase>?  <phrase>En</phrase> este curso aprenderás los principios para comprender <phrase>la</phrase> terminología, conceptos básicos y herramientas más importantes para resolver problemas de análisis de datos enfocándonos <phrase>en</phrase> los problemas y las aplicaciones. <phrase>El</phrase> objetivo <phrase>es</phrase> proporcionar una visión de sistema para entender los retos más importantes que <phrase>nos</phrase> encontramos cuando trabajamos <phrase>en</phrase> entornos con grandes volúmenes de datos.  <phrase>En</phrase> <phrase>el</phrase> curso <phrase>se</phrase> plantea una introducción a diversas herramientas utilizadas de forma común <phrase>en</phrase> <phrase>la</phrase> comunidad como <phrase>Hadoop</phrase>, Spark o Hive y tendrás que resolver diferentes retos de análisis de datos mediante <phrase>su</phrase> uso.  Al terminar <phrase>el</phrase> curso habrás adquirido conocimientos sobre <phrase>el</phrase> ecosistema de herramientas <phrase>Big Data</phrase> incluyendo ejemplos de uso con problemas industriales y científicos. Tendrás una serie de recursos sobre cómo un análisis a realizar <phrase>se</phrase> traduce <phrase>en</phrase>  una serie de operaciones de recolección de datos, monitorización, almacenamiento, análisis y creación de informes sobre los resultados obtenidos. También adquirirás un criterio para elegir cuál <phrase>es</phrase> <phrase>la</phrase> herramienta más adecuada para resolver un cierto problema de análisis de datos a partir de los requerimientos de uso de las herramientas.   <phrase>El</phrase> curso está orientado tanto a estudiantes universitarios de primeros cursos de estudios universitarios relacionados con <phrase>la</phrase> informática, <phrase>la</phrase> ingeniería o las matemáticas, como a otros estudiantes con conocimientos de programación, interesados <phrase>en</phrase> aprender cómo utilizar de análisis de datos con herramientas de código abierto.  Para realizar los ejercicios <phrase>es</phrase> necesario utilizar una máquina virtual que deberá ser instalada <phrase>en</phrase> <phrase>tu</phrase> ordenador.
Welcome to <phrase>Logistic Regression</phrase> in R for <phrase>Public Health</phrase>!   Why <phrase>logistic regression</phrase> for <phrase>public health</phrase> rather than just <phrase>logistic regression</phrase>? Well, there are some particular considerations for every <phrase>data</phrase> set, and <phrase>public health</phrase> <phrase>data</phrase> sets have particular features that need special attention. In a word, they're messy. Like the others in the series, this is a hands-on course, giving you plenty of practice with R on <phrase>real-life</phrase>, messy <phrase>data</phrase>, with predicting who has <phrase>diabetes</phrase> from a set of patient characteristics as the worked example for this course. Additionally, the interpretation of the outputs from the <phrase>regression</phrase> <phrase>model</phrase> can differ depending on the perspective that you take, and <phrase>public health</phrase> doesn’t just take the perspective of an individual patient but must also consider the <phrase>population</phrase> angle. That said, much of what is <phrase>covered</phrase> in this course is true for <phrase>logistic regression</phrase> when applied to any <phrase>data</phrase> set, so you will be able to apply the principles of this course to <phrase>logistic regression</phrase> more broadly too.   By the end of this course, you will be able to:  Explain when it is valid to use <phrase>logistic regression</phrase>  Define odds and odds ratios  <phrase>Run</phrase> simple and multiple <phrase>logistic regression</phrase> analysis in R and interpret the output  Evaluate the <phrase>model</phrase> assumptions for multiple <phrase>logistic regression</phrase> in R  Describe and compare some common ways to choose a multiple <phrase>regression</phrase> <phrase>model</phrase>   This course builds on skills such as <phrase>hypothesis testing</phrase>, p values, and how to use R, which are <phrase>covered</phrase> in the first two courses of the <phrase>Statistics</phrase> for <phrase>Public Health</phrase> specialisation. If you are unfamiliar with these skills, we suggest you review Statistical Thinking for <phrase>Public Health</phrase> and <phrase>Linear Regression</phrase> for <phrase>Public Health</phrase> before beginning this course. If you are already familiar with these skills, we are confident that you will enjoy furthering your <phrase>knowledge</phrase> and skills in <phrase>Statistics</phrase> for <phrase>Public Health</phrase>: <phrase>Logistic Regression</phrase> for <phrase>Public Health</phrase>.   We hope you enjoy the course!
In this course, you will learn how to find <phrase>GIS</phrase> <phrase>data</phrase> for your own projects, and how to create a well-designed map that effectively communicates your message. The first section focuses on the <phrase>basic</phrase> <phrase>building blocks</phrase> of <phrase>GIS</phrase> <phrase>data</phrase>, so that you know what types of <phrase>GIS</phrase> files exist, and the implications of choosing one type over another. Next, we'll discuss <phrase>metadata</phrase> (which is <phrase>information</phrase> about a <phrase>data</phrase> set) so you know how to evaluate a <phrase>data</phrase> set before you decide to use it, as well as preparing <phrase>data</phrase> by merging and <phrase>clipping</phrase> files as needed. We'll then <phrase>talk</phrase> about how to take non-<phrase>GIS</phrase> <phrase>data</phrase>, such as a list of addresses, and convert it into "mappable" <phrase>data</phrase> using <phrase>geocoding</phrase>. Finally, you'll learn about how to take <phrase>data</phrase> that you have found and <phrase>design</phrase> a map using cartographic principles. In the course project, you will find your own <phrase>data</phrase> and create your own quantitative map.
This course will <phrase>introduce</phrase> the learner to <phrase>information visualization</phrase> basics, with a focus on reporting and charting using the matplotlib <phrase>library</phrase>. The course will start with a <phrase>design</phrase> and <phrase>information literacy</phrase> perspective, touching on what makes a good and bad visualization, and what statistical measures translate into in terms of visualizations. The second week will focus on the <phrase>technology</phrase> used to make visualizations in <phrase>python</phrase>, matplotlib, and <phrase>introduce</phrase> users to <phrase>best practices</phrase> when creating <phrase>basic</phrase> charts and how to realize <phrase>design</phrase> decisions in the framework. The third week will be a <phrase>tutorial</phrase> of functionality available in matplotlib, and demonstrate a <phrase>variety</phrase> of <phrase>basic</phrase> statistical charts helping learners to identify when a particular <phrase>method</phrase> is good for a particular problem. The course will end with a discussion of other forms of structuring and visualizing <phrase>data</phrase>.   This course should be taken after Introduction to <phrase>Data Science</phrase> in <phrase>Python</phrase> and before the remainder of the Applied <phrase>Data Science</phrase> with <phrase>Python</phrase> courses: Applied <phrase>Machine Learning</phrase> in <phrase>Python</phrase>, Applied <phrase>Text Mining</phrase> in <phrase>Python</phrase>, and Applied <phrase>Social Network</phrase> Analysis in <phrase>Python</phrase>.
In this course you will learn how to program in R and how to use R for effective <phrase>data</phrase> analysis. You will learn how to <phrase>install</phrase> and configure <phrase>software</phrase> necessary for a statistical <phrase>programming environment</phrase> and describe <phrase>generic programming</phrase> <phrase>language</phrase> concepts as they are implemented in a <phrase>high</phrase>-level statistical <phrase>language</phrase>. The course covers <phrase>practical issues</phrase> in statistical <phrase>computing</phrase> which includes <phrase>programming</phrase> in R, <phrase>reading</phrase> <phrase>data</phrase> into R, accessing R packages, writing R functions, <phrase>debugging</phrase>, profiling R code, and organizing and commenting R code. Topics in statistical <phrase>data</phrase> analysis will provide working examples.
This course is for novice programmers or <phrase>business</phrase> people who would like to understand the core tools used to wrangle and analyze <phrase>big data</phrase>. With no prior experience, you will have the opportunity to walk through hands-on examples with <phrase>Hadoop</phrase> and Spark frameworks, two of the most common in the <phrase>industry</phrase>. You will be comfortable explaining the specific components and <phrase>basic</phrase> processes of the <phrase>Hadoop</phrase> <phrase>architecture</phrase>, <phrase>software</phrase> <phrase>stack</phrase>, and <phrase>execution environment</phrase>.   In the assignments you will be guided in how <phrase>data</phrase> scientists apply the important concepts and techniques such as Map-Reduce that are used to solve fundamental problems in <phrase>big data</phrase>.  You'll feel empowered to have conversations about <phrase>big data</phrase> and the <phrase>data</phrase> analysis process.
Want to make sense of the volumes of <phrase>data</phrase> you have collected?  Need to incorporate <phrase>data</phrase>-driven decisions into your process?  This course provides an overview of <phrase>machine learning</phrase> techniques to explore, analyze, and leverage <phrase>data</phrase>.  You will be introduced to tools and <phrase>algorithms</phrase> you can use to create <phrase>machine learning</phrase> models that learn from <phrase>data</phrase>, and to scale those models up to <phrase>big data</phrase> problems.  At the end of the course, you will be able to: •	<phrase>Design</phrase> an approach to leverage <phrase>data</phrase> using the steps in the <phrase>machine learning</phrase> process. •	Apply <phrase>machine learning</phrase> techniques to explore and prepare <phrase>data</phrase> for modeling. •	Identify the type of <phrase>machine learning</phrase> problem in <phrase>order</phrase> to apply the appropriate set of techniques. •	Construct models that learn from <phrase>data</phrase> using widely available <phrase>open source</phrase> tools. •	Analyze <phrase>big data</phrase> problems using scalable <phrase>machine learning</phrase> <phrase>algorithms</phrase> on Spark.  <phrase>Software</phrase> Requirements:  Cloudera <phrase>VM</phrase>, KNIME, Spark
<phrase>La</phrase> toma de <phrase>decisiones</phrase> está <phrase>en</phrase> <phrase>la</phrase> esencia de los negocios. Gerenciar <phrase>es</phrase> tomar <phrase>decisiones</phrase>, muchas veces bajo presión, con información desordenada y <phrase>en</phrase> un contexto de incertidumbre. Un aspecto básico <phrase>es</phrase> entender y analizar <phrase>la</phrase> información, organizar los datos de forma de facilitar <phrase>su</phrase> posterior uso y <phrase>la</phrase> toma de <phrase>decisiones</phrase>. <phrase>Si</phrase> bien <phrase>hay</phrase> muchas otras dimensiones que entran <phrase>en</phrase> juego, <phrase>el</phrase> primer paso <phrase>es</phrase> formular bien <phrase>el</phrase> problema, estructurarlo y procesar <phrase>la</phrase> información. <phrase>En</phrase> este sentido, <phrase>el</phrase> principal objetivo de este curso <phrase>es</phrase> ayudarlo a ser un mejor tomador de <phrase>decisiones</phrase> a través de herramientas técnicas. A lo largo de este curso <phrase>el</phrase> alumno desarrollará habilidades cuantitativas para <phrase>la</phrase> toma de <phrase>decisiones</phrase>, a través del aprendizaje de métodos estadísticos con aplicaciones a los negocios <phrase>en</phrase> <phrase>Excel</phrase>. <phrase>El</phrase> foco está <phrase>en</phrase> <phrase>la</phrase> comprensión y <phrase>en</phrase> <phrase>el</phrase> uso de herramientas básicas de análisis <phrase>e</phrase> inferencia estadística tratando de que <phrase>el</phrase> alumno sea un usuario de estos métodos, comprenda <phrase>en</phrase> qué consisten, cuál <phrase>es</phrase> <phrase>la</phrase> intuición, <phrase>su</phrase> uso y aplicaciones.
The increased capabilities of a collection of logically interrelated <phrase>databases</phrase> distributed over a <phrase>computer</phrase> network enable scalable <phrase>data</phrase> processing. This course addresses the components of these systems, covering the following main topics: <phrase>distributed database</phrase> architectures, <phrase>distributed data</phrase> storage and indexing, distributed and parallel <phrase>query processing</phrase>/optimization, and <phrase>concurrency control</phrase> in distributed Parallel <phrase>Database Systems</phrase>.
Dieser einwöchige Intensivkurs baut auf vorherigen Kursen der Spezialisierung "<phrase>Data</phrase> <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform" auf. Sie lernen <phrase>mit</phrase> Videoschulungen, Demonstrationen und praxisorientierten Labs, wie Sie <phrase>Computing</phrase>-Cluster erstellen und verwalten, sodass <phrase>Hadoop</phrase>-, Spark-, <phrase>Pig</phrase>- und/<phrase>oder</phrase> Hive-Jobs auf der <phrase>Google</phrase> <phrase>Cloud</phrase> Platform ausgeführt werden können. Außerdem lernen Sie, wie Sie von diesen <phrase>Computing</phrase>-Clustern aus auf verschiedene <phrase>Cloud</phrase> Storage-Optionen zugreifen und die Funktionen <phrase>des</phrase> maschinellen Lernens von <phrase>Google</phrase> in die Analyseprogramme dieser Anwendungen integrieren.     In den praxisorientierten Labs verwenden Sie die Webkonsole und Befehlszeilenschnittstellen, um Dataproc-Cluster zu erstellen und zu verwalten. Darüber hinaus nutzen Sie Cluster, um Spark- und <phrase>Pig</phrase>-Jobs auszuführen. Sie erstellen dann IPython-Notebooks, die in BigQuery und Storage integriert werden können und die Spark verwenden. Zum Abschluss integrieren Sie die <phrase>APIs</phrase> für maschinelles Lernen in Ihre Datenanalyse.   Voraussetzungen  • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform <phrase>Big Data</phrase> & <phrase>Machine Learning</phrase> Fundamentals (<phrase>oder</phrase> vergleichbare Erfahrung)  • Einige Kenntnisse über <phrase>Python</phrase>
Quer aprender a melhorar a precisão <phrase>dos</phrase> seus modelos de aprendizado de máquina? Que tal descobrir quais colunas de dados criam <phrase>os</phrase> atributos mais úteis? Damos as <phrase>boas</phrase>-vindas <phrase>ao</phrase> curso Feature <phrase>Engineering</phrase> no <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Falaremos sobre a diferença entre atributos bons <phrase>e</phrase> ruins, além de como pré-processar <phrase>e</phrase> transformar essas variáveis para o uso ideal <phrase>nos</phrase> seus modelos de aprendizado de máquina.  Nesse curso, você fará laboratórios interativos para ver <phrase>na</phrase> prática como escolher atributos <phrase>e</phrase> fazer o pré-processamento no <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Nossos instrutores apresentarão as soluções de código <phrase>em</phrase> detalhes, que também serão disponibilizadas para usar como referência <phrase>nos</phrase> seus próprios projetos de aprendizado de máquina.
In the last course of our specialization, Overview of Advanced Methods of <phrase>Reinforcement Learning</phrase> in <phrase>Finance</phrase>, we will take a deeper look into topics discussed in our third course, <phrase>Reinforcement Learning</phrase> in <phrase>Finance</phrase>.  In particular, we will <phrase>talk</phrase> about links between <phrase>Reinforcement Learning</phrase>, <phrase>option</phrase> pricing and <phrase>physics</phrase>, implications of Inverse <phrase>Reinforcement Learning</phrase> for modeling market impact and price dynamics, and <phrase>perception</phrase>-<phrase>action</phrase> cycles in <phrase>Reinforcement Learning</phrase>. Finally, we will overview trending and <phrase>potential applications</phrase> of <phrase>Reinforcement Learning</phrase> for <phrase>high-frequency trading</phrase>, <phrase>cryptocurrencies</phrase>, <phrase>peer-to-peer lending</phrase>, and more.  After taking this course, students will be able to  - explain fundamental concepts of <phrase>finance</phrase> such as market equilibrium, no <phrase>arbitrage</phrase>, predictability, - discuss market modeling, - Apply the methods of <phrase>Reinforcement Learning</phrase> to <phrase>high-frequency trading</phrase>, <phrase>credit risk</phrase> <phrase>peer-to-peer lending</phrase>, and <phrase>cryptocurrencies</phrase> trading.
Was ist maschinelles Lernen und welche Probleme lassen sich damit lösen? Für <phrase>Google</phrase> geht <phrase>es</phrase> beim maschinellen Lernen (<phrase>ML</phrase>) mehr um Logik als nur um Daten. In diesem Kurs erfahren Sie, warum dieser <phrase>Ansatz</phrase> beim Erstellen einer <phrase>Pipeline</phrase> aus <phrase>ML</phrase>-Modellen nützlich ist. Außerdem erläutern wir die fünf Phasen zur Umsetzung eines für <phrase>ML</phrase> geeigneten Anwendungsfalls und warum keine dieser Phasen übersprungen werden darf. Zum Abschluss besprechen wir die Verzerrung, die durch <phrase>ML</phrase> entstehen kann, und erklären, wie <phrase>man</phrase> sie erkennt.
The Capstone project is an individual assignment. Participants decide the theme they want to explore and define the issue they want to solve. Their “playing field” should provide <phrase>data</phrase> from various sectors (such as farming and <phrase>nutrition</phrase>, <phrase>culture</phrase>, <phrase>economy</phrase> and <phrase>employment</phrase>, <phrase>Education</phrase> & <phrase>Research</phrase>, International & <phrase>Europe</phrase>, Housing, <phrase>Sustainable</phrase>, Development & Energies, <phrase>Health</phrase> & Social, <phrase>Society</phrase>, Territories & <phrase>Transport</phrase>). Participants are encouraged to mix the different fields and leverage the existing <phrase>information</phrase> with other (properly sourced) <phrase>open data</phrase> sets.  Deliverable 1 is the preliminary preparation and problem qualification <phrase>step</phrase>. The objectives is to define the what, why & how. What issue do we want to solve? Why does it promise value for <phrase>public</phrase> authorities, companies, citizens? How do we want to explore the provided <phrase>data</phrase>?   For Deliverable 2, the participant needs to present the intermediary outputs and adjustments to the analysis framework. The objectives is to confirm the how and the relevancy of the first <phrase>results</phrase>.   Finally, with Deliverable 3, the participant needs to present the final outputs and the value <phrase>case</phrase>. The objective is to confirm the why. Why will it create value for <phrase>public</phrase> authorities, companies, and citizens.  Assessment and grading: the participants will present their <phrase>results</phrase> to their <phrase>peers</phrase> on a regular basis. An evaluation framework will be provided for the participants to assess the quality of each other’s deliverables.
ペタバイト規模のデータに対して数秒でクエリを実行して処理する方法や、データの増大に合わせて自動的にスケーリングされるデータ分析に関心がある場合は、ぜひ <phrase>Data</phrase> Insights コースを受講してください。   この 1 週間のオンライン速習コースに参加すると、<phrase>Google</phrase> <phrase>Cloud</phrase> Platform を使用したデータ分析と可視化によって分析情報を得る方法を学習できます。本コースにはインタラクティブなシナリオとハンズオンラボが用意されており、参加者は <phrase>Google</phrase> BigQuery のさまざまなデータセットを使用して、分析情報の探索、マイニング、読み込み、可視化、抽出を行うことができます。本コースでは、データの読み込み、クエリの実行、スキーマのモデル化、パフォーマンスの最適化、クエリの料金、データの可視化を扱います。   前提条件   本コースを最大限に活用するには、この専門講座で以下のコースを事前に完了する必要があります。  • Exploring and Preparing your <phrase>Data</phrase> • Storing and Visualizing your <phrase>Data</phrase> • <phrase>Architecture</phrase> and Performance   >>> この専門講座に登録すると、よくある質問に記載されているとおり Qwiklabs の利用規約に同意したことになります。詳しくは、https://qwiklabs.com/terms_of_service をご覧ください。 <<<
Probabilistic <phrase>graphical</phrase> models (PGMs) are a rich framework for encoding <phrase>probability distributions</phrase> over complex domains: joint (multivariate) <phrase>distributions</phrase> over <phrase>large numbers</phrase> of <phrase>random variables</phrase> that interact with each other. These representations sit at the intersection of <phrase>statistics</phrase> and <phrase>computer science</phrase>, relying on concepts from <phrase>probability theory</phrase>, <phrase>graph</phrase> <phrase>algorithms</phrase>, <phrase>machine learning</phrase>, and more. They are the basis for the <phrase>state</phrase>-of-the-<phrase>art</phrase> methods in a wide <phrase>variety</phrase> of applications, such as <phrase>medical</phrase> diagnosis, <phrase>image understanding</phrase>, <phrase>speech recognition</phrase>, <phrase>natural language processing</phrase>, and many, many more. They are also a foundational tool in formulating many <phrase>machine learning</phrase> problems.   This course is the third in a <phrase>sequence</phrase> of three. Following the first course, which focused on representation, and the second, which focused on inference, this course addresses the question of learning: how a PGM can be learned from a <phrase>data</phrase> set of examples. The course discusses the key problems of <phrase>parameter estimation</phrase> in both <phrase>directed</phrase> and undirected models, as well as the structure learning <phrase>task</phrase> for <phrase>directed</phrase> models. The (highly recommended) honors <phrase>track</phrase> contains two hands-on <phrase>programming</phrase> assignments, in which key routines of two commonly used <phrase>learning algorithms</phrase> are implemented and applied to a <phrase>real-world</phrase> problem.
This course has a singular and clear aim, to empower you to do statistical <phrase>tests</phrase>, ready for incorporation into your dissertations, <phrase>research</phrase> papers, and presentations.  The ability to summarize <phrase>data</phrase>, create plots and charts, and to do the <phrase>tests</phrase> that you commonly see in the <phrase>literature</phrase> is a powerful skill indeed.  Not only will it further your career, but it will put you in the position to contribute to the advancement of humanity though <phrase>scientific research</phrase>.  We <phrase>live</phrase> in a wonderful age with great tools at our disposal, ready to achieve this goal.  None are quite as easy to learn, yet as powerful to use, as the <phrase>Wolfram Language</phrase>.  <phrase>Knowledge</phrase> is literally built into the <phrase>language</phrase>.  With its well-structured and consistent approach to creating code, you will become an expert in no time.  This course follows the modern trend of learning <phrase>statistical analysis</phrase> through the use of a <phrase>computer</phrase> <phrase>language</phrase>.  It requires no <phrase>prior knowledge</phrase> of coding.  An exciting journey awaits.
The practice of <phrase>investment management</phrase> has been transformed in <phrase>recent years</phrase> by <phrase>computational methods</phrase>. Instead of merely explaining the <phrase>science</phrase>, we help you build on that foundation in a practical manner, with an emphasis on the hands-on implementation of those ideas in the <phrase>Python programming language</phrase>. In this course, we <phrase>cover</phrase> the estimation, of <phrase>risk</phrase> and return parameters for meaningful portfolio decisions, and also <phrase>introduce</phrase> a <phrase>variety</phrase> of <phrase>state</phrase>-of-the-<phrase>art</phrase> portfolio <phrase>construction</phrase> techniques that have <phrase>proven</phrase> popular in <phrase>investment management</phrase> and portfolio <phrase>construction</phrase> due to their enhanced robustness.  As we <phrase>cover</phrase> the theory and <phrase>math</phrase> in lecture videos, we'll also implement the concepts in <phrase>Python</phrase>, and you'll be able to code along with us so that you have a deep and practical understanding of how those methods work. By the time you are done, not only will you have a foundational understanding of modern <phrase>computational methods</phrase> in <phrase>investment management</phrase>, you'll have practical mastery in the implementation of those methods. If you follow along and implement all the lab exercises, you will complete the course with a powerful <phrase>toolkit</phrase> that you will be able to use to perform your own analysis and build your own implementations and perhaps even use your newly acquired <phrase>knowledge</phrase> to improve on current methods.
>>> By enrolling in this course you agree to the <phrase>End User License Agreement</phrase> as set out in the FAQ.  Once enrolled you can access the license in the Resources <phrase>area</phrase> <<<  This course, Advanced <phrase>Machine Learning</phrase> and <phrase>Signal Processing</phrase>, is part of the <phrase>IBM</phrase> Advanced <phrase>Data Science</phrase> Specialization which <phrase>IBM</phrase> is currently creating and gives you <phrase>easy access</phrase> to the invaluable insights into Supervised and Unsupervised <phrase>Machine Learning</phrase> Models used by experts in many field relevant disciplines. We’ll learn about the fundamentals of <phrase>Linear Algebra</phrase> to understand how <phrase>machine learning</phrase> modes work. Then we <phrase>introduce</phrase> the most popular <phrase>Machine Learning</phrase> Frameworks for <phrase>python</phrase> <phrase>Scikit-Learn</phrase> and SparkML. SparkML is making up the greatest portion of this course since <phrase>scalability</phrase> is key to <phrase>address</phrase> performance bottlenecks. We learn how to tune the models in parallel by evaluating hundreds of different parameter-combinations in parallel. We’ll continuously use a <phrase>real-life</phrase> example from <phrase>IoT</phrase> (<phrase>Internet</phrase> of Things), for exemplifying the different <phrase>algorithms</phrase>. For passing the course you are even required to create your own <phrase>vibration</phrase> <phrase>sensor</phrase> <phrase>data</phrase> using the <phrase>accelerometer</phrase> sensors in your <phrase>smartphone</phrase>. So you are actually working on a self-created, real dataset throughout the course.  If you choose to take this course and earn the <phrase>Coursera</phrase> course certificate, you will also earn an <phrase>IBM</phrase> <phrase>digital</phrase> badge.  To find out more about <phrase>IBM</phrase> <phrase>digital</phrase> badges follow the link ibm.biz/badging.
This course presents critical concepts and practical methods to support planning, collection, storage, and dissemination of <phrase>data</phrase> in <phrase>clinical research</phrase>.  Understanding and implementing solid <phrase>data management</phrase> principles is critical for any scientific <phrase>domain</phrase>. Regardless of your current (or anticipated) role in the <phrase>research</phrase> enterprise, a strong working <phrase>knowledge</phrase> and skill set in <phrase>data management</phrase> <phrase>principles and practice</phrase> will increase your <phrase>productivity</phrase> and improve your <phrase>science</phrase>. Our goal is to use these modules to help you learn and practice this skill set.   This course assumes very little current <phrase>knowledge</phrase> of <phrase>technology</phrase> other than how to operate a <phrase>web browser</phrase>. We will focus on practical lessons, <phrase>short</phrase> quizzes, and hands-on exercises as we explore together <phrase>best practices</phrase> for <phrase>data management</phrase>.
<phrase>Accounting</phrase> Analytics explores how <phrase>financial statement</phrase> <phrase>data</phrase> and non-financial metrics can be linked to financial performance.  In this course, taught by Wharton’s acclaimed <phrase>accounting</phrase> professors, you’ll learn how <phrase>data</phrase> is used to assess what drives financial performance and to forecast future financial scenarios. While many <phrase>accounting</phrase> and financial organizations deliver <phrase>data</phrase>, <phrase>accounting</phrase> analytics deploys that <phrase>data</phrase> to deliver insight, and this course will explore the many areas in which <phrase>accounting</phrase> <phrase>data</phrase> provides <phrase>insight into</phrase> other <phrase>business</phrase> areas including <phrase>consumer</phrase> behavior predictions, corporate strategy, <phrase>risk management</phrase>, optimization, and more. By the end of this course, you’ll understand how financial <phrase>data</phrase> and non-financial <phrase>data</phrase> interact to forecast events, optimize operations, and determine strategy. This course has been designed to help you make better <phrase>business</phrase> decisions about the emerging roles of <phrase>accounting</phrase> analytics, so that you can apply what you’ve learned to make your own <phrase>business</phrase> decisions and create strategy using financial <phrase>data</phrase>. 
In this first course of the specialization <phrase>Excel</phrase> Skills for <phrase>Business</phrase> you will learn the Essentials of <phrase>Microsoft Excel</phrase>. Within six weeks, you will learn to expertly navigate the <phrase>Excel</phrase> <phrase>user interface</phrase>, perform <phrase>basic</phrase> calculations with formulas and functions, professionally format <phrase>spreadsheets</phrase>, and create visualizations of <phrase>data</phrase> through charts and <phrase>graphs</phrase>.  Whether you are self-taught and want to fill in the gaps for better efficiency and <phrase>productivity</phrase>, or whether you have never used <phrase>Excel</phrase> before, this course will set you up with a solid foundation to become a confident user and develop more advanced skills in later courses.   We have brought together a great teaching team that will be with you every <phrase>step</phrase> of the way. A broad <phrase>range</phrase> of practice quizzes and challenges will provide great opportunities to build up your skillset. Work through each new challenge with our team and in no time you will surprise yourself with how far you have come.   <phrase>Spreadsheet</phrase> <phrase>software</phrase> is one of the most ubiquitous pieces of <phrase>software</phrase> used in workplaces across the world. Learning to confidently operate this <phrase>software</phrase> means adding a highly valuable asset to your employability portfolio. At a time when <phrase>digital</phrase> skills jobs are growing much faster than non-<phrase>digital</phrase> jobs, make sure to position yourself ahead of the rest by adding <phrase>Excel</phrase> skills to your <phrase>employment</phrase> portfolio.
Descripción del curso:   Cuando finalices este curso habrás logrado un gran número de habilidades como introducir información, ordenarla, manipularla, realizar cálculos de diversa índole (matemáticos, trigonométricos, estadísticos, financieros, ingenieriles, probabilísticos), extraer conclusiones, trabajar con fechas y horas, construir gráficos, imprimir reportes y muchas más.  Asimismo, los ejemplos sobre los cuales <phrase>se</phrase> apoyan los contenidos dictados <phrase>en</phrase> este curso tienen una profunda aplicabilidad al mundo de los negocios, con lo que <phrase>su</phrase> inmediata utilización empresarial está al alcance <phrase>de la</phrase> mano.  Finalmente, los profesores que <phrase>han</phrase> diseñado y elaborado este curso para <phrase>ti</phrase>, no solamente dan una visión académica del <phrase>software</phrase> <phrase>sino</phrase> que, debido a <phrase>su</phrase> gran trayectoria profesional apoyada justamente <phrase>en</phrase> una profunda utilización de <phrase>Excel</phrase>, <phrase>te</phrase> transmitirán <phrase>su</phrase> propia vivencia que <phrase>te</phrase> permitirá tener una visión más concreta de las posibilidades que <phrase>te</phrase> brinda esta herramienta.
In this Capstone you will recommend a <phrase>business</phrase> strategy based on a <phrase>data model</phrase> you’ve constructed. Using a <phrase>data</phrase> set designed by Wharton <phrase>Research</phrase> <phrase>Data</phrase> Services (WRDS), you will implement quantitative models in <phrase>spreadsheets</phrase> to identify the best opportunities for success and minimizing <phrase>risk</phrase>. Using your newly acquired <phrase>decision-making</phrase> skills, you will structure a decision and present this course of <phrase>action</phrase> in a <phrase>professional</phrase> quality <phrase>PowerPoint</phrase> presentation which includes both <phrase>data</phrase> and <phrase>data</phrase> analysis from your quantitative models.  Wharton <phrase>Research</phrase> <phrase>Data</phrase> Services (WRDS) is the leading <phrase>data</phrase> <phrase>research</phrase> platform and <phrase>business intelligence</phrase> tool for over 30,000 corporate, <phrase>academic</phrase>, <phrase>government</phrase> and <phrase>nonprofit</phrase> clients in 33 countries. WRDS provides the user with one location to access over 200 terabytes of <phrase>data</phrase> across multiple disciplines including <phrase>Accounting</phrase>, <phrase>Banking</phrase>, <phrase>Economics</phrase>, <phrase>ESG</phrase>, <phrase>Finance</phrase>, <phrase>Insurance</phrase>, <phrase>Marketing</phrase>, and <phrase>Statistics</phrase>.
This course will help lay the foundation of your <phrase>healthcare</phrase> <phrase>data</phrase> journey and provide you with <phrase>knowledge</phrase> and skills necessary to work in the <phrase>healthcare</phrase> <phrase>industry</phrase> as a <phrase>data</phrase> <phrase>scientist</phrase>. <phrase>Healthcare</phrase> is unique because it is associated with continually evolving and complex processes associated with <phrase>health</phrase> <phrase>management</phrase> and <phrase>medical</phrase> care. We'll learn about the many facets to consider in <phrase>healthcare</phrase> and determine the value and growing need for <phrase>data</phrase> analysts in <phrase>healthcare</phrase>. We'll learn about the Triple Aim and other <phrase>data</phrase>-<phrase>enabled</phrase> <phrase>healthcare</phrase> drivers. We'll <phrase>cover</phrase> different concepts and categories of <phrase>healthcare</phrase> <phrase>data</phrase> and describe how <phrase>ontologies</phrase> and <phrase>related terms</phrase> such as <phrase>taxonomy</phrase> and terminology organize concepts and facilitate computation. We'll discuss the common clinical representations of <phrase>data</phrase> in <phrase>healthcare</phrase> systems, including <phrase>ICD</phrase>-10, SNOMED, LOINC, <phrase>drug</phrase> vocabularies (e.g., RxNorm), and <phrase>clinical data</phrase> standards. We’ll discuss the various types of <phrase>healthcare</phrase> <phrase>data</phrase> and assess the complexity that occurs as you work with pulling in all the different types of <phrase>data</phrase> to aid in decisions. We will analyze various types and sources of <phrase>healthcare</phrase> <phrase>data</phrase>, including clinical, operational claims, and patient generated <phrase>data</phrase> as well as differentiate unstructured, <phrase>semi-structured</phrase> and <phrase>structured data</phrase> within <phrase>health</phrase> <phrase>data</phrase> contexts. We'll examine the inner workings of <phrase>data</phrase> and conceptual <phrase>harmony</phrase>  offer some solutions to the <phrase>data integration</phrase> problem by defining some important concepts, methods, and applications that are important to this <phrase>domain</phrase>.
This course is organized into two parts presenting the theoretical and practical foundations of <phrase>geographic information systems</phrase> (<phrase>GIS</phrase>). - Together theses courses constitute <phrase>an introduction</phrase> to <phrase>GIS</phrase> and require no <phrase>prior knowledge</phrase>. - By following this introduction to <phrase>GIS</phrase> you will quickly acquire the <phrase>basic</phrase> <phrase>knowledge</phrase> required to create <phrase>spatial databases</phrase> and produce <phrase>high</phrase>-quality maps and cartographic representations. - This is a practical course and is based on <phrase>free</phrase>, <phrase>open-source software</phrase>, including QGIS. If you study or work in the fields of land <phrase>management</phrase> or the analysis of <phrase>geographically distributed</phrase> objects such as <phrase>land use planning</phrase>, <phrase>biology</phrase>, <phrase>public health</phrase>, <phrase>ecology</phrase>, or <phrase>energy</phrase>, then this course is for you!  In this first part of the course, we will focus on the digitization and the storage of geodata. In particular, you will learn: - To characterize spatial objects and/or phenomena (territory modeling) with respect to their position in space (through coordinate systems, projections, and spatial relationships) and according to their intrinsic <phrase>nature</phrase> (object/<phrase>vector</phrase> mode vs. Image/<phrase>raster</phrase> mode);  - About the different means used to acquire <phrase>spatial data</phrase>; including direct measurement, georeferencing images, digitization, existing <phrase>data source</phrase>, etc.); - About the different ways in which geodata can be stored - notably, files and <phrase>relational databases</phrase>; - How to use <phrase>data</phrase> modeling tools to describe and create a spatial <phrase>database</phrase>; - To query and analyze <phrase>data</phrase> using <phrase>SQL</phrase>, a common <phrase>data manipulation</phrase> <phrase>language</phrase>.  The second part of this course will focus on methods of <phrase>spatial analysis</phrase> and geodata representation. In this section, you will learn: - How to describe and quantify the spatial properties of discrete variables, for example through <phrase>spatial autocorrelation</phrase>; - To work with continuous variables. In particular, we will look at <phrase>sampling</phrase> strategies, how to construct contour lines and isovalue curves, and we will explore different <phrase>interpolation</phrase> methods; - To use <phrase>digital</phrase> <phrase>elevation</phrase> models and create their <phrase>derivative</phrase> <phrase>products</phrase> (i.e. slope, orientation); - How to evaluate the interaction between different types of geodata through overlay and interaction techniques; - How to create effective maps based around the rules of <phrase>graphic</phrase> <phrase>semiology</phrase>; - Finally, we will also explore other, <phrase>increasingly common</phrase>, forms of spatial representation such as interactive <phrase>web-mapping</phrase> and 3D representations.  You can find an interactive forum for course participants on our <phrase>Facebook</phrase> page: https://www.facebook.com/moocsig
In this course, you'll apply your <phrase>knowledge</phrase> of classification models and embeddings to build a <phrase>ML</phrase> <phrase>pipeline</phrase> that functions as a recommendation <phrase>engine</phrase>.  • Devise a <phrase>content-based</phrase> recommendation <phrase>engine</phrase> • Implement a <phrase>collaborative filtering</phrase> recommendation <phrase>engine</phrase> • Build a <phrase>hybrid</phrase> recommendation <phrase>engine</phrase> with user and content embeddings  >>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<
Con frecuencia, <phrase>en</phrase> <phrase>tu</phrase> actividad profesional <phrase>te</phrase> enfrentas con <phrase>la</phrase> necesidad de analizar una gran cantidad de datos con <phrase>el</phrase> propósito de identificar <phrase>si</phrase> existe alguna relación entre ellos y de esta forma contar con información valiosa que <phrase>te</phrase> permita poder tomar una decisión.  Los datos con los cuales <phrase>se</phrase> <phrase>genera</phrase> <phrase>la</phrase> información, además de requerir de un apropiado tratamiento, demandan también de una adecuada técnica para <phrase>su</phrase> análisis.   <phrase>En</phrase> este curso serás capaz de conocer y utilizar distintas técnicas basadas <phrase>en</phrase> <phrase>el</phrase> análisis estadístico con un enfoque hacia <phrase>la</phrase> inteligencia de negocios (<phrase>Business Intelligence</phrase>), los cuales <phrase>te</phrase> permitirán crear modelos para mejorar <phrase>la</phrase> comprensión de cómo los datos <phrase>se</phrase> relacionan con <phrase>la</phrase> población subyacente, validar <phrase>el</phrase> modelo  y emplear <phrase>el</phrase> análisis predictivo para evaluar escenarios factibles encaminados a orientar tus <phrase>decisiones</phrase> futuras.  Al finalizar este curso habrás desarrollado <phrase>la</phrase> capacidad de utilizar distintas técnicas para <phrase>la</phrase> construcción y evaluación de modelos  que con base <phrase>en</phrase> criterios de desempeño preestablecidos <phrase>te</phrase> permitirán aprovechar <phrase>el</phrase> valor de los datos.  Agradecemos a Fundación Televisa por <phrase>su</phrase> participación <phrase>en</phrase> <phrase>la</phrase> producción de este curso; con lo cual colabora a inspirar y desarrollar <phrase>el</phrase> potencial de las personas, a través de <phrase>su</phrase> compromiso con <phrase>la</phrase> educación y <phrase>la</phrase> cultura.
Learn how advances in geospatial <phrase>technology</phrase> and <phrase>analytical methods</phrase> have changed how we do everything, and discover how to make maps and analyze geographic patterns using the latest tools.  The past decade has seen an explosion of new mechanisms for understanding and using <phrase>location information</phrase> in widely-accessible technologies. This Geospatial <phrase>Revolution</phrase> has resulted in the development of <phrase>consumer</phrase> <phrase>GPS</phrase> tools, interactive web maps, and location-aware <phrase>mobile</phrase> devices. These radical advances are making it possible for people from all <phrase>walks</phrase> of <phrase>life</phrase> to use, collect, and understand <phrase>spatial information</phrase> like never before.   This course brings together <phrase>core concepts</phrase> in <phrase>cartography</phrase>, <phrase>geographic information systems</phrase>, and spatial thinking with <phrase>real-world</phrase> examples to provide the fundamentals necessary to engage with <phrase>Geography</phrase> beyond the <phrase>surface</phrase>-level. We will explore what makes <phrase>spatial information</phrase> special, how <phrase>spatial data</phrase> is created, how <phrase>spatial analysis</phrase> is conducted, and how to <phrase>design</phrase> maps so that they’re effective at telling the stories we wish to share. To gain experience using this <phrase>knowledge</phrase>, we will work with the latest mapping and analysis <phrase>software</phrase> to explore geographic problems.
This course dives into the basics of <phrase>machine learning</phrase> using an approachable, and well-known <phrase>programming language</phrase>, <phrase>Python</phrase>.  In this course, we will be reviewing two <phrase>main components</phrase>: First, you will be learning about the purpose of <phrase>Machine Learning</phrase> and where it applies to the <phrase>real world</phrase>.  Second, you will get a <phrase>general</phrase> overview of <phrase>Machine Learning</phrase> topics such as supervised vs <phrase>unsupervised learning</phrase>,  <phrase>model</phrase> evaluation, and <phrase>Machine Learning</phrase> <phrase>algorithms</phrase>.   In this course, you practice with <phrase>real-life</phrase> examples of <phrase>Machine learning</phrase> and see how it affects <phrase>society</phrase> in ways you may not have guessed!  By just putting in a few hours a week for the next few weeks, this is what you’ll get. 1) New skills to add to your resume, such as <phrase>regression</phrase>, classification, clustering, <phrase>sci</phrase>-kit learn and <phrase>SciPy</phrase>  2) New projects that you can add to your portfolio, including <phrase>cancer</phrase> detection, predicting economic trends, predicting customer churn, recommendation engines, and many more. 3) And a certificate in <phrase>machine learning</phrase> to prove your competency, and share it anywhere you like online or offline, such as <phrase>LinkedIn</phrase> profiles and <phrase>social media</phrase>.  If you choose to take this course and earn the <phrase>Coursera</phrase> course certificate, you will also earn an <phrase>IBM</phrase> <phrase>digital</phrase> badge upon successful completion of the course.  LIMITED TIME OFFER: Subscription is only $39 <phrase>USD</phrase> per month for access to graded materials and a certificate.
In this course, you'll get a big-picture view of using <phrase>SQL</phrase> for <phrase>big data</phrase>, starting with an overview of <phrase>data</phrase>, <phrase>database</phrase> systems, and the common querying <phrase>language</phrase> (<phrase>SQL</phrase>). Then you'll learn the characteristics of <phrase>big data</phrase> and <phrase>SQL</phrase> tools for working on <phrase>big data</phrase> platforms. You'll also <phrase>install</phrase> an exercise environment (<phrase>virtual machine</phrase>) to be used through the specialization courses, and you'll have an opportunity to do some initial exploration of <phrase>databases</phrase> and tables in that environment.  By the end of the course, you will be able to • distinguish operational from analytic <phrase>databases</phrase>, and understand how these are applied in <phrase>big data</phrase>; • understand how <phrase>database</phrase> and table <phrase>design</phrase> provides structures for working with <phrase>data</phrase>; • appreciate how differences in volume and <phrase>variety</phrase> of <phrase>data</phrase> affects your choice of an appropriate <phrase>database</phrase> system; • recognize the features and benefits of <phrase>SQL</phrase> <phrase>dialects</phrase> designed to work with <phrase>big data</phrase> systems for storage and analysis; and  • explore <phrase>databases</phrase> and tables in a <phrase>big data</phrase> platform.  To use the hands-on environment for this course, you need to download and <phrase>install</phrase> a <phrase>virtual machine</phrase> and the <phrase>software</phrase> on which to <phrase>run</phrase> it. Before continuing, be sure that you have access to a <phrase>computer</phrase> that meets the following <phrase>hardware</phrase> and <phrase>software</phrase> requirements: • <phrase>Windows</phrase>, macOS, or <phrase>Linux</phrase> <phrase>operating system</phrase> (<phrase>iPads</phrase> and <phrase>Android</phrase> tablets will not work) • <phrase>64-bit</phrase> <phrase>operating system</phrase> (<phrase>32-bit</phrase> <phrase>operating systems</phrase> will not work) • 8 <phrase>GB</phrase> <phrase>RAM</phrase> or more • 25GB <phrase>free</phrase> <phrase>disk space</phrase> or more • <phrase>Intel</phrase> <phrase>VT</phrase>-x or <phrase>AMD</phrase>-V <phrase>virtualization</phrase> support <phrase>enabled</phrase> (on <phrase>Mac</phrase> <phrase>computers</phrase> with <phrase>Intel</phrase> processors, this is always <phrase>enabled</phrase>; on <phrase>Windows</phrase> and <phrase>Linux</phrase> <phrase>computers</phrase>, you might need to enable it in the <phrase>BIOS</phrase>) • For <phrase>Windows XP</phrase> <phrase>computers</phrase> only: You must have an unzip utility such as 7-<phrase>Zip</phrase> or <phrase>WinZip</phrase> installed (<phrase>Windows</phrase> XP’s built-in unzip utility will not work)
>>> By enrolling in this course you agree to the <phrase>End User License Agreement</phrase> as set out in the FAQ.  Once enrolled you can access the license in the Resources <phrase>area</phrase> <<<  This course, Applied <phrase>Artificial Intelligence</phrase> with DeepLearning, is part of the <phrase>IBM</phrase> Advanced <phrase>Data Science</phrase> Certificate which <phrase>IBM</phrase> is currently creating and gives you <phrase>easy access</phrase> to the invaluable insights into <phrase>Deep Learning</phrase> models used by experts in <phrase>Natural Language</phrase> Processing, <phrase>Computer</phrase> Vision, <phrase>Time Series</phrase> Analysis, and many other disciplines. We’ll learn about the fundamentals of <phrase>Linear Algebra</phrase> and <phrase>Neural Networks</phrase>. Then we <phrase>introduce</phrase> the most popular DeepLearning Frameworks like Keras, TensorFlow, PyTorch, DeepLearning4J and <phrase>Apache</phrase> SystemML. Keras and TensorFlow are making up the greatest portion of this course. We learn about <phrase>Anomaly Detection</phrase>, <phrase>Time Series</phrase> Forecasting, <phrase>Image Recognition</phrase> and <phrase>Natural Language</phrase> Processing by building up models using Keras on <phrase>real-life</phrase> examples from <phrase>IoT</phrase> (<phrase>Internet</phrase> of Things), Financial Marked <phrase>Data</phrase>, <phrase>Literature</phrase> or Image <phrase>Databases</phrase>. Finally, we learn how to scale those <phrase>artificial</phrase> brains using Kubernetes, <phrase>Apache</phrase> Spark and <phrase>GPUs</phrase>.  IMPORTANT: THIS COURSE ALONE IS NOT SUFFICIENT TO OBTAIN THE "<phrase>IBM</phrase> Watson <phrase>IoT</phrase> Certified <phrase>Data</phrase> <phrase>Scientist</phrase> certificate". You need to take three other courses where two of them are currently built. The Specialization will be ready late <phrase>spring</phrase>, early <phrase>summer</phrase> 2018  Using these approaches, no <phrase>matter</phrase> what your skill levels in topics you would like to <phrase>master</phrase>, you can change your thinking and change your <phrase>life</phrase>. If you’re already an expert, this peep under the mental hood will give your ideas for turbocharging successful creation and deployment of DeepLearning models. If you’re struggling, you’ll see a structured <phrase>treasure trove</phrase> of practical techniques that walk you through what you need to do to get on <phrase>track</phrase>. If you’ve ever wanted to become better at anything, this course will help serve as your guide.  Prerequisites: Some coding skills are necessary. Preferably <phrase>python</phrase>, but any other <phrase>programming language</phrase> will do fine. Also some <phrase>basic</phrase> understanding of <phrase>math</phrase> (<phrase>linear algebra</phrase>) is a plus, but we will <phrase>cover</phrase> that part in the first week as well.  If you choose to take this course and earn the <phrase>Coursera</phrase> course certificate, you will also earn an <phrase>IBM</phrase> <phrase>digital</phrase> badge.  To find out more about <phrase>IBM</phrase> <phrase>digital</phrase> badges follow the link ibm.biz/badging.
Systems that perform <phrase>big data</phrase> analytics require highly distributed architectures and new levels of <phrase>memory</phrase> and <phrase>processing power</phrase>. This course covers main topics associated with systems such as Hadopp <phrase>MapReduce</phrase>, <phrase>Apache</phrase> Spark, and <phrase>Graph</phrase> Processing Engines.
This is the second course in the four-course specialization <phrase>Python</phrase> <phrase>Data</phrase> <phrase>Products</phrase> for <phrase>Predictive Analytics</phrase>, building on the <phrase>data</phrase> processing <phrase>covered</phrase> in Course 1 and introducing the basics of designing predictive models in <phrase>Python</phrase>. In this course, you will understand the fundamental concepts of <phrase>statistical learning</phrase> and learn various methods of building predictive models. At each <phrase>step</phrase> in the specialization, you will gain hands-on experience in <phrase>data</phrase> manipulation and building your skills, eventually culminating in a capstone project encompassing all the concepts taught in the specialization.
<phrase>Ce</phrase> cours accéléré d'une semaine s'appuie sur <phrase>les</phrase> cours précédents <phrase>de la</phrase> spécialisation <phrase>Data</phrase> <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. À travers un ensemble de présentations vidéo, de démonstrations et d'ateliers pratiques, vous apprendrez à créer et gérer <phrase>des</phrase> clusters de calcul pour exécuter <phrase>des</phrase> tâches <phrase>Hadoop</phrase>, Spark, <phrase>Pig</phrase> et/ou Hive sur <phrase>Google</phrase> <phrase>Cloud</phrase> Platform. Vous découvrirez également comment accéder à diverses options <phrase>Google</phrase> <phrase>Cloud</phrase> Storage à partir de vos clusters de calcul et comment intégrer <phrase>les</phrase> fonctionnalités de <phrase>machine learning</phrase> de <phrase>Google</phrase> à vos programmes d'analyse.   Lors <phrase>des</phrase> ateliers pratiques, vous allez créer et gérer <phrase>des</phrase> clusters Dataproc à l'aide <phrase>de la</phrase> <phrase>console</phrase> Web et de l'interface de ligne de commande (<phrase>CLI</phrase>). Vous utiliserez <phrase>les</phrase> clusters pour exécuter <phrase>des</phrase> tâches Spark et <phrase>Pig</phrase>. Vous créerez ensuite <phrase>des</phrase> blocs-notes iPython qui s'intègrent à BigQuery et à <phrase>Google</phrase> <phrase>Cloud</phrase> Storage, et qui utilisent Spark. Enfin, vous intégrerez <phrase>les</phrase> <phrase>API</phrase> de <phrase>machine learning</phrase> à vos analyses de données.  Prérequis • Avoir suivi <phrase>la</phrase> formation <phrase>Google</phrase> <phrase>Cloud</phrase> Platform Fundamentals: <phrase>Big Data</phrase> & <phrase>Machine Learning</phrase> (ou une formation équivalente) • Disposer d'une certaine connaissance de <phrase>Python</phrase>
In the capstone, students will engage on a <phrase>real world</phrase> project requiring them to apply skills from the entire <phrase>data science</phrase> <phrase>pipeline</phrase>: preparing, organizing, and transforming <phrase>data</phrase>, constructing a <phrase>model</phrase>, and evaluating <phrase>results</phrase>.    Through a collaboration with Coursolve, each Capstone project is associated with partner stakeholders who have a vested interest in your <phrase>results</phrase> and are eager to deploy them in practice.  These projects will not be straightforward and the outcome is not prescribed -- you will need to tolerate ambiguity and negative <phrase>results</phrase>!  But we believe the experience will be rewarding and will better prepare you for <phrase>data science</phrase> projects in practice.
This if the final course in the specialization which builds upon the <phrase>knowledge</phrase> learned in <phrase>Python</phrase> <phrase>Programming</phrase> Essentials, <phrase>Python</phrase> <phrase>Data</phrase> Representations, and <phrase>Python</phrase> <phrase>Data</phrase> Analysis.  We will learn how to <phrase>install</phrase> external packages for use within <phrase>Python</phrase>, acquire <phrase>data</phrase> from sources on the Web, and then we will clean, process, analyze, and visualize that <phrase>data</phrase>. This course will combine the skills learned throughout the specialization to enable you to write interesting, practical, and useful programs.  By the end of the course, you will be comfortable installing <phrase>Python</phrase> packages, analyzing existing <phrase>data</phrase>, and generating visualizations of that <phrase>data</phrase>.  This course will complete your <phrase>education</phrase> as a scripter, enabling you to locate, <phrase>install</phrase>, and use <phrase>Python</phrase> packages written by others. You will be able to effectively utilize tools and packages that are widely available to amplify your effectiveness and write useful programs.
<phrase>Digital</phrase> <phrase>health</phrase> is rapidly being realised as the future of <phrase>healthcare</phrase>. While this is placing emphasis on the input of quality <phrase>health</phrase> <phrase>data</phrase> in <phrase>digital</phrase> records and systems, the delivery of safe and quality <phrase>healthcare</phrase> relies not only on the input of <phrase>data</phrase>, but also the ability to access and derive meaning from <phrase>data</phrase> to generate evidence, inform <phrase>decision making</phrase> and drive better <phrase>health</phrase> outcomes.  This course provides <phrase>insight into</phrase> the use of <phrase>healthcare</phrase> <phrase>data</phrase>, including an overview of <phrase>best practices</phrase> and the practical realities of obtaining useful <phrase>information</phrase> from <phrase>digital</phrase> <phrase>health</phrase> systems via the understanding of the fundamental concepts of <phrase>health</phrase> <phrase>data</phrase> analytics.    Learners will understand why <phrase>data quality</phrase> is essential in <phrase>modern healthcare</phrase>, as they are guided through various stages of the <phrase>data</phrase> <phrase>life</phrase> cycle, starting with the generation of quality <phrase>health</phrase> <phrase>data</phrase>, through to discovering patterns and extracting <phrase>knowledge</phrase> from <phrase>health</phrase> <phrase>data</phrase> using common methodologies and tools in the <phrase>basic</phrase> analysis, visualisation and <phrase>communication</phrase> of <phrase>health</phrase> <phrase>data</phrase>. In doing so, learners explore current <phrase>healthcare</phrase> delivery contexts, and future and emerging <phrase>digital</phrase> <phrase>health</phrase> <phrase>data</phrase> systems and applications that are rapidly becoming tomorrow’s <phrase>reality</phrase>.  On completion of this course, you will be able to: 1.	Identify <phrase>digital</phrase> <phrase>health</phrase> technologies, <phrase>health</phrase> <phrase>data</phrase> sources, and the evolving roles of <phrase>health</phrase> workforce in <phrase>digital</phrase> <phrase>health</phrase> environments 2.	Understand key <phrase>health</phrase> <phrase>data</phrase> concepts and terminology, including the significance of <phrase>data integrity</phrase> and stakeholder roles in the <phrase>data</phrase> <phrase>life</phrase> cycle 3.	Use <phrase>health</phrase> <phrase>data</phrase> and <phrase>basic</phrase> <phrase>data</phrase> analysis to inform and improve <phrase>decision making</phrase> and practice. 4.	Apply effective methods of <phrase>communication</phrase> of <phrase>health</phrase> <phrase>data</phrase> to facilitate safe and quality care.  During this course, you will interact with learning content contributed by: •	<phrase>Digital</phrase> <phrase>Health</phrase> <phrase>Cooperative</phrase> <phrase>Research</phrase> Centre •	<phrase>Australian</phrase> <phrase>Digital</phrase> <phrase>Health</phrase> Agency •	<phrase>eHealth</phrase> <phrase>NSW</phrase> •	<phrase>Sydney</phrase> Local <phrase>Health</phrase> <phrase>District</phrase> •	The <phrase>NSW</phrase> Ministry of <phrase>Health</phrase> •	<phrase>Health</phrase> <phrase>Education</phrase> and Training Institute •	Clinical Excellence Commission  •	Chris O’Brien Lifehouse •	<phrase>Monash</phrase> Partners / <phrase>Australian</phrase> <phrase>Health</phrase> <phrase>Research</phrase> <phrase>Alliance</phrase> •	<phrase>Australian</phrase> <phrase>Research</phrase> <phrase>Data</phrase> <phrase>Commons</phrase> •	<phrase>Justice</phrase> <phrase>Health</phrase> & <phrase>Forensic</phrase> <phrase>Mental Health</phrase> Network •	<phrase>South</phrase> Eastern <phrase>Sydney</phrase> Local <phrase>Health</phrase> <phrase>District</phrase> •	<phrase>Western Sydney</phrase> Local <phrase>Health</phrase> <phrase>District</phrase> •	Westmead <phrase>Breast Cancer</phrase> Institute •	Agency for Clinical <phrase>Innovation</phrase> •	<phrase>Western</phrase> <phrase>NSW</phrase> Local <phrase>Health</phrase> <phrase>District</phrase> •	<phrase>Sydney</phrase> Children’s <phrase>Hospital</phrase> Network  This course is a collaborative <phrase>venture</phrase> between <phrase>NSW</phrase> <phrase>Health</phrase>, the <phrase>University</phrase> of <phrase>Sydney</phrase> and the <phrase>Digital</phrase> <phrase>Health</phrase> <phrase>Cooperative</phrase> <phrase>Research</phrase> Centre, including dedicated resources from <phrase>eHealth</phrase> <phrase>NSW</phrase>, <phrase>Health</phrase> <phrase>Education</phrase> and Training Institute, and the <phrase>Research</phrase> in Implementation <phrase>Science</phrase> & <phrase>eHealth</phrase> group. While many <phrase>learning resources</phrase> and <phrase>case</phrase> examples are drawn from the <phrase>NSW</phrase> <phrase>Health service</phrase> <phrase>context</phrase>, this course has relevance for all existing and future <phrase>health</phrase> workforce, regardless of role or work <phrase>context</phrase>. Note: Materials used are for learning purposes and content may not reflect your organisation’s policies. When working with <phrase>data</phrase>, make sure you <phrase>act</phrase> within the guidelines and policies of your organisation.
This course teaches you the fundamentals of computational phenotyping, a <phrase>biomedical informatics</phrase> <phrase>method</phrase> for identifying patient populations. In this course you will learn how different <phrase>clinical data</phrase> types perform when trying to identify patients with a particular <phrase>disease</phrase> or trait. You will also learn how to program different <phrase>data</phrase> manipulations and combinations to increase the complexity and improve the performance of your <phrase>algorithms</phrase>. Finally, you will have a chance to put your skills to the <phrase>test</phrase> with a <phrase>real-world</phrase> <phrase>practical application</phrase> where you develop a computational phenotyping <phrase>algorithm</phrase> to identify patients who have <phrase>hypertension</phrase>. You will complete this work using a real <phrase>clinical data</phrase> set while using a <phrase>free</phrase>, online computational environment for <phrase>data science</phrase> hosted by our <phrase>Industry</phrase> Partner <phrase>Google</phrase> <phrase>Cloud</phrase>.
機械学習モデルの精度を高める方法や、特に有効な特徴を抽出するためのデータ列の見極め方を知りたい人におすすめのコースです。Feature <phrase>Engineering</phrase> on <phrase>Google</phrase> <phrase>Cloud</phrase> Platform では、良い特徴と悪い特徴の要素について、また、機械学習モデルで最適に使用できるように、特徴を前処理して変換する方法についても取り上げます。  このコースでは実践演習として、インタラクティブなラボを使用し、<phrase>Google</phrase> <phrase>Cloud</phrase> Platform 内で特徴を選択して前処理を行います。インストラクターが解答のコードについて説明します。解答のコードは、今後、皆さんが自身の <phrase>ML</phrase> プロジェクトに取り組む際に参照できるよう、一般公開される予定です。
<phrase>Machine learning</phrase> is the study that allows <phrase>computers</phrase> to adaptively improve their performance with experience accumulated from the <phrase>data</phrase> observed. Our two sister courses teach the most fundamental algorithmic, theoretical and practical tools that any user of <phrase>machine learning</phrase> needs to know. This second course of the two would focus more on algorithmic tools, and the other course would focus more on <phrase>mathematical</phrase> tools. [機器學習旨在讓電腦能由資料中累積的經驗來自我進步。我們的兩項姊妹課程將介紹各領域中的機器學習使用者都應該知道的基礎演算法、理論及實務工具。本課程將較為著重方法類的工具，而另一課程將較為著重數學類的工具。]
Neurohacking describes how to use the R <phrase>programming language</phrase> (https://cran.r-project.org/) and its associated package to perform manipulation, processing, and analysis of <phrase>neuroimaging</phrase> <phrase>data</phrase>. We focus on <phrase>publicly-available</phrase> structural <phrase>magnetic resonance imaging</phrase> (<phrase>MRI</phrase>). We discuss concepts such as inhomogeneity correction, <phrase>image registration</phrase>, and image visualization.  By the end of this course, you will be able to:  <phrase>Read/write</phrase> images of the <phrase>brain</phrase> in the NIfTI (<phrase>Neuroimaging</phrase> Informatics <phrase>Technology</phrase> <phrase>Initiative</phrase>) format Visualize and explore these images Perform inhomogeneity correction, <phrase>brain</phrase> extraction, and <phrase>image registration</phrase> (within a subject and to a template).
While telling stories with <phrase>data</phrase> has been part of the <phrase>news</phrase> practice since its earliest days, it is in the midst of a <phrase>renaissance</phrase>. <phrase>Graphics</phrase> desks which used to be deemed as “the <phrase>art</phrase> <phrase>department</phrase>,” a subfield outside the work of newsrooms, are becoming a core part of newsrooms’ operation. Those people (they often have various titles: <phrase>data</phrase> journalists, <phrase>news</phrase> artists, <phrase>graphic</phrase> reporters, developers, etc.) who <phrase>design</phrase> <phrase>news</phrase> <phrase>graphics</phrase> are expected to be full-fledged journalists and work closely with reporters and editors. The purpose of this class is to learn how to think about the visual presentation of <phrase>data</phrase>, how and why it works, and how to doit the right way. We will learn how to make <phrase>graphs</phrase> like The <phrase>New York</phrase> Times, <phrase>Vox</phrase>, <phrase>Pew</phrase>, and FiveThirtyEight. In the end, you can share–embed your beautiful charts in publications, <phrase>blog</phrase> posts, and <phrase>websites</phrase>.  This course assumes you understand <phrase>basic</phrase> coding skills, preferably <phrase>Python</phrase>. However, we also provide a brief review on <phrase>Python</phrase> in Module 1, in <phrase>case</phrase> you want to refresh yourself on the basics and perform simple <phrase>data</phrase> analysis.
This course teaches you the fundamentals of transforming clinical practice using predictive models. This course examines specific challenges and methods of clinical implementation, that <phrase>clinical data</phrase> scientists must be aware of when developing their predictive models.
機械学習とはどのようなもので、どのような問題の解決に役立つのでしょうか。<phrase>Google</phrase> では機械学習について、データだけでなくロジックの面からも独自の視点で考えています。こうした捉え方が、機械学習モデルのパイプライン構築を考えるうえでなぜ有効なのか説明します。次に、候補となるユースケースを機械学習で学習できる形に変換する 5 つの段階について説明し、こうした段階を省略しないことの重要性について論じます。最後に、機械学習が助長する可能性のある偏見の認識と、それを識別する方法について説明します。
Everyday across the world, thousands of businesses are victimized by <phrase>fraud</phrase>.  Who commits these bad acts?  Why? And, how? In this course we are going to help you answer the questions: who commits <phrase>fraud</phrase>, why and how.  We’ll also help you develop skills for catching them.
In this course on <phrase>Linear Algebra</phrase> we look at what <phrase>linear algebra</phrase> is and how it relates to vectors and matrices. Then we look through what vectors and matrices are and how to work with them, including the knotty problem of eigenvalues and <phrase>eigenvectors</phrase>, and how to use these to <phrase>solve problems</phrase>. Finally  we look at how to use these to do fun things with datasets - like how to rotate images of faces and how to extract <phrase>eigenvectors</phrase> to look at how the <phrase>Pagerank</phrase> <phrase>algorithm</phrase> works. Since we're aiming at <phrase>data</phrase>-driven applications, we'll be implementing some of these ideas in code, not just on pencil and <phrase>paper</phrase>. Towards the end of the course, you'll write code blocks and encounter Jupyter notebooks in <phrase>Python</phrase>, but don't worry, these will be quite <phrase>short</phrase>, focussed on the concepts, and will guide you through if you’ve not coded before.  At the end of this course you will have an intuitive understanding of vectors and matrices that will help you <phrase>bridge</phrase> the gap into <phrase>linear algebra</phrase> problems, and how to apply these concepts to <phrase>machine learning</phrase>.
The lectures of this course are based on the first 11 chapters of Prof. Raymond Yeung’s <phrase>textbook</phrase> entitled <phrase>Information</phrase> Theory and Network Coding (<phrase>Springer</phrase> 2008).  This <phrase>book</phrase> and its predecessor, A First Course in <phrase>Information</phrase> Theory (Kluwer 2002, essentially the first edition of the 2008 <phrase>book</phrase>), have been adopted by over 60 <phrase>universities</phrase> around the world as either a <phrase>textbook</phrase> or reference text.  At the completion of this course, the <phrase>student</phrase> should be able to: 1) Demonstrate <phrase>knowledge</phrase> and understanding of the fundamentals of <phrase>information theory</phrase>. 2) Appreciate the notion of fundamental limits in <phrase>communication</phrase> systems and more generally all systems. 3) Develop deeper understanding of <phrase>communication</phrase> systems. 4) Apply the concepts of <phrase>information theory</phrase> to various disciplines in <phrase>information science</phrase>.
Welcome to <phrase>Data</phrase> Analytics Foundations for <phrase>Accountancy</phrase> I! You’re joining thousands of learners currently enrolled in the course. I'm excited to have you in the class and look <phrase>forward</phrase> to your contributions to the learning <phrase>community</phrase>.  To begin, I recommend taking a few minutes to explore the course site. Review the material we’ll <phrase>cover</phrase> each week, and preview the assignments you’ll need to complete to <phrase>pass</phrase> the course. Click Discussions to see forums where you can discuss the course material with <phrase>fellow</phrase> students taking the class.  If you have questions about course content, please post them in the forums to get help from others in the course <phrase>community</phrase>. For technical problems with the <phrase>Coursera</phrase> platform, visit the Learner Help <phrase>Center</phrase>.  Good luck as you get started, and I hope you enjoy the course!
この 1 週間の集中動画セミナーでは、<phrase>Google</phrase> <phrase>Cloud</phrase> Platform（GCP）のビッグデータと機械学習に関する機能をご紹介します。受講者は <phrase>Google</phrase> <phrase>Cloud</phrase> Platform の概要と、データ処理機能の詳細について学ぶことができます。  このコースを完了すると、以下のことができるようになります。 • ビッグデータと機械学習に関連する <phrase>Google</phrase> <phrase>Cloud</phrase> Platform の主なプロダクトの用途と価値の特定 • CloudSQL と <phrase>Cloud</phrase> Dataproc を使用した、既存の <phrase>MySQL</phrase> と <phrase>Hadoop</phrase> / <phrase>Pig</phrase> / Spark / Hive ワークロードの <phrase>Google</phrase> <phrase>Cloud</phrase> Platform への移行 • BigQuery と <phrase>Cloud</phrase> Datalab によるインタラクティブなデータ分析の実行 • <phrase>Cloud</phrase> <phrase>SQL</phrase>、<phrase>BigTable</phrase>、データストアからの選択 • TensorFlow を使用したニューラル ネットワークのトレーニングと使用 • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform のさまざまなデータ処理プロダクトからの取捨選択  このコースに申し込むには、以下の 1 つ以上の分野で約 1 年の経験が必要となります。 • <phrase>SQL</phrase> などの一般的なクエリ言語 • 抽出、変換、読み込みの各アクティビティ • データ モデリング • 機械学習と統計 • <phrase>Python</phrase> でのプログラミング  <phrase>Google</phrase> アカウントについての注意:  • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform の無料トライアルに登録するには、<phrase>Google</phrase> / <phrase>Gmail</phrase> アカウントとクレジット カードまたは口座番号が必要です（<phrase>Google</phrase> のサービスは、現在中国ではご利用いただけません）。 • 請求書の送付先住所が欧州連合（<phrase>EU</phrase>）またはロシアの <phrase>Google</phrase> <phrase>Cloud</phrase> Platform ユーザーは、https://cloud.google.com/billing/docs/resources/vat-overview の VAT 概要文書をお読みください。 • <phrase>Google</phrase> <phrase>Cloud</phrase> Platform の無料トライアルに関するよくある質問については、https://cloud.google.com/<phrase>free</phrase>/?hl=ja をご覧ください。  Looking for the <phrase>English</phrase> version of this course? Check out https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals/ Buscando <phrase>la</phrase> versión <phrase>en</phrase> español de este curso? Visita https://www.coursera.org/learn/gcp-<phrase>big-data</phrase>-<phrase>ml</phrase>-fundamentals-<phrase>es</phrase>/
