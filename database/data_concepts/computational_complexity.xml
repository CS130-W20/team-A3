<doc><id>computational_complexity</id><concept_name>computational complexity</concept_name><wiki>In computer science, the computational complexity, or simply complexity of an algorithm is the amount of resources required for running it (a property unrelated to &#8220;complexity&#8221; in a conventional sense). The computational complexity of a problem is the minimum of the complexities of all possible algorithms for this problem (including the unknown algorithms).
As the amount of needed resources varies with the input, the complexity is generally expressed as a function n &#8594; f(n), where n is the size of the input, and f(n) is either the worst-case complexity, that is the maximum of the amount of resources that are needed for all inputs of size n, or the average-case complexity, that is average of the amount of resources over all input of size n.
When the nature of the resources is not explicitly given, this is usually the time needed for running the algorithm, and one talks of time complexity. However, this depends on the computer that is used, and the time is generally expressed as the number of needed elementary operations, which are supposed to take a constant time on a given computer, and to change by a constant factor when one changes computer.
Otherwise, the resource that is considered is often the size of the memory that is needed, and one talks of space complexity.
</wiki></doc>