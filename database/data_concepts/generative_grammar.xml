<doc><id>generative_grammar</id><concept_name>generative grammar</concept_name><wiki>Generative grammar is a linguistic theory that regards linguistics as the study of a hypothesised innate grammatical structure.[3] Similarly to some structuralist theories, such as glossematics,[4] generative grammar considers grammar as a system of rules that generates exactly those combinations of words that form grammatical sentences in a given language. The difference from structural and functional models[2]  is that the object is placed into the verb phrase in generative grammar.[5] This theory was developed by Noam Chomsky in the 1950s. Linguists who follow the generative approach have been called generativists. The generative school has focused on the study of syntax and addressed other aspects of a language's structure, including morphology and phonology.
Early versions of Chomsky's theory were called transformational grammar, a term still used to include his subsequent theories,[6] the most recent of which is the minimalist program theory: Chomsky and other generativists have argued that many of the properties of a generative grammar arise from a universal grammar that is innate to the human brain, rather than being learned from the environment (see the poverty of the stimulus argument).
There are a number of versions of generative grammar currently practiced within linguistics. 
A contrasting approach is that of constraint-based grammars. Where a generative grammar attempts to list all the rules that result in all well-formed sentences, constraint-based grammars allow anything that is not otherwise constrained. Certain versions of dependency grammar, head-driven phrase structure grammar, lexical functional grammar, categorial grammar, relational grammar, link grammar, and tree-adjoining grammar are constraint-based grammars that have been proposed. In stochastic grammar, grammatical correctness is taken as a probabilistic variable, rather than a discrete (yes or no) property.
</wiki></doc>